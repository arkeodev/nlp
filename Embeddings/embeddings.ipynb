{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCXRMftyonnev7QH8H6Q4T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Embeddings/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unraveling the World of Embeddings"
      ],
      "metadata": {
        "id": "vJw0kRhBl_Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the realm of artificial intelligence, embeddings serve as the cornerstone for translating various types of data into a language that machines can understand. From the intricacies of human language to the complexities of images and sounds, embeddings transform raw data into dense vectors, paving the way for nuanced machine comprehension. This article explores the multifaceted world of embeddings, encompassing text, images, and audio, highlighting their development, applications, and how they're fine-tuned for specific tasks."
      ],
      "metadata": {
        "id": "MASYJx4TmCVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Decoding_Algorithms/images/greedy_decoder.png\" width=\"400\" height=\"400\" alt=\"Greedy Decoder\">\n",
        "    <figcaption>Greedy Decoder</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "B3zCbDdCU-KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Evolution of Text Representation Techniques"
      ],
      "metadata": {
        "id": "o41Z_PmWefoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evolution of text representation techniques in machine learning from simple models like One-Hot Encoding, Bag of Words (BoW), and Term Frequency-Inverse Document Frequency (TF-IDF) to complex embeddings illustrates a journey towards capturing the nuances of language more effectively.\n",
        "\n",
        "Each method addressed specific limitations of its predecessors, adding layers of sophistication and bringing us closer to a more profound understanding of text semantics.\n",
        "\n",
        "Here will not be addressed for the details of each of these techniques. It can be got more information in the Blog Article called [Step by Step Guide to Master NLP â€“ Word Embedding and Text Vectorization](https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/#) for each of them.\n",
        "\n"
      ],
      "metadata": {
        "id": "dIP1Ahjue2SI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. One-Hot Encoding"
      ],
      "metadata": {
        "id": "jYfrZnB6e9Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: In one-hot encoding, each word in the vocabulary is represented by a vector where one element is set to 1, and the rest are set to 0. The vector's length equals the size of the vocabulary, and each word is assigned a unique position in this vector space.\n",
        "\n",
        "\n",
        "\n",
        "**Advantages**:\n",
        "- **Base Method**: Being the foundational method, one-hot encoding's main advantage was its straightforward approach to turning text data into a numerical form that machine learning algorithms could process.\n",
        "- **Simplicity**: Easy to understand and implement.\n",
        "- **Uniqueness**: Each word is uniquely represented, with no overlap between representations.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Sparsity**: One-hot vectors are extremely sparse, leading to inefficient use of memory and computational resources, especially with large vocabularies.\n",
        "- **No Semantic Information**: This method does not capture any semantic relationships between words. Words are treated as independent entities, making it impossible to gauge similarity or relatedness."
      ],
      "metadata": {
        "id": "zFQpHI7jfBJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Example vocabulary\n",
        "words = np.array(['cat', 'dog', 'bird', 'fish']).reshape(-1, 1)\n",
        "\n",
        "# One-hot encode\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_encoded = encoder.fit_transform(words)\n",
        "\n",
        "print(one_hot_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcI5C3oVfwfn",
        "outputId": "59c42413-9827-472a-917b-c8b2095f7c12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "qQJHfnPlgCsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: The Bag of Words model represents text as an unordered collection of words, disregarding grammar and word order but keeping multiplicity. A document is represented by a vector indicating the frequency of each word from the vocabulary in the document.\n",
        "\n",
        "\n",
        "\n",
        "**Advantages**:\n",
        "- **Frequency Information**: Captures the frequency of words in the document, providing more information than one-hot encoding.\n",
        "- **Simplicity**: Still relatively simple to understand and implement.\n",
        "- BoW addressed the lack of information in one-hot encoding by incorporating word frequency, which offered a basic form of \"importance\" to words in a document.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Sparsity and Dimensionality**: Similar to one-hot encoding, BoW vectors can become very sparse and high-dimensional with large vocabularies, leading to inefficiencies.\n",
        "- **Lack of Context and Order**: BoW does not account for the order of words, losing important syntactic and semantic information. It treats \"dog bites man\" and \"man bites dog\" identically.\n",
        "- **No Semantic Relationships**: BoW cannot capture semantic relationships or the meaning of words within the context of a sentence."
      ],
      "metadata": {
        "id": "gncJau26gGOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text\n",
        "documents = [\"the cat sat on the mat\", \"the dog sat on the log\"]\n",
        "\n",
        "# Create and fit the CountVectorizer to the documents\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# This step generates the vocabulary and transforms the documents into a sparse matrix\n",
        "bow = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Vocabulary mapping: word -> column index\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "\n",
        "# Print the vocabulary to understand how words are indexed\n",
        "print(\"Vocabulary:\")\n",
        "for word, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"{word}: {index}\")\n",
        "\n",
        "# Convert the BoW sparse matrix to a dense array and print it\n",
        "dense_bow = bow.toarray()\n",
        "print(\"\\nBag of Words representation (dense array):\")\n",
        "print(dense_bow)\n",
        "\n",
        "# Explain the output based on the vocabulary\n",
        "print(\"\\nInterpreting the BoW representation:\")\n",
        "print(\"Each row in the dense array corresponds to a document.\")\n",
        "print(\"Each column represents a word from the vocabulary, in the order printed above.\")\n",
        "print(\"Values represent how many times each word appears in each document.\")\n",
        "\n",
        "# For a more detailed explanation\n",
        "print(\"\\nDetailed explanation for the first document:\")\n",
        "print(documents[0])\n",
        "for word, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"The word '{word}' appears {dense_bow[0, index]} time(s).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcrtccx-gKn8",
        "outputId": "9c2409ec-494e-4aa6-86f4-2adff19168dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "cat: 0\n",
            "dog: 1\n",
            "log: 2\n",
            "mat: 3\n",
            "on: 4\n",
            "sat: 5\n",
            "the: 6\n",
            "\n",
            "Bag of Words representation (dense array):\n",
            "[[1 0 0 1 1 1 2]\n",
            " [0 1 1 0 1 1 2]]\n",
            "\n",
            "Interpreting the BoW representation:\n",
            "Each row in the dense array corresponds to a document.\n",
            "Each column represents a word from the vocabulary, in the order printed above.\n",
            "Values represent how many times each word appears in each document.\n",
            "\n",
            "Detailed explanation for the first document:\n",
            "the cat sat on the mat\n",
            "The word 'cat' appears 1 time(s).\n",
            "The word 'dog' appears 0 time(s).\n",
            "The word 'log' appears 0 time(s).\n",
            "The word 'mat' appears 1 time(s).\n",
            "The word 'on' appears 1 time(s).\n",
            "The word 'sat' appears 1 time(s).\n",
            "The word 'the' appears 2 time(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Term Frequency-Inverse Document Frequency (TF-IDF)"
      ],
      "metadata": {
        "id": "KRtnNVdvgtqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: TF-IDF builds upon BoW by weighting the word frequencies based on how commonly they appear across documents. Words that appear frequently in a document but less frequently across multiple documents are given higher importance.\n",
        "\n",
        "**Advantages**:\n",
        "- **Weighted Importance**: Weights word frequencies to highlight words that are important in a document but not common across all documents.\n",
        "- **Reduces Impact of Common Words**: Helps mitigate the effect of commonly used words that may not contribute much to the overall meaning of documents.\n",
        "- TF-IDF built on BoW's frequency counts by adding a weighting scheme that emphasizes the significance of words based on their distribution across documents, thus providing a rudimentary form of context sensitivity.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Still Lacks Context and Semantics**: While TF-IDF provides a way to highlight more \"important\" words, it still doesn't capture word meanings, relationships, or the context within which words appear.\n",
        "- **High Dimensionality**: Like BoW, TF-IDF suffers from high dimensionality issues, leading to sparse representations that can be computationally expensive to work with."
      ],
      "metadata": {
        "id": "mseVnh0Keu7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text\n",
        "documents = [\"the cat sat on the mat\", \"the dog sat on the log\"]\n",
        "\n",
        "# Create and fit the TfidfVectorizer to the documents\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# This step generates the vocabulary and transforms the documents into a TF-IDF-weighted sparse matrix\n",
        "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Vocabulary mapping: word -> column index\n",
        "vocabulary = tfidf_vectorizer.vocabulary_\n",
        "\n",
        "# Print the vocabulary to understand how words are indexed\n",
        "print(\"Vocabulary:\")\n",
        "for word, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"{word}: {index}\")\n",
        "\n",
        "# Convert the TF-IDF sparse matrix to a dense array and print it\n",
        "dense_tfidf = tfidf.toarray()\n",
        "print(\"\\nTF-IDF representation (dense array):\")\n",
        "print(dense_tfidf)\n",
        "\n",
        "# Explain the output based on the vocabulary\n",
        "print(\"\\nInterpreting the TF-IDF representation:\")\n",
        "print(\"Each row in the dense array corresponds to a document.\")\n",
        "print(\"Each column represents a word from the vocabulary, in the order printed above.\")\n",
        "print(\"Values are the TF-IDF weights, representing the importance of each word in each document relative to the corpus.\")\n",
        "\n",
        "# For a more detailed explanation\n",
        "print(\"\\nDetailed explanation for the first document:\")\n",
        "print(documents[0])\n",
        "for word, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"The word '{word}' has a TF-IDF weight of {dense_tfidf[0, index]:.4f} in the first document.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiH26tiWhC-1",
        "outputId": "ad6d1315-ff00-4781-d393-ba48e9e1ed32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "cat: 0\n",
            "dog: 1\n",
            "log: 2\n",
            "mat: 3\n",
            "on: 4\n",
            "sat: 5\n",
            "the: 6\n",
            "\n",
            "TF-IDF representation (dense array):\n",
            "[[0.44554752 0.         0.         0.44554752 0.31701073 0.31701073\n",
            "  0.63402146]\n",
            " [0.         0.44554752 0.44554752 0.         0.31701073 0.31701073\n",
            "  0.63402146]]\n",
            "\n",
            "Interpreting the TF-IDF representation:\n",
            "Each row in the dense array corresponds to a document.\n",
            "Each column represents a word from the vocabulary, in the order printed above.\n",
            "Values are the TF-IDF weights, representing the importance of each word in each document relative to the corpus.\n",
            "\n",
            "Detailed explanation for the first document:\n",
            "the cat sat on the mat\n",
            "The word 'cat' has a TF-IDF weight of 0.4455 in the first document.\n",
            "The word 'dog' has a TF-IDF weight of 0.0000 in the first document.\n",
            "The word 'log' has a TF-IDF weight of 0.0000 in the first document.\n",
            "The word 'mat' has a TF-IDF weight of 0.4455 in the first document.\n",
            "The word 'on' has a TF-IDF weight of 0.3170 in the first document.\n",
            "The word 'sat' has a TF-IDF weight of 0.3170 in the first document.\n",
            "The word 'the' has a TF-IDF weight of 0.6340 in the first document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. N-Grams"
      ],
      "metadata": {
        "id": "B0iGm2YpqF4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: N-grams are sequences of *n* contiguous items from a given sample of text or speech. In text processing, these items are typically words or characters. N-grams help capture local context and sequence information within text data, providing a foundation for modeling language beyond individual words.\n",
        "\n",
        "**Advantages**:\n",
        "\n",
        "- **Contextual Information**: N-grams incorporate context by considering sequences of words or characters, capturing more information about language structure than individual words.\n",
        "- **Improved Language Modeling**: By analyzing sequences of words, n-grams allow for better prediction of the next item in a sequence, enhancing language modeling tasks.\n",
        "- **Flexibility and Simplicity**: N-grams offer a simple yet flexible approach to text representation, allowing adjustments in granularity by changing the value of *n*.\n",
        "\n",
        "**Disadvantages**:\n",
        "\n",
        "- **Explosion of Features**: As *n* increases, the number of possible n-grams can grow exponentially, leading to high-dimensional feature spaces and computational challenges.\n",
        "- **Fixed Window Size**: N-grams capture context within a fixed window size (*n*), which may not always align with the actual scope of contextual dependencies in text.\n",
        "- **Lack of Deep Semantic Understanding**: While n-grams can model the presence and co-occurrence of sequences, they lack the ability to capture deeper semantic relationships in the way that embeddings do."
      ],
      "metadata": {
        "id": "0S0QbrvHpgup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text\n",
        "documents = [\"the cat sat on the mat\", \"the dog sat on the log\"]\n",
        "\n",
        "# Create and fit the CountVectorizer to the documents with bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Generate the vocabulary and transform the documents into a sparse matrix of bigrams\n",
        "bigrams = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Vocabulary mapping: bigram -> column index\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "\n",
        "# Print the vocabulary to understand how bigrams are indexed\n",
        "print(\"Vocabulary (Bigrams):\")\n",
        "for bigram, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"{bigram}: {index}\")\n",
        "\n",
        "# Convert the sparse matrix to a dense array and print it\n",
        "dense_bigrams = bigrams.toarray()\n",
        "print(\"\\nBigram representation (dense array):\")\n",
        "print(dense_bigrams)\n",
        "\n",
        "# Explain the output based on the vocabulary\n",
        "print(\"\\nInterpreting the Bigram representation:\")\n",
        "print(\"Each row in the dense array corresponds to a document.\")\n",
        "print(\"Each column represents a bigram from the vocabulary, in the order printed above.\")\n",
        "print(\"Values represent how many times each bigram appears in each document.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arsS548hp-mg",
        "outputId": "6d9a2381-d81e-45b2-c6f7-31c16c9b14e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Bigrams):\n",
            "cat sat: 0\n",
            "dog sat: 1\n",
            "on the: 2\n",
            "sat on: 3\n",
            "the cat: 4\n",
            "the dog: 5\n",
            "the log: 6\n",
            "the mat: 7\n",
            "\n",
            "Bigram representation (dense array):\n",
            "[[1 0 1 1 1 0 0 1]\n",
            " [0 1 1 1 0 1 1 0]]\n",
            "\n",
            "Interpreting the Bigram representation:\n",
            "Each row in the dense array corresponds to a document.\n",
            "Each column represents a bigram from the vocabulary, in the order printed above.\n",
            "Values represent how many times each bigram appears in each document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Transition to Embeddings\n",
        "\n",
        "While the aforementioned methods provided essential advancements, they still fell short in capturing the complex semantics of language, contextual nuances, and the relationships between words. **Embeddings** emerged as a solution to these limitations by offering:\n",
        "\n",
        "- **Dense Representations**: Unlike the sparse representations of previous methods, embeddings are dense, significantly reducing dimensionality and improving computational efficiency.\n",
        "- **Semantic Information**: Embeddings capture not just the presence of words but their meanings, nuances, and the relationships between them, based on how they're used in large corpora.\n",
        "- **Context Awareness**: With the advent of context-based embeddings (like those from BERT), the representation can vary depending on the word's usage, capturing its meaning more accurately in different contexts."
      ],
      "metadata": {
        "id": "VnFNP_f2hSk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Embeedings"
      ],
      "metadata": {
        "id": "oM371R8qahSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings are powerful tools in machine learning and natural language processing that convert various types of data into dense vector representations, making it easier for models to process and analyze. Here's a brief overview of the types of vector embeddings:\n",
        "\n",
        "1. **Word Embeddings**\n",
        "- **Description**: Represent individual words as vectors.\n",
        "- **Techniques**: Word2Vec, GloVe, FastText.\n",
        "- **Applications**: Semantic analysis, natural language processing tasks.\n",
        "\n",
        "2. **Sentence Embeddings**\n",
        "- **Description**: Represent entire sentences as vectors.\n",
        "- **Models**: Universal Sentence Encoder (USE), SkipThought.\n",
        "- **Applications**: Sentence similarity, sentiment analysis, document classification.\n",
        "\n",
        "3. **Document Embeddings**\n",
        "- **Description**: Represent longer texts like articles, papers, or books as vectors.\n",
        "- **Techniques**: Doc2Vec, Paragraph Vectors.\n",
        "- **Applications**: Document classification, information retrieval, content analysis.\n",
        "\n",
        "4. **Image Embeddings**\n",
        "- **Description**: Represent images as vectors by capturing visual features.\n",
        "- **Techniques**: Convolutional Neural Networks (CNNs), ResNet, VGG.\n",
        "- **Applications**: Image classification, object detection, image similarity.\n",
        "\n",
        "5. **Audio Embeddings**\n",
        "- **Description**: Represent audio signals or sounds as vectors by capturing their acoustic features and characteristics. Audio embeddings translate the complex, time-series data of audio files into a structured, high-dimensional space where similar sounds are represented by vectors that are close to each other.\n",
        "- **Techniques**: Deep learning models like WaveNet, Transformer-based models, and spectrogram-based CNNs.\n",
        "- **Applications**: Speech recognition, music classification, sound event detection, speaker identification, and emotion analysis from voice.\n",
        "- **Purpose**: Capture the nuances of audio content, including the tone, pitch, rhythm, and other acoustic properties, enabling machines to understand and process audio data effectively.\n",
        "\n",
        "6. **User Embeddings**\n",
        "- **Description**: Represent users in a system or platform as vectors.\n",
        "- **Applications**: Recommendation systems, personalized marketing, user segmentation.\n",
        "- **Purpose**: Capture user preferences, behaviors, and characteristics.\n",
        "\n",
        "7. **Product Embeddings**\n",
        "- **Description**: Represent products in e-commerce or recommendation systems as vectors.\n",
        "- **Applications**: Product recommendation, comparison, and analysis.\n",
        "- **Purpose**: Capture product attributes, features, and semantic information.\n",
        "\n",
        "8. **Multi-Modal Embeddings**\n",
        "- **Description**: Represent data that combines information from multiple modalities (e.g., text, images, audio) as vectors. Multi-modal embeddings aim to capture the complementary and shared information across different types of data.\n",
        "- **Techniques**: Fusion techniques in deep learning that might involve early fusion, late fusion, or hybrid approaches to integrate features from multiple neural network branches, each processing a different modality. Transformer-based models are increasingly used for their ability to handle sequences of different data types.\n",
        "- **Applications**: Cross-modal information retrieval (e.g., finding images based on text descriptions), automatic captioning of images and videos, visual question answering (VQA), and enhanced recommendation systems that consider visual, textual, and auditory information.\n",
        "\n",
        "  Multi-modal embeddings are at the forefront of advancing AI's ability to understand and interact with the world in a more human-like manner. By effectively combining information from various sources, these embeddings facilitate a deeper understanding of content, context, and user intentions, paving the way for innovative applications that span across text, vision, and audio domains.\n",
        "\n",
        "- **Purpose**: Leverage the strengths of different data types to improve the performance of tasks that require understanding complex relationships between them, providing a richer representation of the data than would be possible with a single modality.\n"
      ],
      "metadata": {
        "id": "-7ir2mo6aeKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Context Based Embeddings vs. Word Based Embeddings"
      ],
      "metadata": {
        "id": "kOiP-6GumE6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context-based embeddings and word-based embeddings are two approaches for representing words in natural language processing (NLP) tasks. Both aim to convert text into numerical vectors, but they differ significantly in how they capture the meaning and usage of words in language. Understanding these differences is crucial for selecting the appropriate technique for various NLP applications."
      ],
      "metadata": {
        "id": "L-ysucSJsHnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word-Based Embeddings"
      ],
      "metadata": {
        "id": "_yh9KxiisNy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: Word-based embeddings represent words  as dense vectors in a continuous vector space. Each word is mapped to a single vector, and this representation is fixed, regardless of the word's context in different sentences. Popular models include Word2Vec, GloVe, and FastText.\n",
        "\n",
        "**Advantages**:\n",
        "- **Simplicity**: Easy to use and understand. Each word has a single, pre-computed vector.\n",
        "- **Efficiency**: Since each word is represented by a single vector, these embeddings can be pre-trained on large corpora and reused across different tasks.\n",
        "- **Semantic Similarity**: Word-based embeddings capture semantic relationships between words. Words that appear in similar contexts tend to have vectors that are close together in the embedding space.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Polysemy and Homonymy**: They struggle with words that have multiple meanings (polysemy) or words that sound the same but have different meanings (homonymy), since each word has only one vector representation.\n",
        "- **Lack of Context**: The meaning of a word can change based on its context, which is not captured in a static, word-based embedding."
      ],
      "metadata": {
        "id": "TwlcBIBFtTzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context-Based Embeddings"
      ],
      "metadata": {
        "id": "HWm022wQtWhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**Description**: Context-based embeddings, also known as contextual embeddings, generate representations for words that take into account the surrounding words. As a result, the same word can have different embeddings based on its context, allowing for a more nuanced understanding of language. Examples include BERT, GPT (Generative Pretrained Transformer), and ELMo.\n",
        "\n",
        "**Advantages**:\n",
        "- **Dynamic Contextual Understanding**: These embeddings capture the meaning of words in context, addressing the limitations of word-based embeddings with polysemous words.\n",
        "- **Richer Representations**: By considering the context, context-based embeddings can capture more complex linguistic patterns and relationships, leading to better performance on a variety of NLP tasks.\n",
        "- **Adaptability**: They can be fine-tuned for specific tasks, allowing the pre-trained models to adapt to the nuances of a particular dataset or domain.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Computational Complexity**: Generating context-based embeddings is computationally more intensive than using static word-based embeddings. Processing requires significant computational resources, especially for large documents.\n",
        "- **Implementation Complexity**: Working with models like BERT and GPT can be more complex due to the need for fine-tuning, handling tokenization in a specific way, and managing larger model sizes."
      ],
      "metadata": {
        "id": "JOTH_8e9TSu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What do \"Words\" Mean According to the Tokenisation?"
      ],
      "metadata": {
        "id": "fLPFZ4qsuDgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When discussing embeddings in the context of natural language processing (NLP), the term \"words\" can indeed refer to actual words, but depending on the tokenization strategy, it can also refer to subwords or characters. This distinction is important because different models and approaches to embeddings might treat the input text in varied ways, leading to different representations"
      ],
      "metadata": {
        "id": "5Yd5jQkJuosD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Words"
      ],
      "metadata": {
        "id": "F7-OQfQiur6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Traditional Approach**: Initially, embeddings were primarily focused on words as the basic units of language, with each unique word in the vocabulary getting its own vector representation.\n",
        "- **Tokenization**: Involves splitting text into individual words based on spaces and punctuation."
      ],
      "metadata": {
        "id": "zUYzkiFeuy7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subwords"
      ],
      "metadata": {
        "id": "CkkPgosXu045"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Models like BERT and GPT**: These models often use subword tokenization strategies (e.g., Byte-Pair Encoding (BPE), SentencePiece, WordPiece) to handle the vocabulary more efficiently.\n",
        "- **Advantages**: Subword tokenization helps deal with the problem of out-of-vocabulary (OOV) words by breaking down unknown words into known subword units, allowing the model to generate embeddings for words it hasn't explicitly seen during training.\n",
        "- **Example**: The word \"unbelievable\" might be tokenized into \"un\", \"##believ\", and \"##able\"."
      ],
      "metadata": {
        "id": "z9UwCqz-u3xF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Characters"
      ],
      "metadata": {
        "id": "fis-LjDLu85c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Character-Level Models**: Some models and approaches operate at the character level, treating each character as the basic unit for generating embeddings.\n",
        "- **Applications**: Character-level embeddings are particularly useful in tasks like named entity recognition (NER) in languages with rich morphology, or in models focusing on spelling and phonetics."
      ],
      "metadata": {
        "id": "hKbHIVICu55m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implications of Different Tokenizations for Embeddings"
      ],
      "metadata": {
        "id": "BVUVDHaDvFgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Flexibility and Coverage**: Subword and character-level tokenization offer more flexibility and better coverage of the language, especially for languages with large vocabularies or agglutinative languages where words can have many forms.\n",
        "- **Context Sensitivity**: Regardless of whether embeddings are generated for words, subwords, or characters, context-based embedding models can dynamically adjust the representation based on the surrounding text. This means that the same subword or character can have different embeddings depending on its context, enhancing the model's ability to capture nuanced meanings.\n",
        "- **Computational Efficiency**: Subword and character-level approaches can improve computational efficiency by reducing the size of the vocabulary that the model needs to handle directly, albeit at the cost of potentially increased complexity in processing sequences."
      ],
      "metadata": {
        "id": "8pPr6hWhuMP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Based Embeddings"
      ],
      "metadata": {
        "id": "DUj6WbygvxMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "7krajITZxqHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Skipgrams"
      ],
      "metadata": {
        "id": "shM_TtqAxwOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Continuous bag of words"
      ],
      "metadata": {
        "id": "t3eryF3Jx2AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Based Embeddings"
      ],
      "metadata": {
        "id": "KWp9_yJLv1fb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t842V5Wwx_Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Fine-Tuning: Customizing Embeddings"
      ],
      "metadata": {
        "id": "dRv-eYt1Voc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning for Domains and Tasks"
      ],
      "metadata": {
        "id": "hCoijS_yVqXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contextualized embeddings generated by transformer models can be fine-tuned for specific domains or tasks, and this is a common practice in natural language processing (NLP) to achieve state-of-the-art results on a wide range of tasks."
      ],
      "metadata": {
        "id": "1HTdevOt0uPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning Process"
      ],
      "metadata": {
        "id": "48gQF9N20wl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning involves taking a pre-trained transformer model, which has learned general language representations from a large corpus of text, and continuing the training process on a smaller, task-specific dataset. This allows the model to adjust its weights, including the embeddings, to better capture the nuances and terminology of the specific domain or task. Here's a general overview of how fine-tuning works:\n"
      ],
      "metadata": {
        "id": "X3IfqIMX020j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Start with a Pre-trained Model**: You begin with a model that has been pre-trained on a large, general-purpose dataset. This model has developed a broad understanding of the language, including its syntax and semantics.\n",
        "\n",
        "2. **Select a Task-specific Dataset**: You then choose a smaller dataset that is specific to your task or domain. This dataset could be related to medical texts, legal documents, customer reviews, etc., depending on your needs.\n",
        "\n",
        "3. **Continue Training**: The pre-trained model is then trained (or fine-tuned) on this task-specific dataset. During this process, all parts of the model, including the initial word embeddings and the transformer layers that produce the contextualized embeddings, are updated to better align with the specifics of the task or domain.\n",
        "\n",
        "4. **Adjust Learning Rate**: It's common practice to use a smaller learning rate during fine-tuning than was used during the initial pre-training. This helps prevent the model from \"forgetting\" its general understanding of the language while it learns the specifics of the new task.\n",
        "\n",
        "5. **Evaluation and Adjustment**: After fine-tuning, the model is evaluated on a separate validation set to ensure it has effectively adapted to the task. Adjustments may be made to the training process based on this evaluation to improve performance."
      ],
      "metadata": {
        "id": "fwM_uG-L1G4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benefits of Fine-Tuning"
      ],
      "metadata": {
        "id": "qtItHpQ71KTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Improved Performance**: Fine-tuning allows the model to adapt its pre-learned language representations to the specific lexical and syntactical characteristics of a domain or task, often leading to improved performance compared to using a pre-trained model directly.\n",
        "\n",
        "- **Efficiency**: Because the model has already learned a lot of general language knowledge during pre-training, fine-tuning on a specific task requires relatively less data and computational resources compared to training a model from scratch.\n",
        "\n",
        "- **Flexibility**: This approach is flexible and can be applied across different tasks (e.g., text classification, question answering, named entity recognition) and domains (e.g., finance, healthcare, law) by simply changing the task-specific dataset used for fine-tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "h9Kr-4avVsau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "xKIcxnQs1Dh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning contextualized embeddings and transformer models on specific domains or tasks is a powerful technique in NLP. It leverages the broad language understanding acquired during pre-training and specializes the model to perform well on tasks that require more specific knowledge or understanding."
      ],
      "metadata": {
        "id": "HgjrifH21Bm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n"
      ],
      "metadata": {
        "id": "ZbyEvR0UV6DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings have emerged as fundamental to advancing artificial intelligence, enabling machines to process and understand the vast complexities of human language, visual content, and sound. By continually refining these representations and tailoring them to specific applications, we unlock new potentials for AI to interact with the world in increasingly sophisticated and intuitive ways. The journey of embeddings, from simple vector representations to complex, context-aware models, illustrates the ongoing evolution of machine intelligence and its boundless future prospects."
      ],
      "metadata": {
        "id": "_2T9-c_gl7ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "XeWSBNxtjUqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For the pre-embedding techniques: [Step by Step Guide to Master NLP â€“ Word Embedding and Text Vectorization](https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/#)"
      ],
      "metadata": {
        "id": "e6MucjOmkwHg"
      }
    }
  ]
}