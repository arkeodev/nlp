{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Embeddings/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJw0kRhBl_Bx"
      },
      "source": [
        "# Unraveling the World of Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MASYJx4TmCVL"
      },
      "source": [
        "In the realm of artificial intelligence, embeddings serve as the cornerstone for translating various types of data into a language that machines can understand. From the intricacies of human language to the complexities of images and sounds, embeddings transform raw data into dense vectors, paving the way for nuanced machine comprehension. This article explores the multifaceted world of embeddings, encompassing text, images, and audio, highlighting their development, applications, and how they're fine-tuned for specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3zCbDdCU-KK"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Embeddings/images/embeddings.png\" width=\"900\" height=\"400\" alt=\"Embeddings\">\n",
        "    <figcaption>Embeddings</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o41Z_PmWefoo"
      },
      "source": [
        "## The Evolution of Text Representation Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIP1Ahjue2SI"
      },
      "source": [
        "The evolution of text representation techniques in machine learning from simple models like One-Hot Encoding, Bag of Words (BoW), and Term Frequency-Inverse Document Frequency (TF-IDF) to complex embeddings illustrates a journey towards capturing the nuances of language more effectively.\n",
        "\n",
        "Each method addressed specific limitations of its predecessors, adding layers of sophistication and bringing us closer to a more profound understanding of text semantics.\n",
        "\n",
        "Here will not be addressed for the details of each of these techniques. It can be got more information in the Blog Article called [Step by Step Guide to Master NLP – Word Embedding and Text Vectorization](https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/#) for each of them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYfrZnB6e9Q-"
      },
      "source": [
        "### 1. One-Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFQpHI7jfBJi"
      },
      "source": [
        "**Description**: In one-hot encoding, each word in the vocabulary is represented by a vector where one element is set to 1, and the rest are set to 0. The vector's length equals the size of the vocabulary, and each word is assigned a unique position in this vector space.\n",
        "\n",
        "\n",
        "\n",
        "**Advantages**:\n",
        "- **Base Method**: Being the foundational method, one-hot encoding's main advantage was its straightforward approach to turning text data into a numerical form that machine learning algorithms could process.\n",
        "- **Simplicity**: Easy to understand and implement.\n",
        "- **Uniqueness**: Each word is uniquely represented, with no overlap between representations.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Sparsity**: One-hot vectors are extremely sparse, leading to inefficient use of memory and computational resources, especially with large vocabularies.\n",
        "- **No Semantic Information**: This method does not capture any semantic relationships between words. Words are treated as independent entities, making it impossible to gauge similarity or relatedness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcI5C3oVfwfn",
        "outputId": "59c42413-9827-472a-917b-c8b2095f7c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Example vocabulary\n",
        "words = np.array(['cat', 'dog', 'bird', 'fish']).reshape(-1, 1)\n",
        "\n",
        "# One-hot encode\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_encoded = encoder.fit_transform(words)\n",
        "\n",
        "print(one_hot_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJHfnPlgCsc"
      },
      "source": [
        "### 2. Bag of Words (BoW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gncJau26gGOo"
      },
      "source": [
        "**Description**: The Bag of Words model represents text as an unordered collection of words, disregarding grammar and word order but keeping multiplicity. A document is represented by a vector indicating the frequency of each word from the vocabulary in the document.\n",
        "\n",
        "\n",
        "\n",
        "**Advantages**:\n",
        "- **Frequency Information**: Captures the frequency of words in the document, providing more information than one-hot encoding.\n",
        "- **Simplicity**: Still relatively simple to understand and implement.\n",
        "- BoW addressed the lack of information in one-hot encoding by incorporating word frequency, which offered a basic form of \"importance\" to words in a document.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Sparsity and Dimensionality**: Similar to one-hot encoding, BoW vectors can become very sparse and high-dimensional with large vocabularies, leading to inefficiencies.\n",
        "- **Lack of Context and Order**: BoW does not account for the order of words, losing important syntactic and semantic information. It treats \"dog bites man\" and \"man bites dog\" identically.\n",
        "- **No Semantic Relationships**: BoW cannot capture semantic relationships or the meaning of words within the context of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcrtccx-gKn8",
        "outputId": "9c2409ec-494e-4aa6-86f4-2adff19168dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            "cat: 0\n",
            "dog: 1\n",
            "log: 2\n",
            "mat: 3\n",
            "on: 4\n",
            "sat: 5\n",
            "the: 6\n",
            "\n",
            "Bag of Words representation (dense array):\n",
            "[[1 0 0 1 1 1 2]\n",
            " [0 1 1 0 1 1 2]]\n",
            "\n",
            "Interpreting the BoW representation:\n",
            "Each row in the dense array corresponds to a document.\n",
            "Each column represents a word from the vocabulary, in the order printed above.\n",
            "Values represent how many times each word appears in each document.\n",
            "\n",
            "Detailed explanation for the first document:\n",
            "the cat sat on the mat\n",
            "The word 'cat' appears 1 time(s).\n",
            "The word 'dog' appears 0 time(s).\n",
            "The word 'log' appears 0 time(s).\n",
            "The word 'mat' appears 1 time(s).\n",
            "The word 'on' appears 1 time(s).\n",
            "The word 'sat' appears 1 time(s).\n",
            "The word 'the' appears 2 time(s).\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text\n",
        "documents = [\"the cat sat on the mat\", \"the dog sat on the log\"]\n",
        "\n",
        "# Create and fit the CountVectorizer to the documents\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# This step generates the vocabulary and transforms the documents into a sparse matrix\n",
        "bow = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Vocabulary mapping: word -> column index\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "\n",
        "# Print the vocabulary to understand how words are indexed\n",
        "print(\"Vocabulary:\")\n",
        "for word, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"{word}: {index}\")\n",
        "\n",
        "# Convert the BoW sparse matrix to a dense array and print it\n",
        "dense_bow = bow.toarray()\n",
        "print(\"\\nBag of Words representation (dense array):\")\n",
        "print(dense_bow)\n",
        "\n",
        "# Explain the output based on the vocabulary\n",
        "print(\"\\nInterpreting the BoW representation:\")\n",
        "print(\"Each row in the dense array corresponds to a document.\")\n",
        "print(\"Each column represents a word from the vocabulary, in the order printed above.\")\n",
        "print(\"Values represent how many times each word appears in each document.\")\n",
        "\n",
        "# For a more detailed explanation\n",
        "print(\"\\nDetailed explanation for the first document:\")\n",
        "print(documents[0])\n",
        "for word, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"The word '{word}' appears {dense_bow[0, index]} time(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRtnNVdvgtqs"
      },
      "source": [
        "### 3. Term Frequency-Inverse Document Frequency (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mseVnh0Keu7o"
      },
      "source": [
        "**Description**: TF-IDF builds upon BoW by weighting the word frequencies based on how commonly they appear across documents. Words that appear frequently in a document but less frequently across multiple documents are given higher importance.\n",
        "\n",
        "**Advantages**:\n",
        "- **Weighted Importance**: Weights word frequencies to highlight words that are important in a document but not common across all documents.\n",
        "- **Reduces Impact of Common Words**: Helps mitigate the effect of commonly used words that may not contribute much to the overall meaning of documents.\n",
        "- TF-IDF built on BoW's frequency counts by adding a weighting scheme that emphasizes the significance of words based on their distribution across documents, thus providing a rudimentary form of context sensitivity.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Still Lacks Context and Semantics**: While TF-IDF provides a way to highlight more \"important\" words, it still doesn't capture word meanings, relationships, or the context within which words appear.\n",
        "- **High Dimensionality**: Like BoW, TF-IDF suffers from high dimensionality issues, leading to sparse representations that can be computationally expensive to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiH26tiWhC-1",
        "outputId": "ad6d1315-ff00-4781-d393-ba48e9e1ed32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            "cat: 0\n",
            "dog: 1\n",
            "log: 2\n",
            "mat: 3\n",
            "on: 4\n",
            "sat: 5\n",
            "the: 6\n",
            "\n",
            "TF-IDF representation (dense array):\n",
            "[[0.44554752 0.         0.         0.44554752 0.31701073 0.31701073\n",
            "  0.63402146]\n",
            " [0.         0.44554752 0.44554752 0.         0.31701073 0.31701073\n",
            "  0.63402146]]\n",
            "\n",
            "Interpreting the TF-IDF representation:\n",
            "Each row in the dense array corresponds to a document.\n",
            "Each column represents a word from the vocabulary, in the order printed above.\n",
            "Values are the TF-IDF weights, representing the importance of each word in each document relative to the corpus.\n",
            "\n",
            "Detailed explanation for the first document:\n",
            "the cat sat on the mat\n",
            "The word 'cat' has a TF-IDF weight of 0.4455 in the first document.\n",
            "The word 'dog' has a TF-IDF weight of 0.0000 in the first document.\n",
            "The word 'log' has a TF-IDF weight of 0.0000 in the first document.\n",
            "The word 'mat' has a TF-IDF weight of 0.4455 in the first document.\n",
            "The word 'on' has a TF-IDF weight of 0.3170 in the first document.\n",
            "The word 'sat' has a TF-IDF weight of 0.3170 in the first document.\n",
            "The word 'the' has a TF-IDF weight of 0.6340 in the first document.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text\n",
        "documents = [\"the cat sat on the mat\", \"the dog sat on the log\"]\n",
        "\n",
        "# Create and fit the TfidfVectorizer to the documents\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# This step generates the vocabulary and transforms the documents into a TF-IDF-weighted sparse matrix\n",
        "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Vocabulary mapping: word -> column index\n",
        "vocabulary = tfidf_vectorizer.vocabulary_\n",
        "\n",
        "# Print the vocabulary to understand how words are indexed\n",
        "print(\"Vocabulary:\")\n",
        "for word, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"{word}: {index}\")\n",
        "\n",
        "# Convert the TF-IDF sparse matrix to a dense array and print it\n",
        "dense_tfidf = tfidf.toarray()\n",
        "print(\"\\nTF-IDF representation (dense array):\")\n",
        "print(dense_tfidf)\n",
        "\n",
        "# Explain the output based on the vocabulary\n",
        "print(\"\\nInterpreting the TF-IDF representation:\")\n",
        "print(\"Each row in the dense array corresponds to a document.\")\n",
        "print(\"Each column represents a word from the vocabulary, in the order printed above.\")\n",
        "print(\"Values are the TF-IDF weights, representing the importance of each word in each document relative to the corpus.\")\n",
        "\n",
        "# For a more detailed explanation\n",
        "print(\"\\nDetailed explanation for the first document:\")\n",
        "print(documents[0])\n",
        "for word, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"The word '{word}' has a TF-IDF weight of {dense_tfidf[0, index]:.4f} in the first document.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0iGm2YpqF4l"
      },
      "source": [
        "### 4. N-Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S0QbrvHpgup"
      },
      "source": [
        "**Description**: N-grams are sequences of *n* contiguous items from a given sample of text or speech. In text processing, these items are typically words or characters. N-grams help capture local context and sequence information within text data, providing a foundation for modeling language beyond individual words.\n",
        "\n",
        "**Advantages**:\n",
        "\n",
        "- **Contextual Information**: N-grams incorporate context by considering sequences of words or characters, capturing more information about language structure than individual words.\n",
        "- **Improved Language Modeling**: By analyzing sequences of words, n-grams allow for better prediction of the next item in a sequence, enhancing language modeling tasks.\n",
        "- **Flexibility and Simplicity**: N-grams offer a simple yet flexible approach to text representation, allowing adjustments in granularity by changing the value of *n*.\n",
        "\n",
        "**Disadvantages**:\n",
        "\n",
        "- **Explosion of Features**: As *n* increases, the number of possible n-grams can grow exponentially, leading to high-dimensional feature spaces and computational challenges.\n",
        "- **Fixed Window Size**: N-grams capture context within a fixed window size (*n*), which may not always align with the actual scope of contextual dependencies in text.\n",
        "- **Lack of Deep Semantic Understanding**: While n-grams can model the presence and co-occurrence of sequences, they lack the ability to capture deeper semantic relationships in the way that embeddings do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arsS548hp-mg",
        "outputId": "6d9a2381-d81e-45b2-c6f7-31c16c9b14e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary (Bigrams):\n",
            "cat sat: 0\n",
            "dog sat: 1\n",
            "on the: 2\n",
            "sat on: 3\n",
            "the cat: 4\n",
            "the dog: 5\n",
            "the log: 6\n",
            "the mat: 7\n",
            "\n",
            "Bigram representation (dense array):\n",
            "[[1 0 1 1 1 0 0 1]\n",
            " [0 1 1 1 0 1 1 0]]\n",
            "\n",
            "Interpreting the Bigram representation:\n",
            "Each row in the dense array corresponds to a document.\n",
            "Each column represents a bigram from the vocabulary, in the order printed above.\n",
            "Values represent how many times each bigram appears in each document.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text\n",
        "documents = [\"the cat sat on the mat\", \"the dog sat on the log\"]\n",
        "\n",
        "# Create and fit the CountVectorizer to the documents with bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "\n",
        "# Generate the vocabulary and transform the documents into a sparse matrix of bigrams\n",
        "bigrams = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Vocabulary mapping: bigram -> column index\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "\n",
        "# Print the vocabulary to understand how bigrams are indexed\n",
        "print(\"Vocabulary (Bigrams):\")\n",
        "for bigram, index in sorted(vocabulary.items(), key=lambda item: item[1]):\n",
        "    print(f\"{bigram}: {index}\")\n",
        "\n",
        "# Convert the sparse matrix to a dense array and print it\n",
        "dense_bigrams = bigrams.toarray()\n",
        "print(\"\\nBigram representation (dense array):\")\n",
        "print(dense_bigrams)\n",
        "\n",
        "# Explain the output based on the vocabulary\n",
        "print(\"\\nInterpreting the Bigram representation:\")\n",
        "print(\"Each row in the dense array corresponds to a document.\")\n",
        "print(\"Each column represents a bigram from the vocabulary, in the order printed above.\")\n",
        "print(\"Values represent how many times each bigram appears in each document.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnFNP_f2hSk5"
      },
      "source": [
        "### 5. Transition to Embeddings\n",
        "\n",
        "While the aforementioned methods provided essential advancements, they still fell short in capturing the complex semantics of language, contextual nuances, and the relationships between words. **Embeddings** emerged as a solution to these limitations by offering:\n",
        "\n",
        "- **Dense Representations**: Unlike the sparse representations of previous methods, embeddings are dense, significantly reducing dimensionality and improving computational efficiency.\n",
        "- **Semantic Information**: Embeddings capture not just the presence of words but their meanings, nuances, and the relationships between them, based on how they're used in large corpora.\n",
        "- **Context Awareness**: With the advent of context-based embeddings (like those from BERT), the representation can vary depending on the word's usage, capturing its meaning more accurately in different contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM371R8qahSa"
      },
      "source": [
        "## Types of Embeedings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ir2mo6aeKb"
      },
      "source": [
        "Embeddings are powerful tools in machine learning and natural language processing that convert various types of data into dense vector representations, making it easier for models to process and analyze. Here's a brief overview of the types of vector embeddings:\n",
        "\n",
        "1. **Word Embeddings**\n",
        "- **Description**: Represent individual words as vectors.\n",
        "- **Techniques**: Word2Vec, GloVe, FastText.\n",
        "- **Applications**: Semantic analysis, natural language processing tasks.\n",
        "\n",
        "2. **Sentence Embeddings**\n",
        "- **Description**: Represent entire sentences as vectors.\n",
        "- **Models**: Universal Sentence Encoder (USE), SkipThought.\n",
        "- **Applications**: Sentence similarity, sentiment analysis, document classification.\n",
        "\n",
        "3. **Document Embeddings**\n",
        "- **Description**: Represent longer texts like articles, papers, or books as vectors.\n",
        "- **Techniques**: Doc2Vec, Paragraph Vectors.\n",
        "- **Applications**: Document classification, information retrieval, content analysis.\n",
        "\n",
        "4. **Image Embeddings**\n",
        "- **Description**: Represent images as vectors by capturing visual features.\n",
        "- **Techniques**: Convolutional Neural Networks (CNNs), ResNet, VGG.\n",
        "- **Applications**: Image classification, object detection, image similarity.\n",
        "\n",
        "5. **Audio Embeddings**\n",
        "- **Description**: Represent audio signals or sounds as vectors by capturing their acoustic features and characteristics. Audio embeddings translate the complex, time-series data of audio files into a structured, high-dimensional space where similar sounds are represented by vectors that are close to each other.\n",
        "- **Techniques**: Deep learning models like WaveNet, Transformer-based models, and spectrogram-based CNNs.\n",
        "- **Applications**: Speech recognition, music classification, sound event detection, speaker identification, and emotion analysis from voice.\n",
        "- **Purpose**: Capture the nuances of audio content, including the tone, pitch, rhythm, and other acoustic properties, enabling machines to understand and process audio data effectively.\n",
        "\n",
        "6. **User Embeddings**\n",
        "- **Description**: Represent users in a system or platform as vectors.\n",
        "- **Applications**: Recommendation systems, personalized marketing, user segmentation.\n",
        "- **Purpose**: Capture user preferences, behaviors, and characteristics.\n",
        "\n",
        "7. **Product Embeddings**\n",
        "- **Description**: Represent products in e-commerce or recommendation systems as vectors.\n",
        "- **Applications**: Product recommendation, comparison, and analysis.\n",
        "- **Purpose**: Capture product attributes, features, and semantic information.\n",
        "\n",
        "8. **Multi-Modal Embeddings**\n",
        "- **Description**: Represent data that combines information from multiple modalities (e.g., text, images, audio) as vectors. Multi-modal embeddings aim to capture the complementary and shared information across different types of data.\n",
        "- **Techniques**: Fusion techniques in deep learning that might involve early fusion, late fusion, or hybrid approaches to integrate features from multiple neural network branches, each processing a different modality. Transformer-based models are increasingly used for their ability to handle sequences of different data types.\n",
        "- **Applications**: Cross-modal information retrieval (e.g., finding images based on text descriptions), automatic captioning of images and videos, visual question answering (VQA), and enhanced recommendation systems that consider visual, textual, and auditory information.\n",
        "\n",
        "  Multi-modal embeddings are at the forefront of advancing AI's ability to understand and interact with the world in a more human-like manner. By effectively combining information from various sources, these embeddings facilitate a deeper understanding of content, context, and user intentions, paving the way for innovative applications that span across text, vision, and audio domains.\n",
        "\n",
        "- **Purpose**: Leverage the strengths of different data types to improve the performance of tasks that require understanding complex relationships between them, providing a richer representation of the data than would be possible with a single modality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOiP-6GumE6h"
      },
      "source": [
        "## The Context Based Embeddings vs. Word Based Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-ysucSJsHnh"
      },
      "source": [
        "Context-based embeddings and word-based embeddings are two approaches for representing words in natural language processing (NLP) tasks. Both aim to convert text into numerical vectors, but they differ significantly in how they capture the meaning and usage of words in language. Understanding these differences is crucial for selecting the appropriate technique for various NLP applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yh9KxiisNy9"
      },
      "source": [
        "### Word-Based Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwlcBIBFtTzX"
      },
      "source": [
        "Word-based embeddings, like Word2Vec and GloVe, generate a single vector representation for each word in the vocabulary, regardless of its context in a sentence. These embeddings are learned from the statistical properties of words' co-occurrence patterns in a corpus.\n",
        "\n",
        "- **Examples**: Word2Vec, GloVe, FastText.\n",
        "- **Characteristics**:\n",
        "  - **Static**: Each word is assigned a single embedding vector, which doesn't change based on the word's context in different sentences.\n",
        "  - **Pre-trained**: Often learned from a large corpus of text and then applied or fine-tuned for specific tasks.\n",
        "  - **Semantic Similarity**: Embeddings capture semantic similarity based on the co-occurrence of words. Words that appear in similar contexts tend to have vectors that are close in the embedding space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWm022wQtWhz"
      },
      "source": [
        "### Context-Based (Contextual) Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOTH_8e9TSu1"
      },
      "source": [
        "Context-based embeddings, like those produced by BERT, generate representations for words that vary depending on the words' context. This means that the same word can have different embeddings when used in different sentences, reflecting its different uses or meanings.\n",
        "\n",
        "- **Examples**: BERT, GPT (all versions), ELMo, RoBERTa, Transformer-XL.\n",
        "- **Characteristics**:\n",
        "  - **Dynamic or Contextual**: The embedding for a word or token is influenced by the other words in its context, allowing the model to capture nuances like polysemy (words having multiple meanings) and syntax.\n",
        "  - **Model Architecture**: Typically derived from deep learning models based on the Transformer architecture, which allows the model to consider the entire context of a word by using self-attention mechanisms.\n",
        "  - **Application**: Beyond providing rich, nuanced word representations, these models can be fine-tuned for a wide range of NLP tasks, such as question answering, text classification, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison and Usage"
      ],
      "metadata": {
        "id": "MocMSlJ_zIwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Word-Based vs. Context-Based**: Word-based embeddings are simpler and less computationally intensive to use but lack the nuanced understanding of language that context-based embeddings provide. Context-based embeddings, while more powerful, require significantly more computational resources both for training and inference.\n",
        "- **Choice of Embedding**: The choice between word-based and context-based embeddings depends on the specific requirements of your task, the computational resources available, and the level of semantic understanding required. Word-based embeddings might be sufficient for tasks with limited context or when working with constrained resources, while context-based embeddings are preferred for tasks requiring a deep understanding of language context and nuance."
      ],
      "metadata": {
        "id": "4m_MNYmPy8wL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLPFZ4qsuDgZ"
      },
      "source": [
        "## What do \"Words\" Mean According to the Tokenisation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yd5jQkJuosD"
      },
      "source": [
        "When discussing embeddings in the context of natural language processing (NLP), the term \"words\" can indeed refer to actual words, but depending on the tokenization strategy, it can also refer to subwords or characters. This distinction is important because different models and approaches to embeddings might treat the input text in varied ways, leading to different representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7-OQfQiur6H"
      },
      "source": [
        "### Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUYzkiFeuy7M"
      },
      "source": [
        "- **Traditional Approach**: Initially, embeddings were primarily focused on words as the basic units of language, with each unique word in the vocabulary getting its own vector representation.\n",
        "- **Tokenization**: Involves splitting text into individual words based on spaces and punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkkPgosXu045"
      },
      "source": [
        "### Subwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9UwCqz-u3xF"
      },
      "source": [
        "- **Models like BERT and GPT**: These models often use subword tokenization strategies (e.g., Byte-Pair Encoding (BPE), SentencePiece, WordPiece) to handle the vocabulary more efficiently.\n",
        "- **Advantages**: Subword tokenization helps deal with the problem of out-of-vocabulary (OOV) words by breaking down unknown words into known subword units, allowing the model to generate embeddings for words it hasn't explicitly seen during training.\n",
        "- **Example**: The word \"unbelievable\" might be tokenized into \"un\", \"##believ\", and \"##able\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fis-LjDLu85c"
      },
      "source": [
        "### Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKbHIVICu55m"
      },
      "source": [
        "- **Character-Level Models**: Some models and approaches operate at the character level, treating each character as the basic unit for generating embeddings.\n",
        "- **Applications**: Character-level embeddings are particularly useful in tasks like named entity recognition (NER) in languages with rich morphology, or in models focusing on spelling and phonetics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVUVDHaDvFgV"
      },
      "source": [
        "### Implications of Different Tokenizations for Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pPr6hWhuMP1"
      },
      "source": [
        "- **Flexibility and Coverage**: Subword and character-level tokenization offer more flexibility and better coverage of the language, especially for languages with large vocabularies or agglutinative languages where words can have many forms.\n",
        "- **Context Sensitivity**: Regardless of whether embeddings are generated for words, subwords, or characters, context-based embedding models can dynamically adjust the representation based on the surrounding text. This means that the same subword or character can have different embeddings depending on its context, enhancing the model's ability to capture nuanced meanings.\n",
        "- **Computational Efficiency**: Subword and character-level approaches can improve computational efficiency by reducing the size of the vocabulary that the model needs to handle directly, albeit at the cost of potentially increased complexity in processing sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRv-eYt1Voc4"
      },
      "source": [
        "## Training an Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conceptual Overview"
      ],
      "metadata": {
        "id": "SQX50fzT04jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At its core, `nn.Embedding` creates a lookup table where each row corresponds to an embedding for a specific item in your vocabulary. The parameters `num_embeddings` and `embedding_dim` define the shape of this table:\n",
        "\n",
        "- **`num_embeddings`**: This is the size of the lookup table and effectively the size of your vocabulary. It's the total number of unique items (e.g., words) that you want to embed.\n",
        "- **`embedding_dim`**: This specifies the dimensionality of each embedding vector. It's how many numbers will represent each item in the embedding space."
      ],
      "metadata": {
        "id": "njmlkSjr07A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple model that includes an Embedding layer\n",
        "class SimpleEmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SimpleEmbeddingModel, self).__init__()\n",
        "        # The Embedding layer: 10 words in vocab, 5-dimensional embeddings\n",
        "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.embeddings(input)\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10  # Size of the vocabulary\n",
        "embedding_dim = 5  # Dimensionality of the embeddings\n",
        "\n",
        "# Create an instance of the model\n",
        "model = SimpleEmbeddingModel(vocab_size, embedding_dim)\n",
        "\n",
        "# Sample input: indices for words in the vocabulary\n",
        "# Let's say our input consists of indices for two words from our vocabulary\n",
        "input_indices = torch.tensor([2, 5], dtype=torch.long)\n",
        "\n",
        "# Get embeddings for the input\n",
        "embeddings = model(input_indices)\n",
        "\n",
        "print(\"Input Indices:\", input_indices)\n",
        "print(\"Embeddings:\\n\", embeddings)"
      ],
      "metadata": {
        "id": "AGxfbFgv0Rhy",
        "outputId": "2d7fdc85-e916-411e-c5b1-12e8e9e62719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Indices: tensor([2, 5])\n",
            "Embeddings:\n",
            " tensor([[ 0.1753,  0.7546, -0.1105, -1.4256,  1.3238],\n",
            "        [-0.2482, -0.1961, -1.8618, -0.0753,  1.4812]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Internals and Operation"
      ],
      "metadata": {
        "id": "jwwl0ykk09Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Initialization**: When an `nn.Embedding` layer is instantiated, it initializes a matrix (or table) of shape `(num_embeddings, embedding_dim)`. Each row of this matrix is the embedding of one item in your vocabulary. The initialization can be random or based on some predefined logic (PyTorch defaults to random initialization with values from a uniform distribution).\n",
        "\n",
        "2. **Lookup Process**: The operation performed by the embedding layer is essentially a lookup. When the layer receives integer indices (either a single integer or a batch of them), it returns the corresponding row(s) from its embedding matrix. This process is efficiently implemented and does not involve any computation like matrix multiplication or addition, just direct retrieval of rows based on the indices.\n",
        "\n",
        "3. **Learning**: Although the embedding matrix is initialized randomly, it's part of the model's parameters and gets updated during training through backpropagation. The learning process adjusts the values of these embeddings to minimize the model's loss function, effectively tuning the embeddings to capture meaningful semantic relationships as dictated by the task at hand.\n",
        "\n",
        "4. **Gradient Update**: During backpropagation, gradients are calculated for the embeddings used in the forward pass, and the embeddings are updated accordingly. This means that the embeddings are fine-tuned to better represent the relationships in the data as seen by the model."
      ],
      "metadata": {
        "id": "cSHn0X841ApV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Implications"
      ],
      "metadata": {
        "id": "2_bHT30w1CxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Embedding Lookup**: The simplicity of the lookup process makes embedding layers highly efficient for transforming categorical data into dense vectors. This is especially valuable in NLP, where dealing with large vocabularies and sparse one-hot encoded vectors would be computationally expensive and memory-intensive.\n",
        "\n",
        "- **Semantic Space**: The learned embeddings ideally represent the items in a semantic space where the distances between items reflect their semantic similarity. For example, in a well-trained model, words with similar meanings should be closer together in the embedding space.\n",
        "\n",
        "- **Generalization**: Embeddings can capture and generalize relationships in the data, making the model capable of handling unseen data better. For example, words used in similar contexts will end up with similar embeddings, even if the model has not seen them together during training."
      ],
      "metadata": {
        "id": "Uzv3eLtc0Vh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning: Customizing Embeddings"
      ],
      "metadata": {
        "id": "hMAlBYhWshNZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCoijS_yVqXq"
      },
      "source": [
        "### Fine-Tuning for Domains and Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HTdevOt0uPR"
      },
      "source": [
        "Contextualized embeddings generated by transformer models can be fine-tuned for specific domains or tasks, and this is a common practice in natural language processing (NLP) to achieve state-of-the-art results on a wide range of tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48gQF9N20wl5"
      },
      "source": [
        "### Fine-Tuning Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3IfqIMX020j"
      },
      "source": [
        "Fine-tuning involves taking a pre-trained transformer model, which has learned general language representations from a large corpus of text, and continuing the training process on a smaller, task-specific dataset. This allows the model to adjust its weights, including the embeddings, to better capture the nuances and terminology of the specific domain or task. Here's a general overview of how fine-tuning works:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwM_uG-L1G4j"
      },
      "source": [
        "1. **Start with a Pre-trained Model**: You begin with a model that has been pre-trained on a large, general-purpose dataset. This model has developed a broad understanding of the language, including its syntax and semantics.\n",
        "\n",
        "2. **Select a Task-specific Dataset**: You then choose a smaller dataset that is specific to your task or domain. This dataset could be related to medical texts, legal documents, customer reviews, etc., depending on your needs.\n",
        "\n",
        "3. **Continue Training**: The pre-trained model is then trained (or fine-tuned) on this task-specific dataset. During this process, all parts of the model, including the initial word embeddings and the transformer layers that produce the contextualized embeddings, are updated to better align with the specifics of the task or domain.\n",
        "\n",
        "4. **Adjust Learning Rate**: It's common practice to use a smaller learning rate during fine-tuning than was used during the initial pre-training. This helps prevent the model from \"forgetting\" its general understanding of the language while it learns the specifics of the new task.\n",
        "\n",
        "5. **Evaluation and Adjustment**: After fine-tuning, the model is evaluated on a separate validation set to ensure it has effectively adapted to the task. Adjustments may be made to the training process based on this evaluation to improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtItHpQ71KTB"
      },
      "source": [
        "### Benefits of Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Kr-4avVsau"
      },
      "source": [
        "- **Improved Performance**: Fine-tuning allows the model to adapt its pre-learned language representations to the specific lexical and syntactical characteristics of a domain or task, often leading to improved performance compared to using a pre-trained model directly.\n",
        "\n",
        "- **Efficiency**: Because the model has already learned a lot of general language knowledge during pre-training, fine-tuning on a specific task requires relatively less data and computational resources compared to training a model from scratch.\n",
        "\n",
        "- **Flexibility**: This approach is flexible and can be applied across different tasks (e.g., text classification, question answering, named entity recognition) and domains (e.g., finance, healthcare, law) by simply changing the task-specific dataset used for fine-tuning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKIcxnQs1Dh6"
      },
      "source": [
        "### Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgjrifH21Bm3"
      },
      "source": [
        "Fine-tuning contextualized embeddings and transformer models on specific domains or tasks is a powerful technique in NLP. It leverages the broad language understanding acquired during pre-training and specializes the model to perform well on tasks that require more specific knowledge or understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUj6WbygvxMl"
      },
      "source": [
        "## Word Based Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the evolving landscape of Natural Language Processing (NLP), the quest for efficient and effective word representation has been a central pursuit. This journey saw a pivotal turn with the introduction of the Neural Language Model (NLM) by Bengio et al., which laid the groundwork for subsequent innovations in word embedding techniques. Among these, Word2Vec emerged as a groundbreaking approach, heralded for its simplicity and computational efficiency. But what spurred this evolution from Bengio's initial model to the streamlined architecture of Word2Vec?"
      ],
      "metadata": {
        "id": "AFyV5xShGBGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yoshua Bengio and his colleagues proposed a novel idea: to represent words from a vocabulary through learnable vectors within a feed-forward neural network, featuring an embedding layer, hidden layers, and a softmax function. This model was transformative, allowing the embedding layer to generate word embeddings through a shallow network, optimizing the vectors via backpropagation. The architecture was designed to capture the nuances of language, predicting the probability distribution of words across a vast vocabulary."
      ],
      "metadata": {
        "id": "VigOHa1FGGT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the brilliance of Bengio's model came with a computational caveat. The architecture, particularly between the hidden layer and the projection layer, demanded significant computational resources. The reasons were twofold: the projection yielded dense values, and the hidden layer was tasked with computing probability distributions for every word in the vocabulary—a monumental feat for any model."
      ],
      "metadata": {
        "id": "5TlUc7VDGJgM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7krajITZxqHE"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In 2013, Mikolov et al. introduced Word2Vec, a model designed to address the computational challenges of Bengio's NLM. Word2Vec simplified the architecture by eliminating the hidden layer, yet retained the shared projection layer for all words, mirroring Bengio's approach. This simplification meant that, without the complexities of a full neural network, the model might not capture data representations as precisely with smaller datasets. However, with ample data, Word2Vec proved exceptionally adept at accurately modeling language within the embedding space. Moreover, this streamlined model significantly reduced complexity, enabling the processing of larger datasets with greater efficiency.\n",
        "\n",
        "Word2Vec offered two distinct architectures:"
      ],
      "metadata": {
        "id": "E2Uets4SGT5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Continuous Bag-of-Words Model (CBOW)**: This model predicts a target word based on its context, effectively using surrounding words to understand and forecast the focal word."
      ],
      "metadata": {
        "id": "IsCp0IJQHhj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Embeddings/images/cbow.png\" width=\"400\" height=\"400\" alt=\"Continous Bag of Words\">\n",
        "    <figcaption>Continous Bag of Words</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "UFDb2u2THFJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Continuous Skip-gram Model**: In contrast, the Skip-gram model takes a target word to predict its surrounding context, offering a mirror approach to CBOW with its focus on leveraging a word to understand its contextual landscape."
      ],
      "metadata": {
        "id": "HuqSwlp_Hk8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Embeddings/images/skipgram.png\" width=\"400\" height=\"400\" alt=\"Skipgram\">\n",
        "    <figcaption>Skipgram</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "YRfueDIPHFk6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3eryF3Jx2AH"
      },
      "source": [
        "#### Continuous Bag of Words Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shM_TtqAxwOD"
      },
      "source": [
        "#### SkipGram Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWp9_yJLv1fb"
      },
      "source": [
        "## Context Based Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t842V5Wwx_Hj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbyEvR0UV6DH"
      },
      "source": [
        "## Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2T9-c_gl7ni"
      },
      "source": [
        "Embeddings have emerged as fundamental to advancing artificial intelligence, enabling machines to process and understand the vast complexities of human language, visual content, and sound. By continually refining these representations and tailoring them to specific applications, we unlock new potentials for AI to interact with the world in increasingly sophisticated and intuitive ways. The journey of embeddings, from simple vector representations to complex, context-aware models, illustrates the ongoing evolution of machine intelligence and its boundless future prospects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeWSBNxtjUqt"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6MucjOmkwHg"
      },
      "source": [
        "- The Illustrated Word2vec: https://jalammar.github.io/illustrated-word2vec/\n",
        "- For the pre-embedding techniques: [Step by Step Guide to Master NLP – Word Embedding and Text Vectorization](https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/#)\n",
        "- A Neural Probabilistic Language Model: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
        "- Distributed Representations of Words and Phrases\n",
        "and their Compositionality: https://arxiv.org/pdf/1310.4546.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}