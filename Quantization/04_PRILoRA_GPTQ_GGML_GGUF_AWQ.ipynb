{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "\\text{$\\Large “Just\\ improve\\ yourself;\\ that\\ is\\ the\\ only\\ thing\\ you\\ can\\ do\\ to\\ better\\ the\\ world.”$} \\\\\n",
    "{\\text{{$\\small Ludwig\\ Wittgenstein$}}} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Techniques for LLM Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Types of Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Training**:\n",
    "\n",
    "   - **Quantization-Aware Training (QAT)**: During training, the model simulates the effects of quantization. This helps the model learn to be robust to the quantization process. Both weights and activations are quantized, but gradients are typically kept in higher precision to avoid significant loss in training effectiveness.\n",
    "\n",
    "   - **Dynamic Range Quantization**: This method quantizes weights and activations dynamically, usually after each forward pass, to simulate the quantization effect during training.\n",
    "\n",
    "2. **Inference**:\n",
    "\n",
    "   - **Post-Training Quantization**: After training, the model is quantized. This involves:\n",
    "\n",
    "     - **Static Quantization**: Calibrating the model with representative data to determine the optimal quantization parameters.\n",
    "     \n",
    "     - **Dynamic Quantization**: Quantizing weights and dynamically quantizing activations during inference. This is simpler and does not require extensive calibration.\n",
    "\n",
    "   - **QLoRA**: This method involves quantizing the weights to a lower precision format like NF4 or 8-bit integers, allowing for significant memory and computation savings during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRILoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruned and Rank-Increasing Low-Rank Adaptation (PRILoRA)** enhances LoRA by increasing efficiency through two mechanisms: linearly increasing ranks and ongoing importance-based pruning.\n",
    "\n",
    "1. **Linear Rank Increase**: PRILoRA increases the rank linearly across layers, starting with a low rank and increasing it for each subsequent layer.\n",
    "\n",
    "   Neural networks, especially deep ones, process information hierarchically. Lower layers often capture more general features, while higher layers capture more specific features. Starting with a low rank and increasing it in higher layers aligns with this hierarchical nature. Lower layers might not need as much capacity (low rank) to represent general features, whereas higher layers require more capacity (high rank) for complex, specific features.\n",
    "\n",
    "2. **A-Weight Pruning**: It prunes the least significant weights in the A matrix based on an importance matrix, which reduces memory requirements and fine-tuning time.\n",
    "\n",
    "   Neural networks can be memory-intensive, particularly when dealing with large models. Pruning the A matrix by removing the least significant weights helps reduce memory consumption, making the model more efficient and deployable on resource-constrained devices.\n",
    "\n",
    "3. **Importance Matrix?**: An importance matrix is a matrix that quantifies the significance of each weight in the A matrix. It typically reflects how crucial each weight is for the model's performance.\n",
    "\n",
    "   The importance of each weight can be calculated using various methods, such as:\n",
    "   - **Magnitude-Based Methods:** Weights with smaller magnitudes are often considered less important.\n",
    "   - **Gradient-Based Methods:** Weights that contribute less to the gradient (i.e., have smaller gradients) might be deemed less significant.\n",
    "   - **Saliency Scores:** Calculated based on how much the model's output is affected by changes in a particular weight.\n",
    "\n",
    "   The importance matrix is used to guide the pruning process. Weights that are deemed less important (e.g., those with lower scores in the importance matrix) are pruned first. This targeted pruning ensures that the most critical parameters are retained, minimizing the impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranks for each layer: tensor([ 4,  4,  5,  6,  6,  7,  8,  9,  9, 10, 11, 12], dtype=torch.int32)\n",
      "Initialized SimpleNN model with PRILoRA:\n",
      "SimpleNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=4, bias=True)\n",
      "    (1): Linear(in_features=4, out_features=4, bias=True)\n",
      "    (2): Linear(in_features=4, out_features=5, bias=True)\n",
      "    (3): Linear(in_features=5, out_features=6, bias=True)\n",
      "    (4): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (5): Linear(in_features=6, out_features=7, bias=True)\n",
      "    (6): Linear(in_features=7, out_features=8, bias=True)\n",
      "    (7): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (8): Linear(in_features=9, out_features=9, bias=True)\n",
      "    (9): Linear(in_features=9, out_features=10, bias=True)\n",
      "    (10): Linear(in_features=10, out_features=11, bias=True)\n",
      "    (11): Linear(in_features=11, out_features=12, bias=True)\n",
      "    (12): Linear(in_features=12, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "Output of the model before pruning: tensor([[-0.1081,  0.1043, -0.1301, -0.3825]], grad_fn=<AddmmBackward0>)\n",
      "Original Weights (Layer 1): tensor([[-0.1687, -0.1560, -0.0004, -0.2240,  0.0957,  0.0761,  0.1720,  0.2098,\n",
      "         -0.0458,  0.0868, -0.0525, -0.0293,  0.0239,  0.1865,  0.2158,  0.0088],\n",
      "        [ 0.0201, -0.0570, -0.1651,  0.0015,  0.0252, -0.0444,  0.0499,  0.1755,\n",
      "          0.1122, -0.1430, -0.0137,  0.2129, -0.0493,  0.1957, -0.0671,  0.2087],\n",
      "        [ 0.1917, -0.1344, -0.1555,  0.0270, -0.1832, -0.0289,  0.1124,  0.2324,\n",
      "          0.1216,  0.1748, -0.2084, -0.0280, -0.0303,  0.2117, -0.0231, -0.0094],\n",
      "        [-0.1064, -0.1109,  0.2093,  0.2466,  0.1872,  0.0971,  0.1753, -0.2067,\n",
      "          0.2140, -0.1790, -0.0017, -0.0888, -0.0649, -0.0692,  0.0672, -0.2212]])\n",
      "Importance Matrix (Layer 1): tensor([[0.4605, 0.8486, 0.7648, 0.0235, 0.5249, 0.0984, 0.0036, 0.4367, 0.9353,\n",
      "         0.3998, 0.3414, 0.3084, 0.2829, 0.5090, 0.2080, 0.4068],\n",
      "        [0.6671, 0.5332, 0.3183, 0.4322, 0.8245, 0.3768, 0.6079, 0.2266, 0.0596,\n",
      "         0.7388, 0.4405, 0.5700, 0.4723, 0.1867, 0.0131, 0.0900],\n",
      "        [0.6446, 0.9815, 0.2895, 0.1740, 0.0012, 0.9762, 0.9430, 0.0421, 0.6153,\n",
      "         0.5431, 0.4184, 0.8932, 0.7024, 0.3278, 0.4801, 0.6357],\n",
      "        [0.6354, 0.7949, 0.5317, 0.9143, 0.3730, 0.6818, 0.6353, 0.1720, 0.4987,\n",
      "         0.1055, 0.6637, 0.6715, 0.9945, 0.4378, 0.5625, 0.0929]])\n",
      "Pruned Weights (Layer 1): tensor([[-0.1687, -0.1560, -0.0004,  0.0000,  0.0957,  0.0000,  0.0000,  0.2098,\n",
      "         -0.0458,  0.0868, -0.0525, -0.0293,  0.0239,  0.1865,  0.2158,  0.0088],\n",
      "        [ 0.0201, -0.0570, -0.1651,  0.0015,  0.0252, -0.0444,  0.0499,  0.1755,\n",
      "          0.0000, -0.1430, -0.0137,  0.2129, -0.0493,  0.1957,  0.0000,  0.0000],\n",
      "        [ 0.1917, -0.1344, -0.1555,  0.0270,  0.0000, -0.0289,  0.1124,  0.0000,\n",
      "          0.1216,  0.1748, -0.2084, -0.0280, -0.0303,  0.2117, -0.0231, -0.0094],\n",
      "        [-0.1064, -0.1109,  0.2093,  0.2466,  0.1872,  0.0971,  0.1753, -0.2067,\n",
      "          0.2140,  0.0000, -0.0017, -0.0888, -0.0649, -0.0692,  0.0672,  0.0000]])\n",
      "Output of the model after pruning: tensor([[-0.1081,  0.1043, -0.1301, -0.3825]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "PRILoRA Explanation:\n",
      "\n",
      "1. Linear Rank Increase: Ranks for each layer are linearly increased from 4 to 12\n",
      "   This allows lower layers to use fewer parameters while higher layers can use more parameters.\n",
      "\n",
      "2. A-Weight Pruning: Least significant weights in the A matrix are pruned based on an importance matrix.\n",
      "   This reduces memory requirements and fine-tuning time while maintaining model performance.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a function to increase rank linearly across layers\n",
    "def increase_rank_linearly(layers, start_rank, end_rank):\n",
    "    \"\"\"\n",
    "    Generate ranks linearly increasing from start_rank to end_rank across the specified number of layers.\n",
    "    \n",
    "    Args:\n",
    "    layers (int): Number of layers.\n",
    "    start_rank (int): Initial rank at the first layer.\n",
    "    end_rank (int): Final rank at the last layer.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the ranks for each layer.\n",
    "    \"\"\"\n",
    "    ranks = torch.linspace(start_rank, end_rank, steps=layers).int()\n",
    "    return ranks\n",
    "\n",
    "# Define a function to prune least significant weights\n",
    "def prune_weights(matrix, importance_matrix, prune_step):\n",
    "    \"\"\"\n",
    "    Prune the least significant weights in the matrix based on the importance matrix.\n",
    "    \n",
    "    Args:\n",
    "    matrix (torch.Tensor): The weight matrix to be pruned.\n",
    "    importance_matrix (torch.Tensor): The matrix indicating the importance of each weight.\n",
    "    prune_step (int): The number of weights to prune in each step.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The pruned weight matrix.\n",
    "    \"\"\"\n",
    "    for _ in range(prune_step):\n",
    "        # Find the least significant weight (smallest importance)\n",
    "        min_importance, indices = torch.min(importance_matrix.view(-1), dim=0)\n",
    "        # Convert the flat index back to 2D index\n",
    "        index = torch.tensor([indices // importance_matrix.size(1), indices % importance_matrix.size(1)])\n",
    "        # Prune (set to zero) the least significant weight\n",
    "        matrix[index[0], index[1]] = 0\n",
    "        # Update the importance matrix to avoid pruning the same weight again\n",
    "        importance_matrix[index[0], index[1]] = float('inf')\n",
    "    return matrix\n",
    "\n",
    "# Define a simple neural network for demonstration\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_ranks):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for rank in layer_ranks:\n",
    "            self.layers.append(nn.Linear(input_size, rank))\n",
    "            input_size = rank\n",
    "        self.layers.append(nn.Linear(input_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Example usage of the SimpleNN model with PRILoRA\n",
    "layers = 12\n",
    "start_rank = 4\n",
    "end_rank = 12\n",
    "\n",
    "# Generate linearly increasing ranks for each layer\n",
    "ranks = increase_rank_linearly(layers, start_rank, end_rank)\n",
    "print(\"Ranks for each layer:\", ranks)\n",
    "\n",
    "# Define input and output sizes\n",
    "input_size = 16\n",
    "output_size = 4\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN(input_size, output_size, ranks)\n",
    "print(\"Initialized SimpleNN model with PRILoRA:\")\n",
    "\n",
    "# Display model architecture\n",
    "print(model)\n",
    "\n",
    "# Example input tensor\n",
    "input_tensor = torch.randn(1, input_size)\n",
    "\n",
    "# Forward pass through the model\n",
    "output_tensor = model(input_tensor)\n",
    "print(\"Output of the model before pruning:\", output_tensor)\n",
    "\n",
    "# Example weights and importance matrix for pruning\n",
    "weights = model.layers[0].weight.data\n",
    "importance_matrix = torch.rand(weights.size())\n",
    "print(\"Original Weights (Layer 1):\", weights)\n",
    "print(\"Importance Matrix (Layer 1):\", importance_matrix)\n",
    "\n",
    "# Prune weights based on importance\n",
    "pruned_weights = prune_weights(weights, importance_matrix, prune_step=10)\n",
    "print(\"Pruned Weights (Layer 1):\", pruned_weights)\n",
    "\n",
    "# Reassign the pruned weights back to the model\n",
    "model.layers[0].weight.data = pruned_weights\n",
    "\n",
    "# Forward pass through the model after pruning\n",
    "output_tensor_after_pruning = model(input_tensor)\n",
    "print(\"Output of the model after pruning:\", output_tensor_after_pruning)\n",
    "\n",
    "# Explanation of PRILoRA\n",
    "print(\"\\nPRILoRA Explanation:\")\n",
    "print(\"\\n1. Linear Rank Increase: Ranks for each layer are linearly increased from\", start_rank, \"to\", end_rank)\n",
    "print(\"   This allows lower layers to use fewer parameters while higher layers can use more parameters.\")\n",
    "print(\"\\n2. A-Weight Pruning: Least significant weights in the A matrix are pruned based on an importance matrix.\")\n",
    "print(\"   This reduces memory requirements and fine-tuning time while maintaining model performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Pre-Trained Transformer Quantization (GPTQ) is an advanced technique that enhances the inference speed and reduces the memory footprint of transformer-based models like GPT, by quantizing the model parameters. The process involves fine-tuning a pre-trained model with quantization-aware training to maintain performance while reducing the model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Pre-Trained Transformer Quantization (GPTQ)** is a layer-wise quantization method aimed at minimizing output error through mean squared error (MSE).\n",
    "\n",
    "1. **Lazy Batch Updating**:\n",
    "- **Concept**: Instead of quantizing the entire model all at once, weights are processed in smaller groups or batches.\n",
    "- **Process**:\n",
    "  - Weights are divided into manageable batches.\n",
    "  - Each batch is quantized individually.\n",
    "  - The mean squared error (MSE) between the original and quantized weights is computed.\n",
    "  - The quantized weights are updated based on the MSE to ensure that the output error is minimized.\n",
    "\n",
    "2. **Mixed INT4/FP16 Quantization**:\n",
    "  - **INT4 (4-bit integers)**: Model weights are quantized to 4-bit integers. This significantly reduces the memory and computational requirements.\n",
    "  - **FP16 (16-bit floating point)**: Activations (the intermediate outputs of the network) remain in 16-bit floating point format. This ensures that during inference, the model maintains a high level of precision and accuracy.\n",
    "\n",
    "\n",
    "GPTQ is a sophisticated quantization method that achieves a balance between efficiency and accuracy by combining low-bit weight quantization with higher precision activation representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key differences between GPTQ and QLoRA**:\n",
    "\n",
    "1. **Quantization Techniques**:\n",
    "   - GPTQ primarily focuses on the quantization of weights to INT4 while keeping activations in FP16.\n",
    "   - QLoRA employs both quantization and low-rank adaptation, often quantizing weights and activations to lower precision levels like INT8 or lower.\n",
    "\n",
    "2. **Adaptation Method**:\n",
    "   - GPTQ does not inherently include low-rank adaptation; it focuses on batch-wise quantization and MSE minimization.\n",
    "   - QLoRA combines low-rank matrix factorization with quantization, enabling efficient adaptation and fine-tuning.\n",
    "\n",
    "3. **Application Scenarios**:\n",
    "   - GPTQ is well-suited for scenarios where maintaining activation precision is critical, and the primary goal is to reduce model size through weight quantization.\n",
    "   - QLoRA is designed for environments where fine-tuning pre-trained models with minimal computational resources is essential, leveraging both quantization and low-rank adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see a simplified pseudo-code to guide through applying GPTQ to a pretrained transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your pre-trained transformer model\n",
    "model = load_pretrained_model('model_name')\n",
    "\n",
    "# Step 2: Define the quantization configuration\n",
    "quant_config = {\n",
    "    'num_bits_weights': 4,             # Number of bits for weights (INT4)\n",
    "    'num_bits_activations': 16,        # Number of bits for activations (FP16)\n",
    "    'batch_size': 1000                 # Number of weights to quantize at once\n",
    "}\n",
    "\n",
    "# Step 3: Apply quantization-aware training setup\n",
    "quantized_model = setup_quantization(model, quant_config)\n",
    "\n",
    "def setup_quantization(model, quant_config):\n",
    "    # Step 3.1: Define quantization-aware layers\n",
    "    model = replace_with_quantized_layers(model, quant_config)\n",
    "    \n",
    "    # Step 3.2: Lazy Batch Updating for weights\n",
    "    model = apply_lazy_batch_updating(model, quant_config)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Replace standard layers with quantization-aware versions\n",
    "def replace_with_quantized_layers(model, quant_config):\n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            quant_layer = QuantizedLinear(layer, quant_config)\n",
    "            setattr(model, name, quant_layer)\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            quant_layer = QuantizedConv2d(layer, quant_config)\n",
    "            setattr(model, name, quant_layer)\n",
    "    return model\n",
    "\n",
    "# Quantization-aware Linear Layer\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, layer, quant_config):\n",
    "        super(QuantizedLinear, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.num_bits_weights = quant_config['num_bits_weights']\n",
    "        self.num_bits_activations = quant_config['num_bits_activations']\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Quantize weights to INT4\n",
    "        self.layer.weight.data = quantize(self.layer.weight.data, self.num_bits_weights)\n",
    "        # Keep activations in FP16\n",
    "        x = x.half()\n",
    "        output = self.layer(x)\n",
    "        return output.float()\n",
    "\n",
    "# Quantization-aware Conv2d Layer\n",
    "class QuantizedConv2d(nn.Module):\n",
    "    def __init__(self, layer, quant_config):\n",
    "        super(QuantizedConv2d, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.num_bits_weights = quant_config['num_bits_weights']\n",
    "        self.num_bits_activations = quant_config['num_bits_activations']\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Quantize weights to INT4\n",
    "        self.layer.weight.data = quantize(self.layer.weight.data, self.num_bits_weights)\n",
    "        # Keep activations in FP16\n",
    "        x = x.half()\n",
    "        output = self.layer(x)\n",
    "        return output.float()\n",
    "\n",
    "# Lazy Batch Updating\n",
    "def apply_lazy_batch_updating(model, quant_config):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Divide weights into batches\n",
    "            weight_batches = torch.split(param.data, quant_config['batch_size'])\n",
    "            quantized_batches = []\n",
    "            for batch in weight_batches:\n",
    "                quantized_batch = quantize(batch, quant_config['num_bits_weights'])\n",
    "                mse = ((batch - quantized_batch) ** 2).mean()\n",
    "                updated_batch = update_weights_based_on_mse(batch, quantized_batch, mse)\n",
    "                quantized_batches.append(updated_batch)\n",
    "            param.data = torch.cat(quantized_batches)\n",
    "    return model\n",
    "\n",
    "# Helper functions for quantization, dequantization, and weight update\n",
    "def quantize(tensor, num_bits):\n",
    "    scale = (tensor.max() - tensor.min()) / (2**num_bits - 1)\n",
    "    quantized_tensor = ((tensor - tensor.min()) / scale).round() * scale + tensor.min()\n",
    "    return quantized_tensor\n",
    "\n",
    "def dequantize(tensor, num_bits):\n",
    "    scale = (tensor.max() - tensor.min()) / (2**num_bits - 1)\n",
    "    dequantized_tensor = tensor * scale + tensor.min()\n",
    "    return dequantized_tensor\n",
    "\n",
    "def update_weights_based_on_mse(original, quantized, mse):\n",
    "    # Update logic to minimize MSE\n",
    "    return updated_weights\n",
    "\n",
    "# Step 4: Fine-tune the quantized model\n",
    "for epoch in range(total_epochs):\n",
    "    for batch_data in training_data:\n",
    "        loss = train_step(quantized_model, batch_data)\n",
    "        update_model(quantized_model, loss)\n",
    "\n",
    "# Step 5: Evaluate the quantized model\n",
    "evaluate_model(quantized_model, validation_data)\n",
    "\n",
    "# Step 6: Save or deploy the quantized model\n",
    "save_quantized_model(quantized_model, 'quantized_model_name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGML/GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GGML (Georgi Gerganov Machine Learning)** and **GGUF (GPT-Generated Unified Format)** are designed for the quantization of Llama models to run on CPUs.\n",
    "\n",
    "1. **k-Quant System**: Weights are divided into blocks and quantized with different bit widths depending on the quant method.\n",
    "\n",
    "2. **Quant Methods**: Various methods convert weights to different precision levels, such as q2_k for 2-bit and 4-bit integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Weights: tensor([[-1.3340, -0.8287, -0.8182,  ...,  0.9366,  1.2391, -0.5770],\n",
      "        [ 0.0329, -0.4731,  0.0298,  ..., -1.2462,  0.4124,  1.4921],\n",
      "        [-0.3709,  0.6167, -0.2904,  ..., -0.0169, -0.0743,  0.6046],\n",
      "        ...,\n",
      "        [-0.3844, -1.5232,  0.4136,  ...,  0.8508,  1.2860,  0.8044],\n",
      "        [-0.4893,  0.7189, -1.0660,  ...,  0.9606,  0.0521, -1.5387],\n",
      "        [-1.7366,  1.1037, -1.0770,  ..., -1.4119, -2.2881,  0.0177]])\n",
      "K-Quant Weights (4-bit): tensor([[-1.3111, -0.7867, -0.7867,  ...,  1.0489,  1.3111, -0.5244],\n",
      "        [ 0.0000, -0.5244,  0.0000,  ..., -1.3111,  0.5244,  1.5733],\n",
      "        [-0.2622,  0.5244, -0.2622,  ..., -0.0000, -0.0000,  0.5244],\n",
      "        ...,\n",
      "        [-0.2622, -1.5733,  0.5244,  ...,  0.7867,  1.3111,  0.7867],\n",
      "        [-0.5244,  0.7867, -1.0489,  ...,  1.0489,  0.0000, -1.5733],\n",
      "        [-1.8356,  1.0489, -1.0489,  ..., -1.3111, -2.3600,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define k-quant function\n",
    "def k_quant(weights, bit_width):\n",
    "    # Calculate scaling factor based on the largest weight\n",
    "    scale = torch.max(torch.abs(weights))\n",
    "    # Quantize weights to specified bit width\n",
    "    quant_weights = torch.round(weights / scale * (2 ** bit_width - 1)) / (2 ** bit_width - 1) * scale\n",
    "    return quant_weights\n",
    "\n",
    "# Example usage\n",
    "weights = torch.randn(32, 32)\n",
    "print(\"Original Weights:\", weights)\n",
    "\n",
    "# Quantize weights using k-quant method\n",
    "k_quant_weights = k_quant(weights, 4)\n",
    "print(\"K-Quant Weights (4-bit):\", k_quant_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation-Aware Weight Quantization (AWQ)** tailors the precision of weights based on the activations during inference, skipping over salient weights to minimize accuracy loss.\n",
    "\n",
    "1. **Calibration**: Collect activation statistics to identify salient weights.\n",
    "\n",
    "2. **Selective Quantization**: Salient weights remain in FP16, while others are quantized to INT3 or INT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define function to collect activation statistics\n",
    "def collect_activation_statistics(model, data_loader):\n",
    "    activation_stats = []\n",
    "    for data in data_loader:\n",
    "        output = model(data)\n",
    "        activation_stats.append(output)\n",
    "    return torch.cat(activation_stats)\n",
    "\n",
    "# Define function to quantize based on activations\n",
    "def activation_aware_quantize(weights, activations):\n",
    "    # Calculate threshold for salient weights\n",
    "    threshold = torch.mean(activations) + torch.std(activations)\n",
    "    mask = activations > threshold\n",
    "    # Skip salient weights and quantize the rest\n",
    "    quant_weights = torch.where(mask, weights, torch.round(weights / 7.5) * 7.5)\n",
    "    return quant_weights\n",
    "\n",
    "# Example usage\n",
    "model = nn.Linear(10, 10)\n",
    "data_loader = [torch.randn(10) for _ in range(100)]\n",
    "activation_stats = collect_activation_statistics(model, data_loader)\n",
    "print(\"Activation Statistics:\", activation_stats)\n",
    "\n",
    "weights = model.weight.data\n",
    "print(\"Original Weights:\", weights)\n",
    "\n",
    "# Apply activation-aware quantization\n",
    "quant_weights = activation_aware_quantize(weights, activation_stats)\n",
    "print(\"Quantized Weights with AWQ:\", quant_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
