{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "\\text{$\\Large “Just\\ improve\\ yourself;\\ that\\ is\\ the\\ only\\ thing\\ you\\ can\\ do\\ to\\ better\\ the\\ world.”$} \\\\\n",
    "{\\text{{$\\small Ludwig\\ Wittgenstein$}}} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Techniques for LLM Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Types of Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Training**:\n",
    "\n",
    "   - **Quantization-Aware Training (QAT)**: During training, the model simulates the effects of quantization. This helps the model learn to be robust to the quantization process. Both weights and activations are quantized, but gradients are typically kept in higher precision to avoid significant loss in training effectiveness.\n",
    "\n",
    "   - **Dynamic Range Quantization**: This method quantizes weights and activations dynamically, usually after each forward pass, to simulate the quantization effect during training.\n",
    "\n",
    "2. **Inference**:\n",
    "\n",
    "   - **Post-Training Quantization**: After training, the model is quantized. This involves:\n",
    "\n",
    "     - **Static Quantization**: Calibrating the model with representative data to determine the optimal quantization parameters.\n",
    "     \n",
    "     - **Dynamic Quantization**: Quantizing weights and dynamically quantizing activations during inference. This is simpler and does not require extensive calibration.\n",
    "\n",
    "   - **QLoRA**: This method involves quantizing the weights to a lower precision format like NF4 or 8-bit integers, allowing for significant memory and computation savings during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TheBloke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization and sharding (splitting the large models into smaller chunks) every time for a new model is not an easy task. Instead, these models have often already been sharded and quantized for us to use. [TheBloke](https://huggingface.co/TheBloke) in particular is a user on HuggingFace that performs a bunch of quantizations for us to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Quantization/images/thebloke.png\" width=\"600\" height=\"600\">\n",
    "    <figcaption>Models already sharded and quantized</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRILoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruned and Rank-Increasing Low-Rank Adaptation (PRILoRA)** enhances LoRA by increasing efficiency through two mechanisms: linearly increasing ranks and ongoing importance-based pruning.\n",
    "\n",
    "1. **Linear Rank Increase**: PRILoRA increases the rank linearly across layers, starting with a low rank and increasing it for each subsequent layer.\n",
    "\n",
    "   Neural networks, especially deep ones, process information hierarchically. Lower layers often capture more general features, while higher layers capture more specific features. Starting with a low rank and increasing it in higher layers aligns with this hierarchical nature. Lower layers might not need as much capacity (low rank) to represent general features, whereas higher layers require more capacity (high rank) for complex, specific features.\n",
    "\n",
    "2. **A-Weight Pruning**: It prunes the least significant weights in the A matrix based on an importance matrix, which reduces memory requirements and fine-tuning time.\n",
    "\n",
    "   Neural networks can be memory-intensive, particularly when dealing with large models. Pruning the A matrix by removing the least significant weights helps reduce memory consumption, making the model more efficient and deployable on resource-constrained devices.\n",
    "\n",
    "3. **Importance Matrix?**: An importance matrix is a matrix that quantifies the significance of each weight in the A matrix. It typically reflects how crucial each weight is for the model's performance.\n",
    "\n",
    "   The importance of each weight can be calculated using various methods, such as:\n",
    "   - **Magnitude-Based Methods:** Weights with smaller magnitudes are often considered less important.\n",
    "   - **Gradient-Based Methods:** Weights that contribute less to the gradient (i.e., have smaller gradients) might be deemed less significant.\n",
    "   - **Saliency Scores:** Calculated based on how much the model's output is affected by changes in a particular weight.\n",
    "\n",
    "   The importance matrix is used to guide the pruning process. Weights that are deemed less important (e.g., those with lower scores in the importance matrix) are pruned first. This targeted pruning ensures that the most critical parameters are retained, minimizing the impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranks for each layer: tensor([ 4,  4,  5,  6,  6,  7,  8,  9,  9, 10, 11, 12], dtype=torch.int32)\n",
      "Initialized SimpleNN model with PRILoRA:\n",
      "SimpleNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=4, bias=True)\n",
      "    (1): Linear(in_features=4, out_features=4, bias=True)\n",
      "    (2): Linear(in_features=4, out_features=5, bias=True)\n",
      "    (3): Linear(in_features=5, out_features=6, bias=True)\n",
      "    (4): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (5): Linear(in_features=6, out_features=7, bias=True)\n",
      "    (6): Linear(in_features=7, out_features=8, bias=True)\n",
      "    (7): Linear(in_features=8, out_features=9, bias=True)\n",
      "    (8): Linear(in_features=9, out_features=9, bias=True)\n",
      "    (9): Linear(in_features=9, out_features=10, bias=True)\n",
      "    (10): Linear(in_features=10, out_features=11, bias=True)\n",
      "    (11): Linear(in_features=11, out_features=12, bias=True)\n",
      "    (12): Linear(in_features=12, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "Output of the model before pruning: tensor([[-0.2037,  0.1573, -0.1325,  0.1433]], grad_fn=<AddmmBackward0>)\n",
      "Original Weights (Layer 1): tensor([[-0.1853,  0.1752,  0.1427, -0.1259,  0.0520,  0.1262, -0.2172,  0.2457,\n",
      "          0.1364, -0.2338,  0.1917,  0.1443, -0.1269, -0.0404, -0.1381, -0.0482],\n",
      "        [-0.2217,  0.2132, -0.2500,  0.0448, -0.0704,  0.0878, -0.1748, -0.0398,\n",
      "          0.2219, -0.1470,  0.0672,  0.1453, -0.2366,  0.1958, -0.2376, -0.1721],\n",
      "        [-0.1484, -0.0516,  0.0330, -0.0318,  0.2098,  0.2197,  0.1515,  0.0018,\n",
      "          0.2342, -0.0665, -0.0716,  0.0140, -0.1073, -0.1496, -0.0390,  0.1285],\n",
      "        [ 0.0810,  0.2236, -0.1489, -0.0503, -0.0477, -0.1958,  0.2079,  0.1464,\n",
      "          0.1455,  0.1824,  0.0656,  0.0742,  0.1625, -0.2187,  0.0966,  0.0064]])\n",
      "Importance Matrix (Layer 1): tensor([[0.8381, 0.8769, 0.9843, 0.7670, 0.3528, 0.3294, 0.4206, 0.5092, 0.1683,\n",
      "         0.3971, 0.2507, 0.0600, 0.7947, 0.7010, 0.2579, 0.1768],\n",
      "        [0.3236, 0.1162, 0.8937, 0.5498, 0.9970, 0.1867, 0.5087, 0.4022, 0.2284,\n",
      "         0.2624, 0.9357, 0.1158, 0.0190, 0.4146, 0.8704, 0.9244],\n",
      "        [0.8157, 0.1329, 0.6516, 0.6978, 0.4015, 0.0466, 0.6962, 0.2531, 0.1325,\n",
      "         0.6223, 0.5051, 0.7052, 0.5656, 0.3648, 0.4522, 0.7190],\n",
      "        [0.6492, 0.9133, 0.6285, 0.1551, 0.6970, 0.1401, 0.9918, 0.4462, 0.3468,\n",
      "         0.6508, 0.7745, 0.5006, 0.7119, 0.8990, 0.9043, 0.1304]])\n",
      "Pruned Weights (Layer 1): tensor([[-0.1853,  0.1752,  0.1427, -0.1259,  0.0520,  0.1262, -0.2172,  0.2457,\n",
      "          0.1364, -0.2338,  0.1917,  0.0000, -0.1269, -0.0404, -0.1381, -0.0482],\n",
      "        [-0.2217,  0.0000, -0.2500,  0.0448, -0.0704,  0.0878, -0.1748, -0.0398,\n",
      "          0.2219, -0.1470,  0.0672,  0.0000,  0.0000,  0.1958, -0.2376, -0.1721],\n",
      "        [-0.1484,  0.0000,  0.0330, -0.0318,  0.2098,  0.0000,  0.1515,  0.0018,\n",
      "          0.0000, -0.0665, -0.0716,  0.0140, -0.1073, -0.1496, -0.0390,  0.1285],\n",
      "        [ 0.0810,  0.2236, -0.1489,  0.0000, -0.0477,  0.0000,  0.2079,  0.1464,\n",
      "          0.1455,  0.1824,  0.0656,  0.0742,  0.1625, -0.2187,  0.0966,  0.0000]])\n",
      "Output of the model after pruning: tensor([[-0.2037,  0.1573, -0.1325,  0.1433]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "PRILoRA Explanation:\n",
      "\n",
      "1. Linear Rank Increase: Ranks for each layer are linearly increased from 4 to 12\n",
      "   This allows lower layers to use fewer parameters while higher layers can use more parameters.\n",
      "\n",
      "2. A-Weight Pruning: Least significant weights in the A matrix are pruned based on an importance matrix.\n",
      "   This reduces memory requirements and fine-tuning time while maintaining model performance.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a function to increase rank linearly across layers\n",
    "def increase_rank_linearly(layers, start_rank, end_rank):\n",
    "    \"\"\"\n",
    "    Generate ranks linearly increasing from start_rank to end_rank across the specified number of layers.\n",
    "    \n",
    "    Args:\n",
    "    layers (int): Number of layers.\n",
    "    start_rank (int): Initial rank at the first layer.\n",
    "    end_rank (int): Final rank at the last layer.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: A tensor containing the ranks for each layer.\n",
    "    \"\"\"\n",
    "    ranks = torch.linspace(start_rank, end_rank, steps=layers).int()\n",
    "    return ranks\n",
    "\n",
    "# Define a function to prune least significant weights\n",
    "def prune_weights(matrix, importance_matrix, prune_step):\n",
    "    \"\"\"\n",
    "    Prune the least significant weights in the matrix based on the importance matrix.\n",
    "    \n",
    "    Args:\n",
    "    matrix (torch.Tensor): The weight matrix to be pruned.\n",
    "    importance_matrix (torch.Tensor): The matrix indicating the importance of each weight.\n",
    "    prune_step (int): The number of weights to prune in each step.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The pruned weight matrix.\n",
    "    \"\"\"\n",
    "    for _ in range(prune_step):\n",
    "        # Find the least significant weight (smallest importance)\n",
    "        min_importance, indices = torch.min(importance_matrix.view(-1), dim=0)\n",
    "        # Convert the flat index back to 2D index\n",
    "        index = torch.tensor([indices // importance_matrix.size(1), indices % importance_matrix.size(1)])\n",
    "        # Prune (set to zero) the least significant weight\n",
    "        matrix[index[0], index[1]] = 0\n",
    "        # Update the importance matrix to avoid pruning the same weight again\n",
    "        importance_matrix[index[0], index[1]] = float('inf')\n",
    "    return matrix\n",
    "\n",
    "# Define a simple neural network for demonstration\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_ranks):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for rank in layer_ranks:\n",
    "            self.layers.append(nn.Linear(input_size, rank))\n",
    "            input_size = rank\n",
    "        self.layers.append(nn.Linear(input_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Example usage of the SimpleNN model with PRILoRA\n",
    "layers = 12\n",
    "start_rank = 4\n",
    "end_rank = 12\n",
    "\n",
    "# Generate linearly increasing ranks for each layer\n",
    "ranks = increase_rank_linearly(layers, start_rank, end_rank)\n",
    "print(\"Ranks for each layer:\", ranks)\n",
    "\n",
    "# Define input and output sizes\n",
    "input_size = 16\n",
    "output_size = 4\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN(input_size, output_size, ranks)\n",
    "print(\"Initialized SimpleNN model with PRILoRA:\")\n",
    "\n",
    "# Display model architecture\n",
    "print(model)\n",
    "\n",
    "# Example input tensor\n",
    "input_tensor = torch.randn(1, input_size)\n",
    "\n",
    "# Forward pass through the model\n",
    "output_tensor = model(input_tensor)\n",
    "print(\"Output of the model before pruning:\", output_tensor)\n",
    "\n",
    "# Example weights and importance matrix for pruning\n",
    "weights = model.layers[0].weight.data\n",
    "importance_matrix = torch.rand(weights.size())\n",
    "print(\"Original Weights (Layer 1):\", weights)\n",
    "print(\"Importance Matrix (Layer 1):\", importance_matrix)\n",
    "\n",
    "# Prune weights based on importance\n",
    "pruned_weights = prune_weights(weights, importance_matrix, prune_step=10)\n",
    "print(\"Pruned Weights (Layer 1):\", pruned_weights)\n",
    "\n",
    "# Reassign the pruned weights back to the model\n",
    "model.layers[0].weight.data = pruned_weights\n",
    "\n",
    "# Forward pass through the model after pruning\n",
    "output_tensor_after_pruning = model(input_tensor)\n",
    "print(\"Output of the model after pruning:\", output_tensor_after_pruning)\n",
    "\n",
    "# Explanation of PRILoRA\n",
    "print(\"\\nPRILoRA Explanation:\")\n",
    "print(\"\\n1. Linear Rank Increase: Ranks for each layer are linearly increased from\", start_rank, \"to\", end_rank)\n",
    "print(\"   This allows lower layers to use fewer parameters while higher layers can use more parameters.\")\n",
    "print(\"\\n2. A-Weight Pruning: Least significant weights in the A matrix are pruned based on an importance matrix.\")\n",
    "print(\"   This reduces memory requirements and fine-tuning time while maintaining model performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Pre-Trained Transformer Quantization (GPTQ) is an advanced technique that enhances the inference speed and reduces the memory footprint of transformer-based models like GPT, by quantizing the model parameters. GPTQ falls into the PTQ (Post-Training Quantization) category and this is particularly interesting for massive models, for which full model training or even fine-tuning can be very expensive.\n",
    "\n",
    "Specifically, GPTQ adopts a mixed int4/fp16 quantization scheme where weights are quantized as int4 while activations remain in float16. During inference, weights are dequantized on the fly and the actual compute is performed in float16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPTQ is a layer-wise quantization method aimed at minimizing output error through mean squared error (MSE).\n",
    "\n",
    "1. **Lazy Batch Updating**:\n",
    "- Instead of quantizing the entire model all at once, weights are processed in smaller groups or batches.\n",
    "- **Process**:\n",
    "- The GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse (a matrix that helps decide how to adjust the weights)\n",
    "- It then runs in loops, handling batches of columns at a time.\n",
    "- For each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly.\n",
    "- After processing the calibration batch, all the remaining weights in the matrix are updated in accordance with the MSE of the initial batch.\n",
    "\n",
    "  First, all the model’s weights are converted into a matrix, which is worked through in batches of 128 columns at a time through a process called lazy batch updating. This involves quantizing the weights in batch, calculating the MSE, and updating the weights to values that diminish it. After processing the calibration batch, all the remaining weights in the matrix are updated in accordance with the MSE of the initial batch – and then all the individual layers are re-combined to produce a quantized model. \n",
    "\n",
    "2. **Mixed INT4/FP16 Quantization**:\n",
    "  - **INT4 (4-bit integers)**: Model weights are quantized to 4-bit integers. This significantly reduces the memory and computational requirements.\n",
    "  - **FP16 (16-bit floating point)**: Activations (the intermediate outputs of the network) remain in 16-bit floating point format. This ensures that during inference, the model maintains a high level of precision and accuracy.\n",
    "\n",
    "GPTQ has also been integrated into Hugging Face via the [AutoGPTQ](https://huggingface.co/blog/gptq-integration#a-gentle-summary-of-the-gptq-paper) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key differences between GPTQ and QLoRA**:\n",
    "\n",
    "1. **Quantization Techniques**:\n",
    "   - GPTQ primarily focuses on the quantization of weights to INT4 while keeping activations in FP16.\n",
    "   - QLoRA employs both quantization and low-rank adaptation, often quantizing weights and activations to lower precision levels like INT8 or lower.\n",
    "\n",
    "2. **Adaptation Method**:\n",
    "   - GPTQ does not inherently include low-rank adaptation; it focuses on batch-wise quantization and MSE minimization.\n",
    "   - QLoRA combines low-rank matrix factorization with quantization, enabling efficient adaptation and fine-tuning.\n",
    "\n",
    "3. **Application Scenarios**:\n",
    "   - GPTQ is well-suited for scenarios where maintaining activation precision is critical, and the primary goal is to reduce model size through weight quantization.\n",
    "   - QLoRA is designed for environments where fine-tuning pre-trained models with minimal computational resources is essential, leveraging both quantization and low-rank adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see a simplified pseudo-code to guide through applying GPTQ to a pretrained transformer model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing a GPT-2 Model with AutoGPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Install Required Packages**\n",
    "\n",
    "First, install the necessary packages. Make sure you have CUDA-enabled hardware for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Import Libraries and Define Configuration**\n",
    "\n",
    "Next, import the required libraries and define the model and output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the model ID and output directory for the quantized model\n",
    "model_id = \"gpt2\"\n",
    "output_directory = model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Load the Model and Tokenizer**\n",
    "\n",
    "Load the tokenizer using the `AutoTokenizer` class and the model with a specific quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Load pretrained model to do quantization requires CUDA available.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the model with the quantization configuration\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model and tokenizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoGPTQForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantize_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and tokenizer loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.11/site-packages/auto_gptq/modeling/auto.py:76\u001b[0m, in \u001b[0;36mAutoGPTQForCausalLM.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, quantize_config, max_memory, trust_remote_code, **model_init_kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_init_kwargs,\n\u001b[1;32m     74\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseGPTQForCausalLM:\n\u001b[1;32m     75\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m check_and_get_model_type(pretrained_model_name_or_path, trust_remote_code)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPTQ_CAUSAL_LM_MODEL_MAP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantize_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.11/site-packages/auto_gptq/modeling/_base.py:716\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, quantize_config, max_memory, trust_remote_code, torch_dtype, **model_init_kwargs)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"load un-quantized pretrained model to cpu\"\"\"\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad pretrained model to do quantization requires CUDA available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mskip\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Load pretrained model to do quantization requires CUDA available."
     ]
    }
   ],
   "source": [
    "# Define the quantization configuration\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,           # Number of bits for quantization\n",
    "    group_size=128,   # Group size for lazy batch quantization\n",
    "    damp_percent=0.01, # Damping percentage for Cholesky reformulation\n",
    "    desc_act=False    # Disable descending activation order for simplicity\n",
    ")\n",
    "\n",
    "# Load the model with the quantization configuration\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(\"Model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Prepare the Dataset**\n",
    "\n",
    "Use the C4 dataset to generate samples for the quantization process. Tokenize the dataset and format the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Number of samples to use for quantization\n",
    "n_samples = 1024\n",
    "\n",
    "# Load the C4 dataset\n",
    "print(\"Loading and tokenizing the C4 dataset...\")\n",
    "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
    "print(\"Dataset tokenized.\")\n",
    "\n",
    "# Format tokenized examples\n",
    "examples = []\n",
    "for _ in range(n_samples):\n",
    "    # Randomly select a segment from the tokenized data\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    \n",
    "    # Extract input IDs and create attention mask\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    # Append the example to the list\n",
    "    examples.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "print(f\"Formatted {n_samples} examples for quantization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Quantize the Model**\n",
    "\n",
    "Start the quantization process with a batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Quantize the model using GPTQ\n",
    "print(\"Starting the quantization process...\")\n",
    "model.quantize(\n",
    "    examples,\n",
    "    batch_size=1,    # Batch size for quantization\n",
    "    use_triton=True, # Use Triton for GPU acceleration\n",
    ")\n",
    "\n",
    "# Save the quantized model and tokenizer\n",
    "print(f\"Saving quantized model to {output_directory}...\")\n",
    "model.save_quantized(output_directory, use_safetensors=True)\n",
    "tokenizer.save_pretrained(output_directory)\n",
    "print(\"Quantized model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Reload and Test the Quantized Model**\n",
    "\n",
    "Reload the quantized model and tokenizer, then test it with a text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Reload the quantized model and tokenizer\n",
    "print(\"Reloading the quantized model and tokenizer...\")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    output_directory,\n",
    "    device=device,\n",
    "    use_triton=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_directory)\n",
    "print(\"Quantized model and tokenizer reloaded.\")\n",
    "\n",
    "# Test the quantized model using a text generation pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Testing the quantized model with text generation...\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "result = generator(\"I have a dream\", do_sample=True, max_length=50)[0]['generated_text']\n",
    "print(\"Generated text:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GGML/GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GGUF (GPT-Generated Unified Format)** and **GGML (GPT-Generated Model Library)** are quantization methods designed to make large language models (LLMs) more accessible, especially for users with limited hardware resources. These methods focus on efficient inference on CPUs, with the option to offload some computational layers to GPUs for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGUF vs GGML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspects                    | GGML                                                                                                                                   | GGUF                                                                                                     |\n",
    "|----------------------------|---------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| Basic                      | GGML is an obsolete format for creating quantized LLMs using the GGML tensor library.                                                  | GGUF is the successor of the GGML format that has better efficiency. It is also created with the GGML tensor library.   |\n",
    "| Speed                      | Compared to GGUF, the load time of the model and inference speed is on the slower side.                                               | GGUF LLMs have mmap compatibility that enhances load time and faster inference speed.                   |\n",
    "| Special Tokens             | Special tokens are not supported by GGML.                                                                                            | GGUF supports special tokens which are useful for creating effective prompts and also in llm fine-tuning. |\n",
    "| Support for Non-Llama Models | Non-llama models are not supported.                                                                                                  | GGUF format has extended compatibility with non-llama architecture models like Falcon, Bloom, Phi, Mistral, etc.   |\n",
    "| Extensibility & Flexibility| GGML had extensibility issues where small changes in the base model used to result in breaking changes.                              | GGUF format has been designed to be more extensible & flexible allowing the addition of new features without breaking anything. |\n",
    "| Ease of Use                | Compared to GGUF, the setup for GGML required more inputs from user and also had dependency on external libraries.                    | GGUF is much more user-friendly for setup, with not much dependency on external libraries.                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGUF Quantization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Selecting a Quantization Level**: Choose the desired bit precision (e.g., 4-bit, 8-bit). \n",
    "\n",
    "2. **Compressing the Model**: Apply the k-quant system to compress the model weights. The \"k\" in k-quant refers to the number of clusters used in the quantization process. Choosing an appropriate k value is crucial. A lower k leads to more aggressive compression but might introduce higher accuracy loss. The k-quant system works by:\n",
    "\n",
    "   - **Mapping High-Precision Weights**: Converting high-precision (e.g., 32-bit floating point) weights to lower precision (e.g., 4-bit integer) representations.\n",
    "\n",
    "   - **Reducing Redundancy**: In a trained model, many weights might have similar or redundant values. This redundancy can be exploited to reduce the overall storage requirements. By clustering similar weight values together and representing them with fewer bits, we can reduce the number of unique weight values that need to be stored. Techniques such as Huffman coding or other entropy-based methods can be used to encode these clustered values more efficiently.\n",
    "\n",
    "   - **Optimizing Storage**: Ensuring that the compressed weights are stored efficiently to allow for fast retrieval and use during inference. Specialized data structures, such as lookup tables or sparse matrices, may be used to store the quantized weights in a compact form.\n",
    "\n",
    "3. **Saving the Model**: Store the quantized model in the GGUF format, which includes the necessary metadata and configurations for loading and running the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a GGUF Formatted Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Install Necessary Packages**\n",
    "\n",
    "The `ctransformers` package is designed to facilitate the efficient use of transformer models for various tasks such as text generation, classification, and more. It supports model quantization and optimized inference, making it suitable for running large models on both CPUs and GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip install ctransformers[cuda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Load the Model and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load LLM and Tokenizer\n",
    "# Use `gpu_layers` to specify how many layers will be offloaded to the GPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/zephyr-7B-beta-GGUF\",\n",
    "    model_file=\"zephyr-7b-beta.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\", \n",
    "    gpu_layers=50,  # Offload 50 layers to the GPU for better performance\n",
    "    hf=True  # Use the Hugging Face model hub\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\", \n",
    "    use_fast=True  # Use the fast tokenizer for improved speed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Create a Text Generation Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Run a Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Please prepare me a GGUF/GGML tutorial.\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "outputs = pipe(prompt, max_new_tokens=256)\n",
    "\n",
    "# Print the generated text\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation-Aware Weight Quantization (AWQ)** tailors the precision of weights based on the activations during inference, skipping over salient weights to minimize accuracy loss.\n",
    "\n",
    "1. **Calibration**: Collect activation statistics to identify salient weights.\n",
    "\n",
    "2. **Selective Quantization**: Salient weights remain in FP16, while others are quantized to INT3 or INT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define function to collect activation statistics\n",
    "def collect_activation_statistics(model, data_loader):\n",
    "    activation_stats = []\n",
    "    for data in data_loader:\n",
    "        output = model(data)\n",
    "        activation_stats.append(output)\n",
    "    return torch.cat(activation_stats)\n",
    "\n",
    "# Define function to quantize based on activations\n",
    "def activation_aware_quantize(weights, activations):\n",
    "    # Calculate threshold for salient weights\n",
    "    threshold = torch.mean(activations) + torch.std(activations)\n",
    "    mask = activations > threshold\n",
    "    # Skip salient weights and quantize the rest\n",
    "    quant_weights = torch.where(mask, weights, torch.round(weights / 7.5) * 7.5)\n",
    "    return quant_weights\n",
    "\n",
    "# Example usage\n",
    "model = nn.Linear(10, 10)\n",
    "data_loader = [torch.randn(10) for _ in range(100)]\n",
    "activation_stats = collect_activation_statistics(model, data_loader)\n",
    "print(\"Activation Statistics:\", activation_stats)\n",
    "\n",
    "weights = model.weight.data\n",
    "print(\"Original Weights:\", weights)\n",
    "\n",
    "# Apply activation-aware quantization\n",
    "quant_weights = activation_aware_quantize(weights, activation_stats)\n",
    "print(\"Quantized Weights with AWQ:\", quant_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Please read the article of Mxim Lebonne to understand the GPTQ better: https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34\n",
    "\n",
    "- TheBloke user: https://huggingface.co/TheBloke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
