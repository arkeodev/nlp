{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Hugging_Face/3-HF_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DrOGgGrUXga"
      },
      "source": [
        "# Tokenizers and Their Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb8f5GWKUXgb"
      },
      "source": [
        "Tokenization is a fundamental step in natural language processing (NLP) tasks. It involves breaking down text into smaller units (tokens), which can be words, subwords, or characters. These tokens are then used by models to understand and process the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz4wRcLfUXgc"
      },
      "source": [
        "## Types of Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLq5Ava2UXgc"
      },
      "source": [
        "### Word Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFEOqZJVUXgc"
      },
      "source": [
        "**Definition:** Word tokenizers split text into individual words using spaces and punctuation as delimiters. They are simple and effective for languages with clear word boundaries marked by spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK1lwuPqUXgc",
        "outputId": "75e772bf-2590-415f-d6fa-3787f17b4811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Tokenization: ['Tokenization', 'is', 'essential', 'for', 'NLP', 'tasks.']\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load a pre-trained BERT tokenizer that uses word tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Simple word tokenization using Python's split method\n",
        "sentence = \"Tokenization is essential for NLP tasks.\"\n",
        "tokens = sentence.split()\n",
        "\n",
        "print(\"Word Tokenization:\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeRFcYgxUXgd"
      },
      "source": [
        "### Subword Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPvumIyJUXgd"
      },
      "source": [
        "**Definition:** Subword tokenizers break down words into subwords or symbols. These subwords can represent common prefixes, suffixes, or roots. This approach allows the model to handle rare words better and improves its ability to generalize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaGEoSVVUXgd",
        "outputId": "27dadfd7-a189-45a9-83fb-8bb582fba286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subword Tokenization: ['Token', 'ization', 'Ġis', 'Ġessential', 'Ġfor', 'ĠN', 'LP', 'Ġtasks', '.']\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# Load a pre-trained RoBERTa tokenizer that uses subword tokenization (Byte-level BPE)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(\"Tokenization is essential for NLP tasks.\")\n",
        "\n",
        "print(\"Subword Tokenization:\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoU7Se-KUXgd"
      },
      "source": [
        "### Character Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4W3kZtzUXge"
      },
      "source": [
        "**Definition:** Character tokenizers decompose text into individual characters. This level of granularity is useful for character-level modeling or in languages without clear word delimiters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7gd8IVjUXge",
        "outputId": "d75c7358-13f7-474d-c3f4-1c6e8c07daf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Character Tokenization: ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'e', 's', 's', 'e', 'n', 't', 'i', 'a', 'l', ' ', 'f', 'o', 'r', ' ', 'N', 'L', 'P', ' ', 't', 'a', 's', 'k', 's', '.']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import CharBPETokenizer\n",
        "\n",
        "# Initialize a character tokenizer (this requires a trained tokenizer, but for demonstration, we will tokenize the string into characters manually)\n",
        "tokenizer = CharBPETokenizer()\n",
        "\n",
        "# Manual character tokenization for the purpose of this example\n",
        "tokens = list(\"Tokenization is essential for NLP tasks.\")\n",
        "\n",
        "print(\"Character Tokenization:\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciN5j5h3UXge"
      },
      "source": [
        "### Byte Pair Encoding (BPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rZGD2QCUXge"
      },
      "source": [
        "**Definition:** BPE is a middle ground between word-level and character-level tokenization. It starts with a large corpus of text and repeatedly merges the most frequent pairs of bytes (or characters) to create a vocabulary of the most common subwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNGhzXaxUXge",
        "outputId": "6d5bb1d3-e9af-4ac4-81f0-1d107cf00978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Byte Pair Encoding Tokenization: ['Token', 'ization', ' is', ' essential', ' for', ' N', 'LP', ' tasks', '.']\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.encode(\"Tokenization is essential for NLP tasks.\", add_special_tokens=False)\n",
        "\n",
        "# Decode tokens into text\n",
        "tokenized_text = [tokenizer.decode([tok]) for tok in tokens]\n",
        "\n",
        "print(\"Byte Pair Encoding Tokenization:\", tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0cjF9uLUXge"
      },
      "source": [
        "### Comparisons of Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9MnZ8uIUXgf"
      },
      "source": [
        "When comparing the four tokenization methods—word tokenizers, subword tokenizers, character tokenizers, and Byte Pair Encoding (BPE)—each serves a distinct purpose and is preferred in different scenarios.\n",
        "\n",
        "### Word Tokenizers:\n",
        "- **Prefer for**: Languages with clear word boundaries and sufficient training data.\n",
        "- **Advantages**: Simplicity and interpretability.\n",
        "- **Limitations**: Struggles with out-of-vocabulary (OOV) words.\n",
        "- **Use Case**: Good for high-resource languages where the vocabulary can be comprehensively captured.\n",
        "\n",
        "### Subword Tokenizers:\n",
        "- **Prefer for**: Handling OOV words and morphologically rich languages.\n",
        "- **Advantages**: Balance between the granularity of characters and the context of words.\n",
        "- **Limitations**: May split semantically linked parts of a word.\n",
        "- **Use Case**: Useful in NLP tasks that benefit from understanding word parts, such as translation and text generation.\n",
        "\n",
        "### Character Tokenizers:\n",
        "- **Prefer for**: Languages without clear word delimiters, or for character-level tasks.\n",
        "- **Advantages**: No OOV words, as all text can be broken down into characters.\n",
        "- **Limitations**: Longer sequences to process, which may lead to higher computational costs.\n",
        "- **Use Case**: Preferred for character-level models and certain types of text classification where morphology is less important.\n",
        "\n",
        "### Byte Pair Encoding (BPE):\n",
        "- **Prefer for**: Efficiently encoding the input data by capturing the most frequent subwords.\n",
        "- **Advantages**: Reduces the vocabulary size without significant loss of information.\n",
        "- **Limitations**: Can be suboptimal for languages with large character sets or highly inflectional.\n",
        "- **Use Case**: Often used in large-scale language models like GPT and BERT for its efficiency and ability to handle a wide range of text types.\n",
        "\n",
        "In summary:\n",
        "- Choose **word tokenization** when working with well-resourced languages and the task doesn't require handling rare words.\n",
        "- Opt for **subword tokenization** when dealing with languages that have rich morphology or when you want a good balance between handling unknown words and leveraging context.\n",
        "- Select **character tokenization** for tasks that demand a character-level focus or when working with languages that have ambiguous word boundaries.\n",
        "- Use **BPE** for large-scale language modeling where there's a need to efficiently handle a diverse vocabulary with an effective representation of subwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBDkYGAUXgf"
      },
      "source": [
        "## Special Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke8WVAaYUXgf"
      },
      "source": [
        "Special tokens such as SOS/EOS, UNK, and PAD are integral in the language understanding process of transformer-based models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp2bVe3eUXgf",
        "outputId": "a1295a77-abf4-491d-a2f7-12b2344f66dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: tensor([[  101, 17662,  2227,  2003,  2877,  1999,  1996, 17953,  2361,  5884,\n",
            "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "Decoded tokens with special tokens: ['[CLS]', 'hugging', 'face', 'is', 'leading', 'in', 'the', 'nl', '##p', 'domain', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load a pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example text\n",
        "text = \"Hugging Face is leading in the NLP domain.\"\n",
        "\n",
        "# Add special tokens\n",
        "encoded = tokenizer.encode_plus(\n",
        "    text,\n",
        "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
        "    max_length=20,            # Set a max_length for padding & truncation\n",
        "    pad_to_max_length=True,   # Enable padding\n",
        "    return_tensors='pt',      # Return PyTorch tensor\n",
        "    truncation=True           # Explicitly truncate examples to max length\n",
        ")\n",
        "\n",
        "# Token IDs\n",
        "print(\"Token IDs:\", encoded['input_ids'])\n",
        "\n",
        "# Decode to show special tokens\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
        "print(\"Decoded tokens with special tokens:\", decoded_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0P1Xqo3UXgf"
      },
      "source": [
        "The output includes special tokens added by the tokenizer `[CLS]` for start of sequence, `[SEP]` for end of sequence, and `[PAD]` for padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_YMw_NfUXgf"
      },
      "source": [
        "- **Start-of-Sequence (SOS) / End-of-Sequence (EOS) Tokens:**\n",
        "  - In BERT's case, the `[CLS]` token serves as a start of sequence token, and `[SEP]` serves as both a separator token for two sequences and an end-of-sequence token for single sequences.\n",
        "  - These tokens are important because they provide the model with clear markers that indicate the beginning and end of a sequence. In tasks such as text classification, the representation of the `[CLS]` token is often used as the aggregate sequence representation for classification tasks.\n",
        "  - For models that generate text, like GPT-2, EOS tokens signal to the model when to stop generating further tokens.\n",
        "\n",
        "- **Unknown (UNK) Token:**\n",
        "  - The `[UNK]` token is used to replace words that are not in the tokenizer's vocabulary. This is important because models have a fixed vocabulary size and cannot accommodate all possible words, especially rare or domain-specific terms.\n",
        "  - By using an UNK token, the model can still process and make predictions about text containing unknown words, although it may lose some of the specific information that those words carry.\n",
        "\n",
        "- **Padding (PAD) Token:**\n",
        "  - The `[PAD]` token is used to fill up sequences so that all sequences in a batch have the same length. This is crucial for batching operations, as deep learning models usually require batched input to be of uniform size.\n",
        "  - Padding allows models to handle input sequences of varying lengths and is necessary for models to efficiently perform computations on a batch of data.\n",
        "\n",
        "In transformer models, the attention mechanism needs to know which positions are padded so it can disregard them when computing the attention scores. This is often achieved through an attention mask, which is an additional input indicating which tokens are padding and should not be attended to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PqIYFO0UXgg"
      },
      "source": [
        "## Why We Use Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5a2XKaoUXgg"
      },
      "source": [
        "Masking is used to inform the model which parts of the input it should consider and which parts it should ignore during training or inference. This is important for:\n",
        "- **Attention Mechanisms:** To prevent the model from paying attention to padding tokens.\n",
        "- **Variable Sequence Lengths:** To handle inputs of variable lengths in a batch for efficient processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhXmWLmfUXgg",
        "outputId": "67cf66fe-6ce2-4983-d186-dd05921fa83f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs of the first sentence :  tensor([  101, 17662,  2227,  2003,  4329,  6026,  1996,  2492,  1997, 17953,\n",
            "         2361,  1012,   102])\n",
            "Token IDs of the second sentence:  tensor([  101,  3698,  4083,  2003, 17160,  1012,   102,     0,     0,     0,\n",
            "            0,     0,     0])\n",
            "Attention Masks of the first sentence :  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Attention Masks of the second sentence:  tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load a pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example text with two sentences of different lengths\n",
        "texts = [\n",
        "    \"Hugging Face is revolutionizing the field of NLP.\",\n",
        "    \"Machine learning is fascinating.\"\n",
        "]\n",
        "\n",
        "# Tokenize the texts and add special tokens ([CLS] and [SEP])\n",
        "# The tokenizer will pad the sequences to the length of the longest sequence\n",
        "encoded_inputs = tokenizer(\n",
        "    texts,\n",
        "    padding=True,  # Pad the sequences to the length of the longest sequence\n",
        "    truncation=True,  # Truncate the sequences to the model's max input length\n",
        "    return_tensors='pt'  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "# Extract the attention mask\n",
        "attention_masks = encoded_inputs['attention_mask']\n",
        "\n",
        "print(\"Token IDs of the first sentence : \", encoded_inputs['input_ids'][0])\n",
        "print(\"Token IDs of the second sentence: \", encoded_inputs['input_ids'][1])\n",
        "print(\"Attention Masks of the first sentence : \", attention_masks[0])\n",
        "print(\"Attention Masks of the second sentence: \", attention_masks[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XohwYbI3UXgg"
      },
      "source": [
        "## AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocXUrA9OUXgg"
      },
      "source": [
        "`AutoTokenizer` is a class within the Hugging Face `transformers` library designed to automatically instantiate a tokenizer class based on the pre-trained model's name or path you provide. It serves as a wrapper that dynamically adjusts to the specific tokenizer class associated with the given pre-trained model, ensuring that you always use the correct tokenizer for your model without needing to manually specify the tokenizer class.\n",
        "\n",
        "When you call `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`, the `AutoTokenizer`:\n",
        "\n",
        "1. Looks up the `bert-base-uncased` model in the Hugging Face model repository.\n",
        "2. Identifies the appropriate tokenizer that pairs with this model, which in this case is the BERT tokenizer.\n",
        "3. Downloads and caches the tokenizer's pre-trained vocabulary and merges it if needed.\n",
        "4. Instantiates the BERT tokenizer with the pre-trained vocabulary, ready for use.\n",
        "\n",
        "This abstraction is particularly useful because different models might use different tokenization algorithms (e.g., BERT uses WordPiece, GPT-2 uses byte-level BPE, and T5 uses SentencePiece). With `AutoTokenizer`, the user doesn't have to worry about these details; they just need to know the model they want to use, and the corresponding tokenizer is automatically selected and loaded.\n",
        "\n",
        "This makes the `AutoTokenizer` class a powerful tool for users who need to switch between different models and tokenizers, simplifying the process of preparing inputs for a wide array of transformer-based models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkBKcx6EUXgg",
        "outputId": "34d30caa-d238-4aee-91da-2a4f7eb551b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 17662, 2227, 2003, 4329, 6026, 9932, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Special tokens added: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer associated with a pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Tokenize a sample text\n",
        "text = \"Hugging Face is revolutionizing AI.\"\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "# Explore the tokens\n",
        "print(tokens)\n",
        "\n",
        "# Special tokens added automatically by the tokenizer\n",
        "print(\"Special tokens added:\", tokenizer.all_special_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE6yD-XrUXgh"
      },
      "source": [
        "## Why Tokenizer Should Match with the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Cl7PcnUXgh"
      },
      "source": [
        "Using the tokenizer with the corresponding model ensures that the data flow from pre-processing to prediction is as smooth and accurate as possible, reproducing the conditions under which the model was originally trained and allowing it to perform at its best.\n",
        "\n",
        "1. **Consistency in Vocabulary**: Pre-trained models are trained with a specific vocabulary, and using the corresponding tokenizer ensures that the tokens fed into the model are recognized. If you use a different tokenizer, words may be split into tokens differently, and some tokens may not be recognized at all by the model, leading to poor performance or even errors.\n",
        "\n",
        "2. **Maintaining Tokenization Method**: Each model may be trained with a particular tokenization method (e.g., BERT uses WordPiece, GPT-2 uses byte-level BPE). The tokenizer must match this method to ensure that the model understands the input as intended during training. Using a different method would mean that the input is presented to the model in an unfamiliar way, which can degrade the model's ability to make predictions.\n",
        "\n",
        "3. **Special Tokens**: Many models rely on special tokens for understanding sentence boundaries, segmenting sentence pairs, or other purposes. The correct tokenizer will insert these tokens as needed. For instance, BERT expects a `[CLS]` token at the beginning of an input and `[SEP]` tokens to separate segments.\n",
        "\n",
        "4. **Pre-processing and Post-processing**: The tokenizer handles critical pre-processing steps like truncation and padding, ensuring inputs are of the correct length. Some tokenizers also perform post-processing to convert token ids back to words. This pre- and post-processing needs to be consistent with the model's training.\n",
        "\n",
        "5. **Optimized Performance**: Matching tokenizers are often optimized to work with their models, providing better speed and efficiency. This is especially true for \"fast\" tokenizers in Hugging Face, which are implemented in Rust.\n",
        "\n",
        "6. **End-to-End Coherence**: For end-to-end tasks, such as text-to-text tasks (e.g., translation, summarization), the tokenizer ensures that the input and output texts are handled consistently. This coherence is essential for the model to learn and generate accurate outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eNUVMQgUXgh"
      },
      "source": [
        "## Training a New Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yhKrzbKUXgh"
      },
      "source": [
        "Sometimes, you might need to train a tokenizer on a custom dataset, particularly if your text data is in a language or domain not covered by pre-trained models.\n",
        "\n",
        "In an other case. when a tokenizer is trained on a corpus that is not similar to the one you're working with—be it a new language, new characters, new domain, or style—it might not perform well. Dissimilarities between the training data of a tokenizer and your data can lead to poor tokenization quality.\n",
        "\n",
        "In such cases, you might need to train a new tokenizer that is more aligned with your specific corpus.\n",
        "\n",
        "For the demonstration, I'll use a simplified example to train a sample tokenizer with a small piece of text directly passed as input. However, note that real tokenizer training is generally done on a much larger dataset and saved to files. Here, we'll tokenize a sample text and then print out the tokenizer's attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkULmflkUXgh",
        "outputId": "f0fdfc0d-1371-4f40-d286-d32f88860bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Vocabulary size: 97\n",
            "Model attributes: <tokenizers.models.BPE object at 0x10f757890>\n",
            "Trainer attributes: <tokenizers.trainers.BpeTrainer object at 0x10f8a5810>\n",
            "Pre-tokenizer: <tokenizers.pre_tokenizers.Whitespace object at 0x10f8f60b0>\n",
            "Tokens: ['Tokenization', 'is', 'essential', 'for', 'ev', 'er', '[UNK]', 'tas', 'k', '.']\n",
            "Token IDs: [82, 33, 95, 84, 60, 39, 0, 73, 21, 5]\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Sample text to train the tokenizer\n",
        "sample_texts = [\n",
        "    \"Hugging Face is revolutionizing AI.\",\n",
        "    \"Their tokenizer is part of a larger suite of NLP tools.\",\n",
        "    \"Tokenization is essential for NLP tasks.\"\n",
        "]\n",
        "\n",
        "# Initialize an empty Byte-Level BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "# Initialize a trainer\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "# Initialize a pre-tokenizer\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train_from_iterator(sample_texts, trainer=trainer)\n",
        "\n",
        "# Print tokenizer's attributes\n",
        "print(\"Vocabulary size:\", tokenizer.get_vocab_size())\n",
        "print(\"Model attributes:\", tokenizer.model)\n",
        "print(\"Trainer attributes:\", trainer)\n",
        "print(\"Pre-tokenizer:\", tokenizer.pre_tokenizer)\n",
        "\n",
        "# You can also encode text to see how the tokenizer performs\n",
        "encoding = tokenizer.encode(\"Tokenization is essential for every task.\")\n",
        "print(\"Tokens:\", encoding.tokens)\n",
        "print(\"Token IDs:\", encoding.ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJRnaA_sUXgh"
      },
      "source": [
        "Here's a breakdown of the components:\n",
        "\n",
        "- **`Tokenizer(BPE(unk_token=\"[UNK]\"))`:** This creates a new tokenizer using the BPE (Byte-Pair Encoding) model with a specified unknown token `[UNK]`.\n",
        "- **`BpeTrainer`:** The trainer is responsible for training the tokenizer model. We can specify special tokens that should be preserved during tokenization.\n",
        "- **`Whitespace`:** The pre-tokenizer is set to split the input text into tokens based on whitespace, a simple and common pre-tokenization step.\n",
        "- **`tokenizer.train_from_iterator()`:** This method trains the tokenizer on the list of sample texts provided.\n",
        "\n",
        "After training, we examine the tokenizer's attributes and test the tokenizer on a new sentence to observe the generated tokens and their corresponding IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_tNESJsUXgi"
      },
      "source": [
        "## Pre-tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WCq1HRcUXgi"
      },
      "source": [
        "Pre-tokenization is the step before the main tokenization process where the text is prepared for further processing. During pre-tokenization, text is typically split into words and symbols (like punctuation) based on whitespace and other simple rules. This initial segmentation makes the subsequent, more complex tokenization steps easier and more consistent.\n",
        "\n",
        "**Why Pre-tokenization is Important**:\n",
        "- **Simplicity**: It simplifies the complex tokenization process by handling the easy and obvious cases like splitting by spaces and punctuation.\n",
        "- **Uniformity**: It ensures a level of uniformity before more sophisticated tokenization algorithms, such as subword tokenization, are applied.\n",
        "- **Efficiency**: It can make the whole tokenization process more efficient, especially when dealing with large corpora.\n",
        "\n",
        "Imagine a scenario where you're tokenizing the text: \"HuggingFace's transformers: amazingly simple!\". Without pre-tokenization, a naive tokenizer might treat \"HuggingFace's\" as a single token, which could be problematic if the model has never seen this sequence before.\n",
        "\n",
        "With pre-tokenization, the text might be split into [\"HuggingFace\", \"'\", \"s\", \"transformers\", \":\", \"amazingly\", \"simple\", \"!\"]. Now, a more sophisticated tokenizer can process these tokens separately, recognizing \"HuggingFace\" and \"amazingly\" as whole words, even if it needs to split or encode \"HuggingFace's\" into subwords due to its absence in the vocabulary.\n",
        "\n",
        "If you don't use pre-tokenization and directly apply a subword tokenizer, you may end up with suboptimal tokenization, where composite words or words with affixes aren't recognized or split properly.\n",
        "\n",
        "Here's a simple example of manual pre-tokenization using Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlaSEBmCUXgi",
        "outputId": "afef4b41-78cd-4955-b589-13142bf3fca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-tokenization: ['HuggingFace', \"'\", 's', 'transformers', ':', 'amazingly', 'simple', '!']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Our sample text\n",
        "text = \"HuggingFace's transformers: amazingly simple!\"\n",
        "\n",
        "# A simple pre-tokenization step that splits based on whitespace and punctuation\n",
        "pre_tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
        "\n",
        "print(\"Pre-tokenization:\", pre_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5-ePUmnUXgi"
      },
      "source": [
        "## What Are Fast Tokenizers in Hugging Face?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivvBSKN6UXgi"
      },
      "source": [
        "Fast tokenizers in Hugging Face refer to a certain class of tokenizers that are implemented in Rust for high performance. These tokenizers are part of the Hugging Face `tokenizers` library, which is designed to provide an ultra-fast and versatile tokenization utility suitable for training and production environments which is critical when preprocessing large datasets or when low latency is required.\n",
        "\n",
        "### Why We Need Fast Tokenizers:\n",
        "\n",
        "1. **Performance**: Fast tokenizers provide a significant speed advantage.\n",
        "\n",
        "2. **Parallelism**: Rust's efficient memory management and concurrency models enable these tokenizers to effectively handle parallel tokenization over large batches of text, which is essential for leveraging modern multi-core processors.\n",
        "\n",
        "3. **Consistency**: They ensure that tokenization is consistent with the pre-trained models since they use the same algorithms and byte-pair encoding (BPE) or WordPiece methodologies.\n",
        "\n",
        "4. **Full-Fledged Features**: Despite their speed, these tokenizers do not compromise on features and support all the required tokenization steps, such as adding special tokens, handling padding, and providing attention masks.\n",
        "\n",
        "5. **Ease of Use**: Fast tokenizers integrate seamlessly with Python, allowing users to benefit from their performance while working within the Python ecosystem, which is widely used in data science and machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN5Iu8ALUXgj"
      },
      "source": [
        "### Demonstration of Fast Tokenizers:\n",
        "\n",
        "Let's do a quick comparison between a standard tokenizer and a fast tokenizer. For this demonstration, we'll tokenize a large list of sentences and measure the time taken by each tokenizer.\n",
        "\n",
        "In this demonstration, we use the `batch_encode_plus` method to process a batch of sentences. The `fast_tokenizer` is expected to perform the same operation significantly faster than the `standard_tokenizer`. The actual performance gain can vary based on the specifics of the system and the workload, but typically, you'll see a substantial reduction in processing time with the fast tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hYdIplJUXgj",
        "outputId": "4920534c-5c9d-47c2-9d4c-386438cb3232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Tokenizer took: 14.25 seconds\n",
            "Fast Tokenizer took: 2.49 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from transformers import BertTokenizer, BertTokenizerFast\n",
        "\n",
        "# Initialize both the standard Python and the fast Rust BERT tokenizers\n",
        "standard_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "fast_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare a large list of sentences\n",
        "sentences = [\"The quick brown fox jumps over the lazy dog.\" for _ in range(100000)]\n",
        "\n",
        "# Tokenize using the standard tokenizer\n",
        "start_time = time.time()\n",
        "standard_tokenizer.batch_encode_plus(sentences, padding=True, truncation=True)\n",
        "standard_time = time.time() - start_time\n",
        "\n",
        "# Tokenize using the fast tokenizer\n",
        "start_time = time.time()\n",
        "fast_tokenizer.batch_encode_plus(sentences, padding=True, truncation=True)\n",
        "fast_time = time.time() - start_time\n",
        "\n",
        "# Compare times\n",
        "print(f\"Standard Tokenizer took: {standard_time:.2f} seconds\")\n",
        "print(f\"Fast Tokenizer took: {fast_time:.2f} seconds\")\n",
        "\n",
        "# Verifying the speed improvement\n",
        "assert fast_time < standard_time, \"The fast tokenizer is expected to be faster than the standard tokenizer!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VuRMiXaUXgj"
      },
      "source": [
        "## What Is Sentence Pair Tokenization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAUMOja5UXgj"
      },
      "source": [
        "Sentence pair tokenization is a process used in NLP tasks where the model needs to understand and compare the relationship between two separate text segments. During tokenization, the two sentences are combined into a single input sequence with special tokens indicating the separation and order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSkias88UXgk"
      },
      "source": [
        "### When and Why We Use It?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhgclFEIUXgk"
      },
      "source": [
        "We use sentence pair tokenization primarily for tasks like:\n",
        "\n",
        "- **Question Answering (QA)**: Understanding the context of a passage to answer a related question.\n",
        "- **Semantic Similarity**: Assessing if two sentences are semantically similar or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiXF4Lw1UXgk"
      },
      "source": [
        "### How It Is Used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oosRSqppUXgk"
      },
      "source": [
        "With models like BERT, sentence pair tokenization typically involves:\n",
        "\n",
        "- Adding a `[CLS]` token at the beginning of the first sentence.\n",
        "- Adding a `[SEP]` token at the end of the first sentence and again at the end of the second sentence.\n",
        "- Generating token type ids to indicate which tokens belong to the first sentence and which to the second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNmrPv8DUXgk"
      },
      "source": [
        "### Demo of Sentence Pair Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2qvdN87UXgk"
      },
      "source": [
        "Here is a demonstration of how to encode sentence pairs and then use a model fine-tuned for sentence similarity to predict how semantically similar the sentences are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXwQnhmrUXgk",
        "outputId": "fb13e32b-9cd0-4ab2-a0fd-6bfbee15c56f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence pair: The weather is nice today. | It's a beautiful day.\n",
            "['[CLS]', 'the', 'weather', 'is', 'nice', 'today', '.', '[SEP]', '[PAD]'] ['[CLS]', 'it', \"'\", 's', 'a', 'beautiful', 'day', '.', '[SEP]']\n",
            "Cosine Similarity: 0.5240273475646973\n",
            "\n",
            "Sentence pair: I'm learning about NLP. | I'm studying natural language processing.\n",
            "['[CLS]', 'i', \"'\", 'm', 'learning', 'about', 'nl', '##p', '.', '[SEP]'] ['[CLS]', 'i', \"'\", 'm', 'studying', 'natural', 'language', 'processing', '.', '[SEP]']\n",
            "Cosine Similarity: 0.741668164730072\n",
            "\n",
            "Sentence pair: The cat sits on the mat. | The dog plays in the yard.\n",
            "['[CLS]', 'the', 'cat', 'sits', 'on', 'the', 'mat', '.', '[SEP]'] ['[CLS]', 'the', 'dog', 'plays', 'in', 'the', 'yard', '.', '[SEP]']\n",
            "Cosine Similarity: 0.19166551530361176\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    \"\"\"\n",
        "    This collapses the token embeddings for each sentence into a single vector per\n",
        "    sentence, using the attention mask to ignore padding tokens.\n",
        "    \"\"\"\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Define two sets of sentences to tokenize together\n",
        "sentence_pairs = [\n",
        "    (\"The weather is nice today.\", \"It's a beautiful day.\"),\n",
        "    (\"I'm learning about NLP.\", \"I'm studying natural language processing.\"),\n",
        "    (\"The cat sits on the mat.\", \"The dog plays in the yard.\")\n",
        "]\n",
        "\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define sentence pairs\n",
        "sentence_pairs = [\n",
        "    (\"The weather is nice today.\", \"It's a beautiful day.\"),\n",
        "    (\"I'm learning about NLP.\", \"I'm studying natural language processing.\"),\n",
        "    (\"The cat sits on the mat.\", \"The dog plays in the yard.\")\n",
        "]\n",
        "\n",
        "# Process each pair\n",
        "for sentence1, sentence2 in sentence_pairs:\n",
        "    print(f\"Sentence pair: {sentence1} | {sentence2}\")\n",
        "\n",
        "    # Tokenize and encode sentences as a batch\n",
        "    encoded_input = tokenizer([sentence1, sentence2], padding=True, truncation=True, return_tensors='pt')\n",
        "    print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0]), tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][1]))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "    # Perform pooling\n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "    # Normalize embeddings\n",
        "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "\n",
        "    # Compute similarity\n",
        "    similarity = torch.matmul(sentence_embeddings[0], sentence_embeddings[1].T)\n",
        "    print(f\"Cosine Similarity: {similarity.item()}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MQP-hfnUXgl"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRuKhPfjUXgl"
      },
      "source": [
        "Tokenizers are an essential part of NLP workflows, and understanding how to effectively use and customize them is crucial for tackling a wide range of tasks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "huggingface",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
