{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Hugging_Face/4-HF_Datasets_Trainer_Accelerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"OjPdnZgDUqNU"},"source":["# Datasets, Trainers"]},{"cell_type":"markdown","metadata":{"id":"CxU1gjLMUqNV"},"source":["This notebook covers essential tools from Hugging Face, including handling datasets, custom datasets, data batching, data manipulation with Pandas, data collators, the Trainer API, and accelerators. Each section contains implementation examples and explanations."]},{"cell_type":"markdown","metadata":{"id":"rtBQlww4UqNV"},"source":["## Datasets"]},{"cell_type":"markdown","metadata":{"id":"_X3lnKpYUqNV"},"source":["Hugging Face's `datasets` library offers a quick and efficient way to access a wide array of NLP datasets."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T21:12:19.249427Z","iopub.status.busy":"2024-04-26T21:12:19.248947Z","iopub.status.idle":"2024-04-26T21:12:25.794279Z","shell.execute_reply":"2024-04-26T21:12:25.793328Z","shell.execute_reply.started":"2024-04-26T21:12:19.249386Z"},"id":"2K7j0tfoUqNW","outputId":"7eb66282-a477-407a-be79-478cbc031d66","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89496fd06081486c9464d61ae850e6fc","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading data: 100%|██████████| 649k/649k [00:00<00:00, 2.70MB/s]\n","Downloading data: 100%|██████████| 75.7k/75.7k [00:00<00:00, 682kB/s]\n","Downloading data: 100%|██████████| 308k/308k [00:00<00:00, 2.85MB/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4e1dc8a97b04f11959212b0d1f71ef8","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7059b1564a724ceaa56e8f53ebc579f4","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a84d7d0bf2d540e8bfcf5c3403451083","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 3668\n","    })\n","    validation: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 408\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'label', 'idx'],\n","        num_rows: 1725\n","    })\n","})\n","{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n"]}],"source":["from datasets import load_dataset\n","\n","# Load the dataset\n","dataset = load_dataset(path='glue', name='mrpc')\n","print(dataset)\n","\n","# Accessing different splits\n","train_dataset = dataset['train']\n","valid_dataset = dataset['validation']\n","test_dataset = dataset['test']\n","\n","# Example: Print the first example from the training dataset\n","print(train_dataset[0])\n"]},{"cell_type":"markdown","metadata":{"id":"i2xADoS5UqNW"},"source":["The output shows a `DatasetDict` object from the Hugging Face `datasets` library, typically containing several datasets organized by their split names such as `train`, `validation`, and `test`. Here's a detailed breakdown of its components and the structure:\n","\n","- **DatasetDict**: This is a Python dictionary-like container that groups multiple `Dataset` objects, each corresponding to a different subset of the data, usually differentiated by the data split (e.g., training, validation, testing). This structure is particularly useful for managing datasets that inherently come with predefined splits for different stages of model training and evaluation.\n","\n","1. **Keys (Splits)**:\n","   - `train`\n","   - `validation`\n","   - `test`\n","\n","2. **Dataset**:\n","   - Each entry under a key is a `Dataset` object, which itself is a structured collection of data points (rows).\n","   - These datasets are implemented efficiently to handle large amounts of data without loading everything into memory.\n","\n","3. **Features**:\n","   - Each `Dataset` has a schema describing its features, which are the columns of the dataset.\n","\n","4. **Num_rows**:\n","   - Indicates the number of rows (data points) available in each dataset split.\n","\n","The `DatasetDict` structure is particularly important for:\n","- **Modularization**: It keeps different data splits well-organized and easily accessible through keys.\n","- **Scalability**: It supports lazy loading from disk, allowing it to handle very large datasets efficiently.\n","- **Flexibility**: It provides built-in methods for common data operations, such as shuffling, batching, and applying transformations."]},{"cell_type":"markdown","metadata":{"id":"OxNYNdZXUqNX"},"source":["### Converting to Pandas DataFrames"]},{"cell_type":"markdown","metadata":{"id":"6x8S02oHUqNX"},"source":["Integration with Pandas allows for sophisticated data manipulation."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T21:12:25.796780Z","iopub.status.busy":"2024-04-26T21:12:25.796328Z","iopub.status.idle":"2024-04-26T21:12:25.832198Z","shell.execute_reply":"2024-04-26T21:12:25.831414Z","shell.execute_reply.started":"2024-04-26T21:12:25.796752Z"},"id":"gUv2RpSxUqNX","outputId":"c613887c-de89-4df1-d31a-3c3602b628aa","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence1</th>\n","      <th>sentence2</th>\n","      <th>label</th>\n","      <th>idx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>PCCW 's chief operating officer , Mike Butcher...</td>\n","      <td>Current Chief Operating Officer Mike Butcher a...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The world 's two largest automakers said their...</td>\n","      <td>Domestic sales at both GM and No. 2 Ford Motor...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>According to the federal Centers for Disease C...</td>\n","      <td>The Centers for Disease Control and Prevention...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A tropical storm rapidly developed in the Gulf...</td>\n","      <td>A tropical storm rapidly developed in the Gulf...</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The company didn 't detail the costs of the re...</td>\n","      <td>But company officials expect the costs of the ...</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           sentence1  \\\n","0  PCCW 's chief operating officer , Mike Butcher...   \n","1  The world 's two largest automakers said their...   \n","2  According to the federal Centers for Disease C...   \n","3  A tropical storm rapidly developed in the Gulf...   \n","4  The company didn 't detail the costs of the re...   \n","\n","                                           sentence2  label  idx  \n","0  Current Chief Operating Officer Mike Butcher a...      1    0  \n","1  Domestic sales at both GM and No. 2 Ford Motor...      1    1  \n","2  The Centers for Disease Control and Prevention...      1    2  \n","3  A tropical storm rapidly developed in the Gulf...      0    3  \n","4  But company officials expect the costs of the ...      0    4  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Convert Hugging Face dataset to Pandas DataFrame\n","df_test = test_dataset.to_pandas()\n","df_test.head()"]},{"cell_type":"markdown","metadata":{"id":"7eKRI2dWUqNX"},"source":["## Data Collators"]},{"cell_type":"markdown","metadata":{"id":"jblMl5rhUqNY"},"source":["In the Hugging Face ecosystem, data collators are utilities designed to handle the common preprocessing steps required to prepare batches of data for model training."]},{"cell_type":"markdown","metadata":{"id":"RZpOr98kUqNY"},"source":["Data collators are used to streamline the process of preparing data for neural networks, particularly those trained on batches where each input needs to have the same shape or size. The main functions of data collators in Hugging Face include:\n","\n","1. **Padding**: Automatically pad input sequences to the maximum length in each batch or to a predefined length, ensuring that all tensor inputs to the model are of the same size.\n","2. **Truncation**: Reduce the length of any input sequence that exceeds a maximum length, either specified or inferred from the batch.\n","3. **Creating Attention Masks**: Generate masks to inform the model which parts of the input are actual data and which are padding, critical for transformer models that use attention mechanisms.\n","4. **Handling Special Tokens**: Insert special tokens that are necessary for certain models (like BERT’s `[CLS]` and `[SEP]` tokens) appropriately across batches."]},{"cell_type":"markdown","metadata":{"id":"NKuqHIh6UqNY"},"source":["Data collators in Hugging Face are often used with the `DataLoader` class from PyTorch or similar batching utilities in other frameworks to automate the process of generating batches from a dataset.\n","\n","Let's consider an example where you have a dataset of text sequences of varying lengths, and we want to prepare batches for training a model using dynamic padding. This will highlight how `DataCollatorWithPadding` dynamically adjusts the padding based on the longest sequence in each batch, thus minimizing the padded length and optimizing GPU memory usage."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T21:12:25.833614Z","iopub.status.busy":"2024-04-26T21:12:25.833326Z","iopub.status.idle":"2024-04-26T21:12:44.311518Z","shell.execute_reply":"2024-04-26T21:12:44.310347Z","shell.execute_reply.started":"2024-04-26T21:12:25.833589Z"},"id":"H6UJq39DUqNY","outputId":"595cffef-985c-4195-a899-3f4cfa1b99b4","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-26 21:12:34.568387: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-26 21:12:34.568490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-26 21:12:34.724381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3737c263d17f4e0c8b2f067371cbcb46","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bbbb50a010e343e58abf18218f91a81a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e4a19adac764d82887166951826500a","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1ffa7da9f7f470a858344d2065cd28c","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Batch Input IDs shape: torch.Size([2, 9])\n","Batch Attention Mask shape: torch.Size([2, 9])\n","Batch Input IDs shape: torch.Size([2, 27])\n","Batch Attention Mask shape: torch.Size([2, 27])\n","Batch Input IDs shape: torch.Size([1, 4])\n","Batch Attention Mask shape: torch.Size([1, 4])\n"]}],"source":["from transformers import AutoTokenizer, DataCollatorWithPadding\n","from torch.utils.data import DataLoader, Dataset\n","import torch\n","\n","# Define a tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Sample text data of varying lengths\n","texts = [\n","    \"Hello world\",\n","    \"Hugging Face is revolutionizing AI.\",\n","    \"They are making natural language processing accessible to developers.\",\n","    \"Here is a very long sentence, purely created to demonstrate how dynamic padding works within the context of training deep learning models.\",\n","    \"Short!\"\n","]\n","\n","# Tokenize the data properly, ensuring all necessary fields are included\n","tokenized_texts = [tokenizer(text, add_special_tokens=True, return_attention_mask=True, truncation=True, padding=False) for text in texts]\n","\n","# Define the dataset class\n","class TextDataset(Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __len__(self):\n","        return len(self.encodings)\n","\n","    def __getitem__(self, idx):\n","        # Directly return the dictionary of inputs for each item\n","        return {key: torch.tensor(self.encodings[idx][key]) for key in self.encodings[idx]}\n","\n","# Initialize the dataset\n","dataset = TextDataset(tokenized_texts)\n","\n","# Initialize the data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='pt')\n","\n","# Create a DataLoader\n","data_loader = DataLoader(dataset, batch_size=2, collate_fn=data_collator)\n","\n","# Iterate through the DataLoader\n","for batch in data_loader:\n","    print(\"Batch Input IDs shape:\", batch['input_ids'].shape)\n","    print(\"Batch Attention Mask shape:\", batch['attention_mask'].shape)\n"]},{"cell_type":"markdown","metadata":{"id":"b8gA8rSiUqNY"},"source":["- **Tokenization**: Each text is tokenized using a pre-trained tokenizer. This typically involves converting texts into a sequence of token IDs.\n","- **Data Collator Initialization**: `DataCollatorWithPadding` is initialized with the same tokenizer to ensure that the padding uses the tokenizer’s padding token ID.\n","- **DataLoader**: The `DataLoader` is set up with the `data_collator` as its `collate_fn`, which means that each batch produced by the DataLoader will be processed by the data collator. This process involves dynamically padding the sequences so that all sequences in a batch are the same length.\n","- **Batch Processing**: When iterating over the DataLoader, each batch is automatically padded to the length of the longest sequence in the batch, and appropriate attention masks are created."]},{"cell_type":"markdown","metadata":{"id":"hDlzVlvxUqNZ"},"source":["## Custom Datasets"]},{"cell_type":"markdown","metadata":{"id":"nPyk4UAAUqNZ"},"source":["Creating custom datasets involves defining a dataset structure that Hugging Face's `datasets` library can process. Custom datasets can be created from various sources, such as Pandas dataframes, using the `from_pandas` method."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-26T21:12:44.314084Z","iopub.status.busy":"2024-04-26T21:12:44.313308Z","iopub.status.idle":"2024-04-26T21:12:44.332882Z","shell.execute_reply":"2024-04-26T21:12:44.331844Z","shell.execute_reply.started":"2024-04-26T21:12:44.314034Z"},"id":"Nqw_1xDqUqNZ","outputId":"acca24b7-d033-476f-ebd6-f0bad55c3e67","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 2\n","})\n"]}],"source":["from datasets import Dataset\n","import pandas as pd\n","\n","data = {'text': ['Hello, world!', 'Hugging Face is amazing'], 'label': [0, 1]}\n","df = pd.DataFrame(data)\n","\n","# Convert a pandas dataframe to a Hugging Face dataset\n","custom_dataset = Dataset.from_pandas(df)\n","print(custom_dataset)"]},{"cell_type":"markdown","metadata":{"id":"VBFezv1JUqNZ"},"source":["## The Trainer API"]},{"cell_type":"markdown","metadata":{"id":"AadXbldGUqNZ"},"source":["The `Trainer` API in Hugging Face's Transformers library is designed to abstract away much of the boilerplate code typically required for training transformer models. It provides a convenient interface for fitting models on datasets, handling everything from setting up the training loop, managing device placement (CPU or GPU), to logging and saving models."]},{"cell_type":"markdown","metadata":{"id":"YChykNQUUqNZ"},"source":["1. **Abstraction of Training Loops**: The `Trainer` handles the intricacies of the training loops internally. Users don't need to manually code the steps for each epoch, batch processing, gradient calculation, or updates.\n","\n","2. **Automatic Device Management**: It automatically moves models, inputs, and outputs to the appropriate devices (CPU or GPU), based on availability and configuration.\n","\n","3. **Built-In Training Features**: Includes support for mixed precision training, multi-GPU training, and even TPU support, simplifying the process of scaling up training to larger models and datasets.\n","\n","4. **Metrics and Logging**: Integrates seamlessly with TensorBoard and other logging frameworks to monitor training progress. This includes automatic logging of loss, evaluation metrics, and other useful training information.\n","\n","5. **Pre-Configured Optimizations**: Comes with pre-configured optimizers and learning rate schedules, which can be customized easily through its flexible argument structure."]},{"cell_type":"markdown","metadata":{"id":"rQqzAmYHUqNa"},"source":["**Main and Critical Arguments of `TrainingArguments`**\n","\n","- **output_dir**: Where to store the output files (e.g., model checkpoints).\n","- **num_train_epochs**: Total number of training epochs.\n","- **per_device_train_batch_size** and **per_device_eval_batch_size**: Batch sizes for training and evaluation, respectively, allowing for different settings during training and evaluation phases.\n","- **warmup_steps**: Number of steps to perform learning rate warmup, which is crucial for stabilizing the model's initial training phase.\n","- **weight_decay**: Regularization parameter to prevent overfitting.\n","- **logging_dir**: Directory for storing logs, useful for tracking training process and for later analysis with tools like TensorBoard."]},{"cell_type":"markdown","metadata":{"id":"0OSSbbFjUqNa"},"source":["Let's create an example using the BERT model for a sentiment classification task on the IMDB dataset. This will illustrate how `Trainer` API facilitates model training:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"referenced_widgets":["e4596dd393a24501b74d665e5548adcb","a44d0032508b44968ca43f8e68a77dab","87bd94cff7db4ba2b2d5e03cbb39a703","0d475ab69ea248c2af016b61e0cf2ddc"]},"execution":{"iopub.execute_input":"2024-04-26T21:31:25.957969Z","iopub.status.busy":"2024-04-26T21:31:25.957606Z","iopub.status.idle":"2024-04-26T21:41:15.716581Z","shell.execute_reply":"2024-04-26T21:41:15.715763Z","shell.execute_reply.started":"2024-04-26T21:31:25.957941Z"},"id":"RHLcENpJUqNa","outputId":"93d34cfc-4d74-47ca-d191-e15accd5571c","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1563/1563 09:43, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.301400</td>\n","      <td>0.270388</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=1563, training_loss=0.3665847448835904, metrics={'train_runtime': 583.5635, 'train_samples_per_second': 42.84, 'train_steps_per_second': 2.678, 'total_flos': 1644444096000000.0, 'train_loss': 0.3665847448835904, 'epoch': 1.0})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from datasets import load_dataset\n","\n","# Load the dataset\n","dataset = load_dataset('imdb')\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Preprocess the text data\n","def preprocess_function(examples):\n","    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n","\n","# Tokenize the dataset using the map function\n","tokenized_datasets = dataset.map(preprocess_function, batched=True)\n","\n","# Load the model\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=1,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=64,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    evaluation_strategy=\"epoch\"\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets['train'],\n","    eval_dataset=tokenized_datasets['test']\n",")\n","\n","# Run training\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"aTIOSaubUqNa"},"source":["- **Dataset Loading and Tokenization**: The IMDB dataset is loaded and tokenized using the BERT tokenizer. The preprocessing step ensures all inputs are padded and truncated to a uniform length necessary for batch processing.\n","\n","- **Model Setup**: A pre-trained BERT model is loaded and configured for sequence classification with two labels (positive and negative sentiments).\n","\n","- **TrainingArguments Configuration**: Critical training parameters are set up, including the directory for outputs, batch size, warmup steps, and the logging directory.\n","\n","- **Trainer Initialization and Execution**: The `Trainer` is set up with the model, training arguments, and the datasets, and the model is trained using a simple call to `trainer.train()`.\n"]},{"cell_type":"markdown","metadata":{"id":"TEjD2ETbUqNa"},"source":["## What Are Accelerators?"]},{"cell_type":"markdown","metadata":{"id":"ctVvozaYUqNb"},"source":["In the context of machine learning, particularly with the Hugging Face ecosystem, \"accelerators\" refer to tools that simplify and optimize the deployment of computation across different hardware configurations, such as CPUs, GPUs, and TPUs. The `Accelerator` class from Hugging Face's `accelerate` library is designed to abstract away most of the boilerplate code associated with device placement and parallel execution."]},{"cell_type":"markdown","metadata":{"id":"-SlgRh5hUqNb"},"source":["The `Accelerator` library automatically handles:\n","1. **Device Placement**: It automatically moves your models and data to the appropriate device (CPU, GPU, TPU).\n","2. **Distributed Computing**: It simplifies the setup for running models on multiple GPUs or TPUs with minimal changes to the codebase.\n","3. **Mixed Precision**: It supports automatic mixed precision to make training faster and more memory efficient without loss of accuracy.\n","4. **Dynamic Adjustment**: It adjusts computation and data loading based on the available hardware, optimizing performance without manual configuration."]},{"cell_type":"markdown","metadata":{"id":"Tyl2L67AUqNb"},"source":["By using `Accelerator`, developers can:\n","- Write hardware-agnostic code: Write once and run on any configuration (single GPU, multi-GPU, TPU, etc.).\n","- Easily implement distributed training: Simplify the complexity typically associated with scaling up training to multiple devices.\n","- Improve performance: Utilize optimizations like mixed precision automatically."]},{"cell_type":"markdown","metadata":{"id":"WXfLVDykUqNb"},"source":["Now let's create an example that uses the `Accelerator` tool to train a model on the IMDB dataset for sentiment classification."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-28T17:28:02.558157Z","iopub.status.busy":"2024-04-28T17:28:02.557320Z","iopub.status.idle":"2024-04-28T17:32:38.707549Z","shell.execute_reply":"2024-04-28T17:32:38.706501Z","shell.execute_reply.started":"2024-04-28T17:28:02.558122Z"},"id":"MWZ67dO3UqNb","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.8140441179275513\n","Loss: 0.014169598929584026\n","Loss: 0.0032063524704426527\n","Loss: 0.0017942762933671474\n","Loss: 0.0013011569390073419\n","Loss: 0.0010204471182078123\n","Loss: 0.0008431096794083714\n","Loss: 0.0006774751236662269\n","Loss: 0.00046975305303931236\n","Loss: 0.00035428686533123255\n","Loss: 0.00030852516647428274\n","Loss: 0.00023880728986114264\n","Loss: 0.00020591284555848688\n","Loss: 0.00017510310863144696\n","Loss: 0.00014232576359063387\n","Loss: 0.00013135997869540006\n","Loss: 0.00010662701970431954\n","Loss: 9.25617350731045e-05\n","Loss: 9.101210162043571e-05\n","Loss: 7.599303353345022e-05\n","Loss: 7.396662840619683e-05\n","Loss: 7.104620453901589e-05\n","Loss: 6.329812458716333e-05\n","Loss: 5.829164365422912e-05\n","Loss: 5.721882189391181e-05\n","Loss: 5.358314228942618e-05\n","Loss: 5.096067616250366e-05\n","Loss: 4.869580880040303e-05\n","Loss: 4.708655978902243e-05\n","Loss: 4.4404474465409294e-05\n","Loss: 4.1722378227859735e-05\n","Loss: 4.404685751069337e-05\n","Loss: 3.8980677345534787e-05\n","Loss: 3.832505171885714e-05\n","Loss: 3.7252208130666986e-05\n","Loss: 3.564294092939235e-05\n","Loss: 3.403367372811772e-05\n","Loss: 3.200718492735177e-05\n","Loss: 3.1530362321063876e-05\n","Loss: 3.0636318115284666e-05\n","Loss: 3.0755520128877833e-05\n","Loss: 2.884823152271565e-05\n","Loss: 2.7298556233290583e-05\n","Loss: 2.6464113034307957e-05\n","Loss: 2.586808113846928e-05\n","Loss: 2.6881332814809866e-05\n","Loss: 2.4318400392076e-05\n","Loss: 2.3781969503033906e-05\n","Loss: 2.2470698240795173e-05\n","Loss: 2.217268047388643e-05\n","Loss: 2.1815063519170508e-05\n","Loss: 2.0921012037433684e-05\n","Loss: 2.0921012037433684e-05\n","Loss: 1.960973531822674e-05\n","Loss: 1.9132909073960036e-05\n","Loss: 1.8715683836489916e-05\n","Loss: 1.8656079191714525e-05\n","Loss: 1.8298458599019796e-05\n","Loss: 1.7762025890988298e-05\n","Loss: 1.6927575416048057e-05\n","Loss: 1.7464008124079555e-05\n","Loss: 1.6271937056444585e-05\n","Loss: 1.603352211532183e-05\n"]}],"source":["from transformers import BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding\n","from datasets import load_dataset\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import AdamW\n","from accelerate import Accelerator\n","\n","# Load the dataset\n","dataset = load_dataset('imdb', split='train[:10%]')\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenization and data preparation function\n","def preprocess_function(examples):\n","    # Tokenize the text and ensure tensor conversion happens here\n","    tokenized_output = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=512, return_tensors='np')\n","    return tokenized_output\n","\n","# Apply preprocessing to tokenize dataset\n","tokenized_data = dataset.map(preprocess_function, remove_columns=['text'], batched=True)\n","\n","# create data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# DataLoader with default settings\n","data_loader = DataLoader(tokenized_data, batch_size=2, shuffle=True,  collate_fn=data_collator)\n","\n","# Load BERT for sequence classification\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Define the optimizer\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Initialize Accelerator\n","accelerator = Accelerator()\n","model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)\n","\n","# Training loop\n","for epoch in range(1):  # One epoch for demonstration\n","    for i, batch in enumerate(data_loader):\n","        data = {k: v for k, v in batch.items()}\n","        outputs = model(**data)\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        if i % 20 == 0:\n","            print(f\"Loss: {loss.item()}\")\n"]},{"cell_type":"markdown","metadata":{"id":"I4KDU6FJUqNb"},"source":["- **Dataset Loading and Processing**: Loads the IMDB dataset and processes it with a tokenizer to make it suitable for BERT.\n","- **Model and Optimizer Setup**: A BERT model is initialized along with an AdamW optimizer.\n","- **Accelerator Integration**: The model, optimizer, and dataset are prepared with the `Accelerator`. This step is crucial as it configures the model and other components to use the appropriate hardware and optimizations (like mixed precision).\n","- **Training Loop**: The model is trained with a simple loop, using `Accelerator` to handle device-specific operations and optimizations transparently."]}],"metadata":{"colab":{"include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
