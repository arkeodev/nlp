{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Hugging_Face/2-HF_Base_Libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u3owiBlVDFT"
      },
      "source": [
        "# Base Libraries (transforms and evaluate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xumamvt_VDFV"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVKh-_npVDFV"
      },
      "source": [
        "Each of the libraries selected below serves a specific purpose in the machine learning workflow, from model training and deployment (`transformers`, `accelerate`) to data handling (`datasets`) and performance evaluation (`evaluate`). These tools collectively provide a robust infrastructure for building, deploying, and managing NLP applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4xUsRoZVDFV"
      },
      "source": [
        "### Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Q0x6q3VDFV"
      },
      "source": [
        "\n",
        "**Definition:** The `transformers` library, developed by Hugging Face, is a comprehensive library for Natural Language Processing (NLP) which provides a wide range of pre-trained models designed to perform various tasks such as text classification, information extraction, question answering, summarization, translation, and more.\n",
        "\n",
        "**Some Models Overview:**\n",
        "- **BERT (Bidirectional Encoder Representations from Transformers):** Designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n",
        "- **GPT (Generative Pre-trained Transformer):** An autoregressive model that uses the left context to predict the next word in a sentence.\n",
        "- **RoBERTa (Robustly Optimized BERT Approach):** A variant of BERT that modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n",
        "- **T5 (Text-To-Text Transfer Transformer):** Converts all NLP problems into a text-to-text format, using a unified approach to handle any task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xqpzBrNVDFW"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhnDK3L5VDFW"
      },
      "source": [
        "**Definition:** The `evaluate` library complements `datasets` and `transformers` by providing a set of tools for model evaluation and comparison across a wide range of metrics. This library is designed to help in assessing the performance of machine learning models, particularly in the field of NLP.\n",
        "\n",
        "**Evaluation Methods:**\n",
        "- **Accuracy:** Measures the proportion of correct predictions among total predictions, commonly used in classification tasks.\n",
        "- **F1 Score:** Harmonic mean of precision and recall, useful in situations with imbalanced datasets.\n",
        "- **BLEU (Bilingual Evaluation Understudy):** Used primarily for evaluating machine translated text against a set of reference translations.\n",
        "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Commonly used in summarization tasks to compare the overlap between the generated summaries and a set of reference summaries.\n",
        "- **Perplexity:** Often used in language modeling to assess how well a probability model predicts a sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AELPIbA8VDFW"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h7ryzPRVDFW"
      },
      "source": [
        "In the realm of NLP tasks using transformer models, Hugging Face provides a highly efficient and user-friendly interface through the `pipeline` class. However, there are times when one might need more control over the process than what is offered by the `pipeline` class. In such cases, creating a manual pipeline can be beneficial. Below, I'll provide definitions and examples for both using the `pipeline` class and creating a manual pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2RWJNuCVDFW"
      },
      "source": [
        "### Pipeline Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLsR17S1VDFW"
      },
      "source": [
        "**Definition:**\n",
        "The `pipeline` class in Hugging Face's `transformers` library provides a high-level, easy-to-use interface for applying pre-trained models to various NLP tasks such as text classification, question answering, text generation, etc. This class abstracts away much of the preprocessing and postprocessing necessary to run predictions, making it accessible for users who may not be familiar with the intricacies of the models or NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GQcLIHbVDFW"
      },
      "source": [
        "Let's first import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJomZUlAVDFX",
        "outputId": "7526d09a-f34d-48a2-b028-294f5b9cd124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.37.2\n",
            "0.4.1\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "import evaluate\n",
        "print(evaluate.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLaWCroNVDFX"
      },
      "source": [
        "Here's how you would use the `pipeline` class for sentiment analysis:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t0qDtHmVDFX"
      },
      "source": [
        "#### 1. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fALT-yLOVDFY"
      },
      "source": [
        "**Model Used:** The `cardiffnlp/twitter-roberta-base-sentiment` model is a variant of the RoBERTa-base transformer model that has been specifically fine-tuned for sentiment analysis tasks on Twitter data. This model is hosted on Hugging Face's model hub and was developed by researchers at Cardiff University's NLP group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQSys7zOVDFY",
        "outputId": "d0380e4a-1f65-496b-eaec-7ae127fdd859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'LABEL_2', 'score': 0.824418306350708}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoModelForSequenceClassification\n",
        "\n",
        "# Using RoBERTa model, which might be more robust\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "result = sentiment_analyzer(\"Transformers are making NLP much simpler.\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbhb3EW-VDFY"
      },
      "source": [
        "The output shows a label and a score indicating the model's confidence in its prediction.\n",
        "\n",
        "For the `cardiffnlp/twitter-roberta-base-sentiment` model, these labels correspond to sentiment classes that the model was trained to predict. These are:\n",
        "\n",
        "- LABEL_0: Negative\n",
        "- LABEL_1: Neutral\n",
        "- LABEL_2: Positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WMBVsPSVDFY"
      },
      "source": [
        "#### 2. Zero-Shot Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1_Q_4QbVDFY"
      },
      "source": [
        "**Model Used:** BART large MNLI, designed for natural language inference tasks but adaptable for zero-shot classification. The model classifies an input sentence into predefined categories without needing labeled data for those categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9g3IchPVDFY",
        "outputId": "0a746806-a2a7-4929-a8cd-36edb6c47110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sequence': 'Discover the latest trends in machine learning.', 'labels': ['technology', 'health', 'finance', 'education'], 'scores': [0.9637537002563477, 0.016171658411622047, 0.010842711664736271, 0.00923205353319645]}\n"
          ]
        }
      ],
      "source": [
        "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Classify the given text into provided categories\n",
        "result = zero_shot_classifier(\n",
        "    \"Discover the latest trends in machine learning.\",\n",
        "    candidate_labels=[\"health\", \"technology\", \"finance\", \"education\"]\n",
        ")\n",
        "\n",
        "# Display the classification results\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIoVZ7auVDFZ"
      },
      "source": [
        "The output includes the text, labels, and scores for each label, showing how well the text fits into each category. Here as expectd the result is labeled as `technology`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhlLTH4wVDFZ"
      },
      "source": [
        "#### 3. Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ1-Y2JmVDFZ"
      },
      "source": [
        "**Model Used:** GPT-2, well-known for its ability to generate coherent and contextually relevant text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijW2bsC7VDFZ",
        "outputId": "57607a79-828d-49a2-f10b-16483ebb6fa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Artificial intelligence in healthcare \\xa0has been used extensively with \"personal health\" technologies for years as it is not even the first, common, technology used in pharmaceutical, diagnostic, and so on. However, what is interesting is that it was a'}]\n"
          ]
        }
      ],
      "source": [
        "text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Generate text starting from the given prompt\n",
        "generated_text = text_generator(\"Artificial intelligence in healthcare \", max_length=50, truncation=True)\n",
        "\n",
        "# Output the generated text\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJK4GapVDFZ"
      },
      "source": [
        "The model outputs a generated text with maximum size defined as 50 as a continuation of the input prompt. The text is creatively expanded, reflecting the model's understanding of the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlrfcNnXVDFZ"
      },
      "source": [
        "#### 4. Masked Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTKcTv9BVDFZ"
      },
      "source": [
        "**Model Used:** XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm2rAPPuVDFZ",
        "outputId": "06780d39-e9d1-4eeb-a8d8-71a88c779680"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'score': 0.24085475504398346, 'token': 16692, 'token_str': 'Donald', 'sequence': 'The current president of the United States is Donald.'}, {'score': 0.059821899980306625, 'token': 76140, 'token_str': 'Barack', 'sequence': 'The current president of the United States is Barack.'}, {'score': 0.05501473322510719, 'token': 14787, 'token_str': 'George', 'sequence': 'The current president of the United States is George.'}, {'score': 0.045015595853328705, 'token': 34202, 'token_str': 'President', 'sequence': 'The current president of the United States is President.'}, {'score': 0.04419546574354172, 'token': 70, 'token_str': 'the', 'sequence': 'The current president of the United States is the.'}]\n"
          ]
        }
      ],
      "source": [
        "mlm = pipeline(\"fill-mask\", model=\"FacebookAI/xlm-roberta-base\")\n",
        "\n",
        "# Fill in the blank in the sentence below\n",
        "result = mlm(\"The current president of the United States is <mask>.\")\n",
        "\n",
        "# Print the results\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6xC1GGAVDFa"
      },
      "source": [
        "The output suggests top fill-ins for the masked token, each with a confidence score.\n",
        "\n",
        "Even it doesn't find the current president correctly, it finds the previous presidents. This shows that the model successfully captured the context given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv9bFuz3VDFa"
      },
      "source": [
        "#### 5. Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnrPlNU6VDFa"
      },
      "source": [
        "**Model Used:** The `distilbert-base-cased-distilled-squad` model used here is a distilled version of BERT, optimized for the question-answering task on the Stanford Question Answering Dataset (SQuAD). DistilBERT retains most of the BERT's predictive power but is more lightweight and faster, making it suitable for environments where resources are limited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZWsvLo2VDFa",
        "outputId": "41eaede2-543b-4a48-e18f-390213b66a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'score': 0.9983620643615723, 'start': 0, 'end': 11, 'answer': 'Alan Turing'}\n"
          ]
        }
      ],
      "source": [
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "# Provide a context and a question whose answer lies within the context\n",
        "result = qa_pipeline({\n",
        "    'question': 'Who is known as the father of computer science?',\n",
        "    'context': 'Alan Turing is widely considered as the father of computer science.'\n",
        "})\n",
        "\n",
        "# Display the answer\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3gzfKUgVDFa"
      },
      "source": [
        "The output indicates a successful identification of the answer with high confidence, demonstrating the model's capability to effectively parse and understand natural language within the specified domain (as trained on SQuAD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NnYJ56WVDFa"
      },
      "source": [
        "#### 6. Translation Model Using Pipeline Class & BLEU Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaC9r8iOVDFb"
      },
      "source": [
        "**Model Used:** Helsinki-NLP's translation model trained for translating English to French. These models are part of the Opus-MT project and have been trained on a diverse set of languages from the OPUS collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3je_XSNwVDFb",
        "outputId": "62593d18-14d0-47a3-ad70-99020d5554c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference translation: Ceci est un exemple d'utilisation d'un modèle Hugging Face pour la traduction.\n",
            "Translated text: C'est un exemple d'utilisation d'un modèle Hugging Face pour la traduction.\n",
            "BLEU score: 0.8313539764691034\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from evaluate import load\n",
        "\n",
        "# Load the translation pipeline\n",
        "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "\n",
        "# Translate the text from English to French\n",
        "translated_text = translator(\"This is an example of using a Hugging Face model for translation.\")\n",
        "\n",
        "# This is our reference translation for evaluation\n",
        "reference_translation = [\"Ceci est un exemple d'utilisation d'un modèle Hugging Face pour la traduction.\"]\n",
        "print(f\"Reference translation: {reference_translation[0]}\")\n",
        "\n",
        "# Extract the translated text\n",
        "translated_text = translated_text[0]['translation_text']\n",
        "\n",
        "# Print the results\n",
        "print(f\"Translated text: {translated_text}\")\n",
        "\n",
        "# Load BLEU metric\n",
        "bleu = load('bleu')\n",
        "\n",
        "# Calculate BLEU score\n",
        "results = bleu.compute(predictions=[translated_text], references=[reference_translation])\n",
        "\n",
        "# Print the BLEU score\n",
        "print(f\"BLEU score: {results['bleu']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaO2HIYjVDFb"
      },
      "source": [
        "\n",
        "The `translation_text` field in the output contains the French translation of the English input text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17hKO60gVDFb"
      },
      "source": [
        "#### 7. Text Summarization Using Pipeline Class & ROUGE Metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf0AzQMlVDFc"
      },
      "source": [
        "**Model Used:** Bart-large-cnn, designed specifically for summarization tasks, trained on the CNN/DailyMail dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNS3tkIZVDFc",
        "outputId": "dd05991b-f954-407a-c540-04550fc2643c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarized text: The Hugging Face library offers state-of-the-art machine learning models for natural language processing tasks. The library is built on top of PyTorch and TensorFlow, giving users flexibility and ease\n",
            "Reference Summary: Hugging Face offers machine learning models for NLP based on PyTorch and TensorFlow.\n",
            "ROUGE scores: {'rouge1': 0.4888888888888889, 'rouge2': 0.27906976744186046, 'rougeL': 0.4888888888888889, 'rougeLsum': 0.4888888888888889}\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from evaluate import load\n",
        "\n",
        "# Load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Text to summarize\n",
        "article_text = \"\"\"\n",
        "The Hugging Face library offers state-of-the-art machine learning models for natural language processing tasks.\n",
        "It provides simple interfaces for NLP tasks such as text classification, translation, and more.\n",
        "The library is built on top of PyTorch and TensorFlow, giving users flexibility and ease of use in model training and deployment.\n",
        "\"\"\"\n",
        "\n",
        "# Perform summarization\n",
        "summary_text = summarizer(article_text, max_length=45, min_length=5, do_sample=False)\n",
        "\n",
        "# Print the summarized text\n",
        "print(f\"Summarized text: {summary_text[0]['summary_text']}\")\n",
        "\n",
        "# This is our reference summary for evaluation (typically a human-generated summary)\n",
        "reference_summary = \"Hugging Face offers machine learning models for NLP based on PyTorch and TensorFlow.\"\n",
        "print(f\"Reference Summary: {reference_summary}\")\n",
        "\n",
        "# Load ROUGE metric\n",
        "rouge = load('rouge')\n",
        "\n",
        "# Calculate ROUGE score\n",
        "rouge_results = rouge.compute(predictions=[summary_text[0]['summary_text']], references=[reference_summary])\n",
        "\n",
        "# Print the ROUGE scores\n",
        "print(f\"ROUGE scores: {rouge_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKVBK1G0VDFc"
      },
      "source": [
        "The ROUGE scores are a set of metrics to evaluate the quality of the summarization by comparing the machine-generated summary to a reference summary. Let's interpret each score:\n",
        "\n",
        "1. **ROUGE-1 (0.4888888888888889)**: This score reflects the overlap of unigrams (individual words) between the generated summary and the reference summary. A ROUGE-1 score of approximately 0.489 indicates that around 48.9% of the words in the reference summary appear in the generated summary. This suggests a moderate level of word overlap, indicating that nearly half of the words from the reference are captured in the generated text.\n",
        "\n",
        "2. **ROUGE-2 (0.27906976744186046)**: This metric looks at the overlap of bigrams (pairs of consecutive words). The ROUGE-2 score of approximately 0.279 means that about 27.9% of the consecutive word pairs in the reference summary are also found in the generated summary. This is typically lower than ROUGE-1 because it's more specific, capturing the sequence of words. The score here is lower, suggesting that there is less overlap in word pairs, indicating that the exact phrases from the reference are less commonly found in the generated summary.\n",
        "\n",
        "3. **ROUGE-L (0.4888888888888889)**: ROUGE-L measures the Longest Common Subsequence (LCS), considering the longest sequence of words that appears in both the generated summary and the reference summary. A score of about 0.489 indicates that the longest sequence of words that appear in both texts comprises around 48.9% of the reference summary. This suggests that there are some reasonably long subsequences shared between the two summaries.\n",
        "\n",
        "4. **ROUGE-Lsum (0.4888888888888889)**: This is similar to ROUGE-L but it considers the entire summary instead of only the longest sequence. It's useful for summarization where sentence-level structure matters. The score being the same as ROUGE-L indicates that the longest subsequence likely captures the most significant overlap in content between the generated and reference summaries.\n",
        "\n",
        "### Overall Interpretation\n",
        "The generated summary captures nearly half of the content of the reference summary in terms of single words and has a moderate level of overlap when it comes to longer phrases and sentence structures. The lower score in ROUGE-2 compared to ROUGE-1 and ROUGE-L suggests that while many correct keywords are included, the exact phrasing and order of words are not as closely matched. This could mean the summary is capturing the key concepts but may be paraphrasing or using different structures to convey the information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYNdCU3OVDFd"
      },
      "source": [
        "### Creating a Manual Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy_6nBjcVDFd"
      },
      "source": [
        "A manual pipeline involves more granular control over the steps in the processing of data through a transformer model. This typically includes:\n",
        "\n",
        "- loading the tokenizer and model\n",
        "- performing tokenization\n",
        "- running the model on these tokens\n",
        "- processing the output.\n",
        "\n",
        "This method is useful when customization is needed beyond what the `pipeline` class offers, such as batch processing, custom preprocessing or postprocessing steps, or using specific model configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT2N2xLDVDFd"
      },
      "source": [
        "Here’s how you would manually create a pipeline for sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01xAlK6dVDFd",
        "outputId": "4f94228d-da96-4f48-97b6-a0d3a4f480fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'label': 'POSITIVE', 'score': 0.9977321624755859}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Prepare input\n",
        "text = \"I love using transformers for machine learning.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# Run model\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Process output\n",
        "scores = torch.nn.functional.softmax(output.logits, dim=-1)\n",
        "predicted_label_index = scores.argmax().item()\n",
        "predicted_label = model.config.id2label[predicted_label_index]\n",
        "confidence = scores.max().item()\n",
        "\n",
        "print({\"label\": predicted_label, \"score\": confidence})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67UE6ojmVDFd"
      },
      "source": [
        "Here the code outputs a sentiment label and confidence score. As can be seen the model outputs a `Positive` output with a high confidence score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSIps96VVDFe"
      },
      "source": [
        "### Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ3fNq6-VDFe"
      },
      "source": [
        "Both methods offer powerful ways to leverage transformer models for NLP tasks, with the `pipeline` class being more suited for straightforward applications and quick testing. In contrast, the manual pipeline method provides greater flexibility and control, which can be essential for production environments or research purposes where custom behavior is necessary."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "huggingface",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
