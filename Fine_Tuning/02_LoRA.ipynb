{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Fine_Tuning/02_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5aHf5l5nzyo"
      },
      "source": [
        "$$\n",
        "\\begin{array}{c}\n",
        "\\text{$\\Large \"Everything\\ should\\ be\\ made\\ as\\ simple\\ as\\ possible,\\ but\\ not\\ simpler.\"$} \\\\\n",
        "{\\text{{$\\small Albert\\ Einstein$}}} \\\\\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSpxIkEVnzyq"
      },
      "source": [
        "# LoRA Tuning with PEFT from Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO8e14Pxnzyq"
      },
      "source": [
        "## Introduction to LoRA and PEFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwMqdVBPnzyq"
      },
      "source": [
        "### What is LoRA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhnjUUuDnzyq"
      },
      "source": [
        "Low-Rank Adaptation (LoRA) is a technique used to adapt large language models with a small number of parameters by decomposing the weight matrices into low-rank representations. This significantly reduces the number of trainable parameters and the computational resources required for fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Idj7uYnzyr"
      },
      "source": [
        "### What is PEFT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUMqf6FLnzyr"
      },
      "source": [
        "Parameter-Efficient Fine-Tuning (PEFT) is a library from Hugging Face that allows efficient fine-tuning of large models by leveraging techniques like LoRA. It makes it easy to integrate and apply these techniques in your projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhuU2ttGnzyr"
      },
      "source": [
        "## Step-by-Step Process of Fine-Tuning with LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkryDP26nzyr"
      },
      "source": [
        ". **Pre-Trained Model Initialization**:\n",
        "   - Start with a pre-trained model, such as GPT-3 or BERT. These models have already been trained on large corpora and have established strong baseline performance on various tasks.\n",
        "\n",
        "2. **Understand Low-Rank Adaptation (LoRA)**:\n",
        "   - **Objective**: LoRA aims to reduce the number of trainable parameters by injecting low-rank trainable matrices into each layer of the transformer model. This allows fine-tuning to be more efficient in terms of both computation and memory. Specifically, in the Transformer architecture, LoRA is typically applied to the weight matrices within the self-attention modules (query, key, and value projections) and can be extended to the MLP layers. This modularity allows for selective adaptation, which can be more efficient depending on the specific task requirements.\n",
        "\n",
        "   - **Concept**: Instead of updating the entire weight matrix of a transformer layer during fine-tuning, LoRA factorizes the weight updates into two low-rank matrices. This significantly reduces the number of parameters that need to be updated and stored.\n",
        "\n",
        "3. **Injecting LoRA Matrices**:\n",
        "   - **Original Weight Matrix**: Consider a weight matrix $( W \\in \\mathbb{R}^{d \\times d} )$ in a transformer layer.\n",
        "\n",
        "   - **Decomposition**: Decompose the weight update into two smaller matrices $( A \\in \\mathbb{R}^{d \\times r} )$ and $( B \\in \\mathbb{R}^{r \\times d} )$, where $( r )$ is the rank of the approximation and much smaller than $( d )$.\n",
        "\n",
        "   - **Modified Weight Update**: During fine-tuning, the weight matrix $( W )$ is modified as:\n",
        "     $$\n",
        "     W_{\\text{new}} = W + \\Delta W\n",
        "     $$\n",
        "     where $( \\Delta W = A \\times B )$.\n",
        "\n",
        "4. **Training Process with LoRA**:\n",
        "   - **Freeze Original Weights**: Keep the original pre-trained weights $( W )$ frozen and only update the matrices $( A )$ and $( B )$.\n",
        "\n",
        "   - **Forward Pass**: During the forward pass, compute the output using the modified weight matrix $( W_{\\text{new}} )$.\n",
        "\n",
        "   - **Backward Pass**: Compute gradients and update the low-rank matrices $( A )$ and $( B )$ only.\n",
        "\n",
        "5. **Implementation Details**:\n",
        "   - **Choosing Rank $( r )$**: The rank $( r )$ should be chosen such that it balances the trade-off between model capacity and computational efficiency. Common choices are small integers like 4 or 8.\n",
        "\n",
        "   - **Initialization**: Initialize $( A )$ and $( B )$ with small random values or using some form of pre-training.\n",
        "\n",
        "   - **Optimizer**: Use standard optimizers (e.g., Adam) to update $( A )$ and $( B )$.\n",
        "\n",
        "6. **Integration into Training Pipeline**:\n",
        "   - **Data Preparation**: Prepare your training data specific to the task you want to fine-tune the model on.\n",
        "\n",
        "   - **Training Loop**: Incorporate the LoRA adaptation into your training loop. Ensure that only $( A )$ and $( B )$ are updated during training.\n",
        "   \n",
        "   - **Evaluation**: After fine-tuning, evaluate the model on validation and test sets to ensure that the fine-tuning has improved performance on the specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpY4B57vnzys"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Fine_Tuning/images/LoRA.png\" width=\"1000\" height=\"400\" alt=\"LoRA\">\n",
        "    <figcaption>LoRA</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baYFGqOvnzys"
      },
      "source": [
        "## Benefits of Using LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmRMi-6Rnzys"
      },
      "source": [
        "- **Parameter Efficiency**: By reducing the number of trainable parameters, LoRA makes the fine-tuning process more memory efficient.\n",
        "\n",
        "- **Speed**: Fine-tuning with fewer parameters can be significantly faster, making it feasible to fine-tune large models on smaller datasets or with limited computational resources.\n",
        "\n",
        "- **Flexibility**: LoRA allows the adaptation of pre-trained models to new tasks without requiring extensive computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50vJLvrbnzys"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL9DX0qhnzys"
      },
      "source": [
        "### 1. Dataset Selection and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhWDXnpqnzyt"
      },
      "source": [
        "We'll use the \"Quora Question Pairs\" dataset from Kaggle, which contains pairs of questions and a label indicating if they are paraphrases.\n",
        "\n",
        "Now we'll install `kaggle` package and upload `kaggle.json` file from the computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCru__RLnzyt",
        "outputId": "5a74ee0e-1a80-45df-fbb0-da2bfcd77f8b",
        "colab": {
          "referenced_widgets": [
            "2668011214544690b2bfd1d9796c0da4"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2668011214544690b2bfd1d9796c0da4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value=(), accept='.json', description='Upload')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle API credentials are set up successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install the kaggle package\n",
        "! pip install kaggle -q\n",
        "\n",
        "# Navigate to the root\n",
        "import os\n",
        "import sys\n",
        "repo_root = os.path.abspath('..')\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "# Import the setup function from helpers.py\n",
        "from helpers.helpers import create_upload_widget\n",
        "\n",
        "# Call the function to create and display the upload widget\n",
        "create_upload_widget()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrDc4ZB0nzyu"
      },
      "source": [
        "Now it is needed to accept the rules to download the file. Here's how you can do it:\n",
        "\n",
        "1. Go to the [Quora Question Pairs competition page](https://www.kaggle.com/c/quora-question-pairs).\n",
        "2. Sign in with your Kaggle account.\n",
        "3. Click on the \"Rules\" tab.\n",
        "4. Scroll down and click the \"I Understand and Accept\" button.\n",
        "\n",
        "After accepting the rules, you can download the dataset using the Kaggle API in your Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr9Sj7b6nzyu",
        "outputId": "b906dc70-9753-4659-e0b2-277925866f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "quora-question-pairs.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  ./quora_question_pairs/test.csv.zip\n",
            "  inflating: ./quora_question_pairs/test.csv  \n",
            "Archive:  ./quora_question_pairs/train.csv.zip\n",
            "  inflating: ./quora_question_pairs/train.csv  \n"
          ]
        }
      ],
      "source": [
        "# Download the Quora Question Pairs dataset\n",
        "! kaggle competitions download -c quora-question-pairs\n",
        "\n",
        "# Unzip the dataset\n",
        "# ! unzip quora-question-pairs.zip -d quora_question_pairs\n",
        "! unzip -o ./quora_question_pairs/test.csv.zip -d ./quora_question_pairs\n",
        "! unzip -o ./quora_question_pairs/train.csv.zip -d ./quora_question_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xa5ruLznzyv"
      },
      "source": [
        "### 2. Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t149_aEOnzyv"
      },
      "source": [
        "We need to install and import the necessary libraries, including Hugging Face's Transformers, Datasets, and PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeT5bA_Nnzyv"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "! pip install transformers datasets peft -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xed7lSejnzyv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, PeftModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGhpG5wBnzyv"
      },
      "source": [
        "### 3. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnqGudfonzyv"
      },
      "source": [
        "Load and preprocess the dataset to make it suitable for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbTopTbonzyw",
        "outputId": "af93f64f-3f41-437c-e943-14fbb4ff5453",
        "colab": {
          "referenced_widgets": [
            "63988d43ccaa4bbc98da4c9e9fc5c47e",
            "f997910f160e4ac79eb65d882e139c08",
            "2aec22b2ef0f42f9beb7b5aaa2e440a0",
            "0c4d5ff19a5e4d45ac169aa0318e222f"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3z/z8_46rbd4g7d_7v62z_m36km0000gn/T/ipykernel_58717/995227424.py:7: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  test_df = pd.read_csv('quora_question_pairs/test.csv')\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63988d43ccaa4bbc98da4c9e9fc5c47e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/404287 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f997910f160e4ac79eb65d882e139c08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3563466 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2aec22b2ef0f42f9beb7b5aaa2e440a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/404287 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c4d5ff19a5e4d45ac169aa0318e222f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/3563466 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "    num_rows: 404287\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "    num_rows: 3563466\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the datasets as pandas dataframes for initial inspection\n",
        "train_df = pd.read_csv('quora_question_pairs/train.csv')\n",
        "test_df = pd.read_csv('quora_question_pairs/test.csv')\n",
        "\n",
        "# Remove any rows with missing data in important columns\n",
        "train_df.dropna(subset=['question1', 'question2', 'is_duplicate'], inplace=True)\n",
        "test_df.dropna(subset=['question1', 'question2'], inplace=True)\n",
        "\n",
        "# Drop the columns we don't need\n",
        "train_df = train_df[['question1', 'question2', 'is_duplicate']]\n",
        "test_df = test_df[['question1', 'question2']]\n",
        "\n",
        "# Convert the dataframes to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    try:\n",
        "        inputs = tokenizer(examples['question1'], examples['question2'], truncation=True, padding='max_length', max_length=128)\n",
        "        if 'is_duplicate' in examples:\n",
        "            inputs['labels'] = examples['is_duplicate']\n",
        "        return inputs\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Preprocess the datasets and handle errors\n",
        "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "encoded_test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n",
        "\n",
        "# Filter out empty results\n",
        "encoded_train_dataset = encoded_train_dataset.filter(lambda x: x['input_ids'] is not None)\n",
        "encoded_test_dataset = encoded_test_dataset.filter(lambda x: x['input_ids'] is not None)\n",
        "\n",
        "# Print to verify the datasets\n",
        "print(encoded_train_dataset)\n",
        "print(encoded_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebvdLym1nzyw"
      },
      "source": [
        "### 4. Model Selection and Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSwMgkcJnzyw"
      },
      "source": [
        "Select a pre-trained model and configure it for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWlDWjdsnzyw",
        "outputId": "1e8cc84f-d8e9-4403-d081-ab27722ed94d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Load a pre-trained model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Define LoRA (Low-Rank Adaptation) configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                   # Rank of the low-rank adaptation matrices. Controls the size of the low-rank projection.\n",
        "    lora_alpha=1,         # Scaling factor for the low-rank matrices. Balances the contribution of the low-rank matrices.\n",
        "    lora_dropout=0.1,      # Dropout rate applied to the low-rank adaptation matrices. Helps prevent overfitting.\n",
        "    bias=\"none\",           # Indicates whether to add a bias term to the low-rank adaptation. Options: \"none\", \"all\", \"lora_only\".\n",
        "    target_modules=[\"query\", \"key\", \"value\"]  # Specifies the target modules in the transformer layers to which LoRA is applied.\n",
        ")\n",
        "\n",
        "# Apply PEFT (Parameter-Efficient Fine-Tuning) using the defined LoRA configuration\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTQjUDzHnzyw"
      },
      "source": [
        "`lora_alpha` is a scaling factor applied to the low-rank adaptation matrices in the LoRA approach. The primary purpose of `lora_alpha` is to adjust the impact of the low-rank adaptation matrices on the model's parameters. It essentially scales the output of these low-rank matrices before adding them to the original model parameters.\n",
        "\n",
        "  Mathematically:\n",
        "  $$\n",
        "  W_{\\text{new}} = W + \\alpha (A \\cdot B)\n",
        "  $$\n",
        "\n",
        "Here, $( A \\cdot B )$ is the output of the low-rank adaptation, and $( \\alpha )$ is `lora_alpha`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1EsZPhknzyx"
      },
      "source": [
        "### 5. LoRA Tuning with PEFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1TtHv0Unzyx"
      },
      "source": [
        "Fine-tune the model using the Trainer API from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKp6zPv5nzyx",
        "outputId": "a0ab4e73-bfc1-42d3-fcce-1a21eea46900",
        "colab": {
          "referenced_widgets": [
            "b43d6d2b3b6b4bf6b2f62d5fa25f362a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b43d6d2b3b6b4bf6b2f62d5fa25f362a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/75804 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.11/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.11/site-packages/transformers/trainer.py:2772\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2772\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2775\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/huggingface/lib/python3.11/site-packages/transformers/trainer.py:2813\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 2813\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2814\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2815\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2816\u001b[0m         )\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2818\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_train_dataset,\n",
        "    eval_dataset=encoded_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUSxB05Ynzyx"
      },
      "source": [
        "### 6. Evaluation and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJTn4ywvnzyx"
      },
      "source": [
        "Evaluate the model on the test set and analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udMO_vNenzyy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "# Analyze the results\n",
        "def predict(question1, question2):\n",
        "    inputs = tokenizer(question1, question2, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    return probabilities\n",
        "\n",
        "# Test the model with example questions\n",
        "question1 = \"How do I cook pasta?\"\n",
        "question2 = \"What is the process of cooking pasta?\"\n",
        "print(predict(question1, question2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ9KNgHinzyy"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX8B6FL9nzyy"
      },
      "source": [
        "In this notebook, we have covered the process of fine-tuning a pre-trained language model using LoRA with the PEFT library from Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemkLfmNnzyy"
      },
      "source": [
        "## Additional Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C-AYRpsnzyy"
      },
      "source": [
        "- The original paper on arXiv: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "\n",
        "- Hugging Face LoRA apapter: [Hugging Face](https://huggingface.co/docs/peft/package_reference/lora)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "huggingface",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}