{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/simple_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMEXPh-5y2jJ"
      },
      "source": [
        "# Simple Transformers Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transformers are at the heart of modern natural language processing (NLP), driving innovations in language translation, text generation, question answering, and more. Introduced in the groundbreaking 2017 paper \"Attention is All You Need\" by Vaswani et al., transformers revolutionized the field by moving away from the traditional, computationally expensive sequence-to-sequence learning models like Recurrent Neural Networks (RNNs). Unlike RNNs, which process data step by step, transformers analyze entire sequences of data at once, drastically improving efficiency.\n",
        "\n",
        "At their core, transformers utilize a mechanism called \"self-attention,\" allowing them to weigh the importance of different parts of the input data differently. This ability to process all data simultaneously, rather than sequentially, transforms their computational complexity from linear to constant time, making them significantly faster and more scalable.\n",
        "\n",
        "Today, transformers are the foundation of many popular NLP models, including ChatGPT and Gemini, where the \"T\" in ChatGPT stands for Transformer. These models can understand and generate human-like text, translate languages, summarize documents, and much more, showcasing the versatility and power of transformer technology in tackling complex NLP challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Attention is All You Need (Base Paper): https://arxiv.org/pdf/1706.03762.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
