{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMoz/H7FsogBdZ6SyyHEvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Simple_Transformers/simple_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Architecture"
      ],
      "metadata": {
        "id": "tuU9A_QV-7PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the transformer architecture based on the description you've provided, let's break it down into its components and outline how each can be coded using PyTorch. The transformer model, as described in the paper \"Attention is All You Need\", consists of several key components: an Encoder and Decoder architecture, with each having multiple layers of self-attention and feed-forward neural networks."
      ],
      "metadata": {
        "id": "x26jAFlT_tuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This guide and explanation of the Transformer architecture draws extensively from \"The Annotated Transformer\" by Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman, building upon the original work by Sasha Rush. This comprehensive article offers a detailed line-by-line implementation of the Transformer model, providing insights into its inner workings and illustrating the theoretical concepts with practical code examples. It serves as an invaluable resource for understanding and implementing the Transformer model."
      ],
      "metadata": {
        "id": "HEx3cKI_BOfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Transformer Architecture"
      ],
      "metadata": {
        "id": "NIpdPfd1Hm28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Simple_Transformers/images/overall-architecture-of-transformers.png\" width=\"400\" height=\"400\" alt=\"Transformers Architecture\">\n",
        "    <figcaption>Transformers Architecture</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "yGDK_at2HtYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a simplified version of how you can implement these components in PyTorch:\n"
      ],
      "metadata": {
        "id": "gId0RUwtB6br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Setup"
      ],
      "metadata": {
        "id": "1-CjaKz4B8X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, ensure all required packages are installed and import necessary libraries."
      ],
      "metadata": {
        "id": "UkesXoMgB_Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "49ToAoDBCFLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Components of the Transformer"
      ],
      "metadata": {
        "id": "IMUoEXHACCq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "CnLqaozUCP54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core of the transformer model is the attention mechanism."
      ],
      "metadata": {
        "id": "JCDwbr31CTcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "metadata": {
        "id": "U38lQRe2CXRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diving into Self-Attention"
      ],
      "metadata": {
        "id": "VOS3epRYH6BU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism is what allows Transformers to process data in parallel. It assigns a weight to each element in the input sequence, based on how relevant each element is to every other element. Self-attention can be described with three main components: Queries, Keys, and Values.\n",
        "\n",
        "- **Queries**: A set of vectors that is matched against the keys to decide the most important elements in the sequence.\n",
        "- **Keys**: Vectors that are paired with values; they are used to extract the information that queries look for.\n",
        "- **Values**: Vectors that contain the actual information of each element in the sequence that is extracted based on the weightage from the keys.\n",
        "\n",
        "Imagine you are in a library with a huge collection of books (the sequence), and you are looking for information on a specific topic.\n",
        "\n",
        "- The **query** is like your question about the topic you’re interested in.\n",
        "- The **keys** represent the index or summary of each book.\n",
        "- The **values** are the actual contents of the books.\n",
        "\n",
        "The librarian (the self-attention mechanism) checks your question against all summaries (keys) to determine which books (values) have the information you need. This process is done simultaneously for all the questions in parallel, which is what makes the transformer model so powerful and efficient."
      ],
      "metadata": {
        "id": "XF36kRWPH-y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following formula represents the scaled dot-product attention, which is the foundation of self-attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Where $( Q )$, $( K )$, and $( V )$ are the matrices for queries, keys, and values respectively, and $( d_k )$ is the dimension of the keys.\n",
        "\n",
        "In code, assuming we have matrices for queries, keys, and values, it can be implemented simply as:"
      ],
      "metadata": {
        "id": "u283a318IBSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x over the specified axis.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
        "\n",
        "# Define the scaled dot product attention function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "    dk = K.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "    attention_weights = softmax(scaled_attention_logits)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Let's consider two sentences for our demonstration\n",
        "sentences = [\"The cat sits on the mat\", \"A dog lies on the rug\"]\n",
        "\n",
        "# Define embeddings for each unique word in the sentences\n",
        "# Embeddings are crafted so that similar words have closer embeddings\n",
        "word_embeddings = {\n",
        "    \"the\": np.array([1, 0, 0, 0, 0, 0, 0, 0]),\n",
        "    \"cat\": np.array([0, 1, 0, 0, 0.1, 0.2, 0.3, 0.4]),\n",
        "    \"sits\": np.array([0, 0.9, 1, 0, 0.2, 0.1, 0.4, 0.3]),\n",
        "    \"on\": np.array([0, 0, 0, 1, 0, 0, 0, 0]),\n",
        "    \"mat\": np.array([0, 0.8, 0.6, 0.3, 0, 0.6, 0.3, 0.2]),\n",
        "    \"a\": np.array([1, 0, 0, 0, 0, 0, 0, 0.1]),\n",
        "    \"dog\": np.array([0, 0.9, 0.1, 0, 0, 0.3, 0.4, 0.3]),\n",
        "    \"lies\": np.array([0, 1, 0.8, 0.1, 0.3, 0.1, 0.4, 0.2]),\n",
        "    \"rug\": np.array([0, 0.9, 0.6, 0.3, 0, 0.5, 0.3, 0.1])\n",
        "}\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
        "\n",
        "# Convert sentences to embeddings using the predefined embeddings\n",
        "sentence_embeddings = [[word_embeddings[word] for word in sentence] for sentence in tokenized_sentences]\n",
        "\n",
        "# Pad sentences to the same length\n",
        "max_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "padded_embeddings = [np.array(sentence + [np.zeros(3)] * (max_length - len(sentence))) for sentence in sentence_embeddings]\n",
        "\n",
        "# Stack the embeddings to create the Q, K, and V matrices\n",
        "Q = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "K = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "V = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "\n",
        "# Apply the scaled dot product attention function\n",
        "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Let's print the attention weights for the first sentence\n",
        "print(\"Attention weights for the first sentence:\")\n",
        "print(attention_weights[0])\n",
        "\n",
        "# Let's print the attention weights for the second sentence\n",
        "print(\"Attention weights for the second sentence:\")\n",
        "print(attention_weights[1])"
      ],
      "metadata": {
        "id": "5Lo6pNiPIJ3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For the first sentence \"The cat sits on the mat\":**\n",
        "\n",
        "1. The attention weights for the word \"the\" (first row) seem to be distributed across \"the\", \"sits\", and \"mat\" with higher weights (0.2079) compared to \"cat\" and \"on\" (0.1460). This might indicate that the model perceives a stronger association between \"the\" and the action (\"sits\") and the object (\"mat\") of the sentence.\n",
        "\n",
        "2. The word \"cat\" (second row) has the highest attention weight when paired with itself (0.2091) and with \"sits\" (0.2004), which could suggest that the model identifies \"cat\" and the action it's performing as key components of the sentence.\n",
        "\n",
        "3. The word, \"sits\" (third row) has the highest attention weight in combination with itself (0.2521) and substantial weight with \"cat\" (0.1814), potentially indicating the model's understanding of the importance of the verb in the context of the subject.\n",
        "\n",
        "4. Finally, \"mat\" (sixth row) pays more attention to \"sits\" (0.2152) as expected.\n",
        "\n",
        "**For the second sentence \"A dog lies on the rug\":**\n",
        "\n",
        "1. \"Dog\" (second row) shows higher attention weights for itself (0.1989) and \"lies\" (0.2039), which is consistent with recognizing the subject and verb relationship.\n",
        "\n",
        "3. \"Lies\" (third row) has the highest weight with itself (0.2388), and a notable weight with \"dog\" (0.1851) and \"rug\" (0.2110), reinforcing the connection between the subject, object and the action.\n",
        "\n",
        "4. \"Rug\" (sixth row), much like \"mat\" in the first sentence, shows a relatively high weight for \"dog\" (0.1854) and \"lies\" (0.2158), as expected.\n",
        "\n",
        "In both sentences, it is evident that the attention mechanism is picking up on the grammatical structure and the relationships between subjects, verbs, and objects. It emphasizes self-attention and relationships that make sense in the context of the sentences."
      ],
      "metadata": {
        "id": "RskdI0JuIPZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention"
      ],
      "metadata": {
        "id": "LDmzVYO7Caqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allows the model to jointly attend to information from different representation subspaces."
      ],
      "metadata": {
        "id": "3pG3peUkCdWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
        "        x, self.attn = scaled_dot_product_attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "4RL7T6pwCfnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feed-Forward Networks"
      ],
      "metadata": {
        "id": "a-vmKmdLCkkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each layer in the encoder and decoder contains a fully connected feed-forward network."
      ],
      "metadata": {
        "id": "P3pxg70tCncl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "metadata": {
        "id": "RowBB9YlCqiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings and Softmax"
      ],
      "metadata": {
        "id": "d32j_xZuCt95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Convert input tokens and output tokens to vectors."
      ],
      "metadata": {
        "id": "XfvJ6aQHCxl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(d_model)"
      ],
      "metadata": {
        "id": "_TcjR--JC0Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Transformer Model"
      ],
      "metadata": {
        "id": "8v85Es50DHEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's put together the main Transformer architecture combining the encoder, decoder, and other components previously described."
      ],
      "metadata": {
        "id": "vSmgcW5mDKOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        c = copy.deepcopy\n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
        "        self.decoder = Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N)\n",
        "        self.src_embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))\n",
        "        self.tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))\n",
        "        self.generator = Generator(d_model, tgt_vocab)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "metadata": {
        "id": "R0ijbTIEDOAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Infrastructure"
      ],
      "metadata": {
        "id": "qEgn4rM5DRxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a simple training loop and data generation for a copy task, which will serve as our \"simple task\". For real data, you would replace the data generation and possibly the training loop to suit your dataset and task."
      ],
      "metadata": {
        "id": "qcGRhZH_DVA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_generator, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(data_generator):\n",
        "            src = batch[0]\n",
        "            tgt = batch[1][:, :-1]\n",
        "            tgt_y = batch[1][:, 1:]\n",
        "            src_mask = (src != 0).unsqueeze(-2)\n",
        "            tgt_mask = make_std_mask(tgt, 0)\n",
        "            out = model(src, tgt, src_mask, tgt_mask)\n",
        "            loss = (out - tgt_y).mean()  # Simplified loss calculation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch} Loss {total_loss / len(data_generator)}\")\n",
        "\n",
        "def data_gen(V, batch, nbatches):\n",
        "    \"Generate random data for a src-tgt copy task.\"\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        data[:, 0] = 1  # Start token\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        yield Batch(src, tgt, 0)"
      ],
      "metadata": {
        "id": "Mz76xVVpDYo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Training"
      ],
      "metadata": {
        "id": "CrOQYwLKDcJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put it all together and run the training for our Transformer model on the simple copy task."
      ],
      "metadata": {
        "id": "M9P96Sv-DgMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "V = 11  # Vocabulary size\n",
        "N = 6  # Number of layers\n",
        "d_model = 512  # Embedding dimension\n",
        "d_ff = 2048  # Feed-forward dimension\n",
        "h = 8  # Number of heads\n",
        "\n",
        "# Instantiate model\n",
        "model = Transformer(V, V, N=N, d_model=d_model, d_ff=d_ff, h=h)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Data generation\n",
        "gen = data_gen(V, 30, 20)\n",
        "\n",
        "# Training\n",
        "train(model, gen, optimizer, num_epochs=10)"
      ],
      "metadata": {
        "id": "zGdyMGH4DjKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying the Model to Real Data"
      ],
      "metadata": {
        "id": "P466rmc9DrTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This implementation is quite high-level and abstracts away many details for brevity. When working with real-world datasets, consider using existing frameworks like Hugging Face's Transformers, which provide pre-implemented versions of Transformer models and utilities for processing data, or adjust the data loading and processing parts of this guide to fit your specific needs."
      ],
      "metadata": {
        "id": "6nzCLBiyDAi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n"
      ],
      "metadata": {
        "id": "bKW0UC-pBdlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Huang, A., Subramanian, S., Sum, J., Almubarak, K., & Biderman, S. (2022). *The Annotated Transformer*. Original by Sasha Rush. Retrieved from [https://nlp.seas.harvard.edu/annotated-transformer/](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "\n",
        "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). *Attention is All You Need*. In Advances in Neural Information Processing Systems (NIPS). Retrieved from [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)"
      ],
      "metadata": {
        "id": "bbIDMOtCBf3n"
      }
    }
  ]
}