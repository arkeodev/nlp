{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Simple_Transformers/simple_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuU9A_QV-7PE"
      },
      "source": [
        "# Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x26jAFlT_tuR"
      },
      "source": [
        "To implement the transformer architecture based on the description you've provided, let's break it down into its components and outline how each can be coded using PyTorch. The transformer model, as described in the paper \"Attention is All You Need\", consists of several key components: an Encoder and Decoder architecture, with each having multiple layers of self-attention and feed-forward neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEx3cKI_BOfd"
      },
      "source": [
        "This guide and explanation of the Transformer architecture draws extensively from \"The Annotated Transformer\" by Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman, building upon the original work by Sasha Rush. This comprehensive article offers a detailed line-by-line implementation of the Transformer model, providing insights into its inner workings and illustrating the theoretical concepts with practical code examples. It serves as an invaluable resource for understanding and implementing the Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Complexity Comparison"
      ],
      "metadata": {
        "id": "sKdKEQX3E6EY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/time-complexity-of-transformers.png\" width=\"1000\" height=\"300\" alt=\"Time complexity\">\n",
        "    <figcaption>Time complexity</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "3zLRb49eETsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table from Vaswani et al. (2017), which is from the seminal paper \"Attention is All You Need,\" compares different neural network layer types across three dimensions: complexity per layer, number of sequential operations, and maximum path length:\n",
        "\n",
        "- **Self-Attention Layer**:\n",
        "  - Computational complexity per layer is $ O(n^2 \\cdot d) $, meaning it scales quadratically with the sequence length and linearly with the dimensionality of the representation.\n",
        "  - Sequential operations are constant $ O(1) $, allowing for high parallelization during training and inference.\n",
        "  - Maximum path length is also constant $ O(1) $, ensuring that long-range dependencies can be learned effectively as any position in the sequence can directly attend to any other position.\n",
        "\n",
        "- **Recurrent Layer**:\n",
        "  - Complexity is $ O(n \\cdot d^2) $, which scales linearly with the sequence length, making it less efficient for longer sequences compared to self-attention.\n",
        "  - Requires $ O(n) $ sequential operations, limiting parallelization and potentially increasing training time.\n",
        "  - Has a linear maximum path length $ O(n) $, which can make learning long-range dependencies more challenging.\n",
        "\n",
        "- **Convolutional Layer**:\n",
        "  - Shows a complexity of $ O(k \\cdot n \\cdot d^2) $, where $ k $ is the kernel size, suggesting an increase in computation if larger receptive fields are needed.\n",
        "  - Has constant sequential operations $ O(1) $, similar to self-attention, facilitating parallelization.\n",
        "  - Exhibits a logarithmic maximum path length $ O(\\log_k(n)) $, improving the learning of dependencies over recurrent layers but potentially less effective than self-attention for very long sequences.\n",
        "\n",
        "- **Restricted Self-Attention Layer (or Local Attention)**:\n",
        "  - Complexity per layer is reduced to $ O(r \\cdot n \\cdot d) $ by restricting attention to a window of $ r $ surrounding tokens, balancing computational efficiency and the ability to capture dependencies.\n",
        "  - Maintains constant sequential operations $ O(1) $, benefiting parallelization.\n",
        "  - Features a maximum path length of $ O(n/r) $, indicating a compromise in the ability to learn dependencies as the sequence length increases compared to full self-attention."
      ],
      "metadata": {
        "id": "bTfRpR-CEUWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dive into the self-attention mechanism, which forms the core component of transformers."
      ],
      "metadata": {
        "id": "s8xn6oKFFVj5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOS3epRYH6BU"
      },
      "source": [
        "## Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism is what allows Transformers to process data in parallel. It assigns a weight to each element in the input sequence, based on how relevant each element is to every other element."
      ],
      "metadata": {
        "id": "fUuZ62MidIOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of Self Attention"
      ],
      "metadata": {
        "id": "Heh5NsV5dKoG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF36kRWPH-y2"
      },
      "source": [
        "Self-attention can be described with three main components: Queries, Keys, and Values.\n",
        "\n",
        "- **Queries**: A set of vectors that is matched against the keys to decide the most important elements in the sequence.\n",
        "- **Keys**: Vectors that are paired with values; they are used to extract the information that queries look for.\n",
        "- **Values**: Vectors that contain the actual information of each element in the sequence that is extracted based on the weightage from the keys.\n",
        "\n",
        "Imagine you are in a library with a huge collection of books (the sequence), and you are looking for information on a specific topic.\n",
        "\n",
        "- The **query** is like your question about the topic youâ€™re interested in.\n",
        "- The **keys** represent the index or summary of each book.\n",
        "- The **values** are the actual contents of the books.\n",
        "\n",
        "The librarian (the self-attention mechanism) checks your question against all summaries (keys) to determine which books (values) have the information you need. This process is done simultaneously for all the questions in parallel, which is what makes the transformer model so powerful and efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative Attention Approaches"
      ],
      "metadata": {
        "id": "9D1aD6_XVi2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Additive Attention**: While the scaled dot-product attention is widely used, alternative attention mechanisms exist, such as additive (or Bahdanau) attention, which uses a feed-forward network to compute attention scores instead of dot products. Each variant has its own advantages and use cases.\n",
        "\n",
        "- **Multi-Head Attention**: The Transformer model extends the basic scaled dot-product attention mechanism through the use of multi-head attention, where the model runs several attention processes in parallel. This approach allows the model to simultaneously attend to information from different representation subspaces at different positions, improving the model's ability to capture various types of relationships in the data.\n",
        "\n",
        "- **Self-Attention vs. Cross-Attention**: The module as described implements self-attention, where queries, keys, and values all come from the same source. In cross-attention settings, keys and values can come from a different source than queries, enabling interactions between different sequences (e.g., between an encoder and a decoder in a sequence-to-sequence model)."
      ],
      "metadata": {
        "id": "uOdKUvIgVorf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mathemetical Foundation of Attention"
      ],
      "metadata": {
        "id": "xJXcjF_aOU8s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u283a318IBSi"
      },
      "source": [
        "The following formula represents the scaled dot-product attention, which is the foundation of self-attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Components and Dimensions:**\n",
        "\n",
        "- $( Q )$: Queries matrix of dimension $( m \\times d_k )$, where $( m )$ is the number of queries and $( d_k )$ is the dimension of each query vector.\n",
        "- $( K )$: Keys matrix of dimension $( n \\times d_k )$, where $( n )$ is the number of keys (and values) and $( d_k )$ is the dimension of each key vector.\n",
        "- $( V )$: Values matrix of dimension $( n \\times d_v )$, where $( n )$ is the number of values and $( d_v )$ is the dimension of each value vector.\n",
        "- $( d_k )$: Dimension of the keys (and queries), used for scaling the dot product scores."
      ],
      "metadata": {
        "id": "5sjro7b7ZPaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Dot Product of $( Q )$ and $( K^T )$**\n",
        "\n",
        "- $( QK^T )$: Since $( Q )$ is $( m \\times d_k )$ and $( K )$ is $( n \\times d_k )$, when we take the transpose of $( K )$ to get $( K^T )$ which is $( d_k \\times n )$, the resulting dot product $( QK^T )$ will be a matrix of dimension $( m \\times n )$. Each element of this matrix represents a score reflecting the relevance of a query to a key.\n",
        "\n",
        "- This step calculates the similarity between all queries and all keys, serving as the basis for determining how the values should be weighted according to each query."
      ],
      "metadata": {
        "id": "UrLWhqd2ZZNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Scaling by $( \\sqrt{d_k} )$**\n",
        "\n",
        "- The scores in the $( m \\times n )$ matrix are scaled down by $( \\sqrt{d_k} )$, which doesn't change the dimensions of the matrix but normalizes the scores to prevent them from becoming too large.\n",
        "\n",
        "- Scaling helps maintain numerical stability, particularly in the softmax step, by ensuring the scores don't lead to extremely small gradients when the dimensionality $( d_k )$ is large."
      ],
      "metadata": {
        "id": "W45x5F9Rap3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Softmax Function**\n",
        "\n",
        "- The softmax function is applied across the rows of the scaled $( m \\times n )$ matrix. The dimensionality of the output remains $( m \\times n )$, but now each row sums to 1, representing probabilities.\n",
        "\n",
        "- Converting the scores to probabilities allows the model to probabilistically decide which keys (and thus, which values) are most relevant to each query, enabling a soft selection mechanism."
      ],
      "metadata": {
        "id": "yFLfIefQaz_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Multiplication by $( V )$**\n",
        "\n",
        "- The softmax output, still $( m \\times n )$, is then multiplied by $( V )$, which is $( n \\times d_v )$. The resulting matrix will have dimensions $( m \\times d_v )$, representing the final output of the attention mechanism.\n",
        "\n",
        "- This step effectively combines the values based on their relevance to each query, as determined by the weighted scores. The final dimension $( m \\times d_v )$ corresponds to the weighted sum of values for each query, now ready for further processing in the model.\n"
      ],
      "metadata": {
        "id": "J32Z64AEYzDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Implementation"
      ],
      "metadata": {
        "id": "jEdINcjFYuY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lo6pNiPIJ3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265ea9a5-2c90-4b0f-e7ec-c8e6b39ff10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights for the first sentence:\n",
            "[[0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.1320772  0.20914044 0.20045296 0.1320772  0.1320772  0.194175  ]\n",
            " [0.11958619 0.18149541 0.25215275 0.11958619 0.11958619 0.20759326]\n",
            " [0.15299844 0.15299844 0.15299844 0.21788799 0.15299844 0.17011824]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12397355 0.18226132 0.21520941 0.13784561 0.12397355 0.21673656]]\n",
            "Attention weights for the second sentence:\n",
            "[[0.20789086 0.14701445 0.1464956  0.14546337 0.20715715 0.14597857]\n",
            " [0.13342497 0.19895022 0.20393542 0.13201726 0.13201726 0.19965486]\n",
            " [0.12073931 0.18519945 0.23888728 0.12420309 0.11988857 0.2110823 ]\n",
            " [0.15216063 0.15216063 0.15763656 0.21669485 0.15216063 0.16918669]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12305363 0.18544201 0.21589025 0.13633987 0.12261934 0.21665489]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x over the specified axis.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
        "\n",
        "# Define the scaled dot product attention function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "    dk = K.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "    attention_weights = softmax(scaled_attention_logits)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Let's consider two sentences for our demonstration\n",
        "sentences = [\"The cat sits on the mat\", \"A dog lies on the rug\"]\n",
        "\n",
        "# Define embeddings for each unique word in the sentences\n",
        "# Embeddings are crafted so that similar words have closer embeddings\n",
        "word_embeddings = {\n",
        "    \"the\": np.array([1, 0, 0, 0, 0, 0, 0, 0]),\n",
        "    \"cat\": np.array([0, 1, 0, 0, 0.1, 0.2, 0.3, 0.4]),\n",
        "    \"sits\": np.array([0, 0.9, 1, 0, 0.2, 0.1, 0.4, 0.3]),\n",
        "    \"on\": np.array([0, 0, 0, 1, 0, 0, 0, 0]),\n",
        "    \"mat\": np.array([0, 0.8, 0.6, 0.3, 0, 0.6, 0.3, 0.2]),\n",
        "    \"a\": np.array([1, 0, 0, 0, 0, 0, 0, 0.1]),\n",
        "    \"dog\": np.array([0, 0.9, 0.1, 0, 0, 0.3, 0.4, 0.3]),\n",
        "    \"lies\": np.array([0, 1, 0.8, 0.1, 0.3, 0.1, 0.4, 0.2]),\n",
        "    \"rug\": np.array([0, 0.9, 0.6, 0.3, 0, 0.5, 0.3, 0.1])\n",
        "}\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
        "\n",
        "# Convert sentences to embeddings using the predefined embeddings\n",
        "sentence_embeddings = [[word_embeddings[word] for word in sentence] for sentence in tokenized_sentences]\n",
        "\n",
        "# Pad sentences to the same length\n",
        "max_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "padded_embeddings = [np.array(sentence + [np.zeros(3)] * (max_length - len(sentence))) for sentence in sentence_embeddings]\n",
        "\n",
        "# Stack the embeddings to create the Q, K, and V matrices\n",
        "Q = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "K = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "V = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "\n",
        "# Apply the scaled dot product attention function\n",
        "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Let's print the attention weights for the first sentence\n",
        "print(\"Attention weights for the first sentence:\")\n",
        "print(attention_weights[0])\n",
        "\n",
        "# Let's print the attention weights for the second sentence\n",
        "print(\"Attention weights for the second sentence:\")\n",
        "print(attention_weights[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RskdI0JuIPZ9"
      },
      "source": [
        "**For the first sentence \"The cat sits on the mat\":**\n",
        "\n",
        "1. The attention weights for the word \"the\" (first row) seem to be distributed across \"the\", \"sits\", and \"mat\" with higher weights (0.2079) compared to \"cat\" and \"on\" (0.1460). This might indicate that the model perceives a stronger association between \"the\" and the action (\"sits\") and the object (\"mat\") of the sentence.\n",
        "\n",
        "2. The word \"cat\" (second row) has the highest attention weight when paired with itself (0.2091) and with \"sits\" (0.2004), which could suggest that the model identifies \"cat\" and the action it's performing as key components of the sentence.\n",
        "\n",
        "3. The word, \"sits\" (third row) has the highest attention weight in combination with itself (0.2521) and substantial weight with \"cat\" (0.1814), potentially indicating the model's understanding of the importance of the verb in the context of the subject.\n",
        "\n",
        "4. Finally, \"mat\" (sixth row) pays more attention to \"sits\" (0.2152) as expected.\n",
        "\n",
        "**For the second sentence \"A dog lies on the rug\":**\n",
        "\n",
        "1. \"Dog\" (second row) shows higher attention weights for itself (0.1989) and \"lies\" (0.2039), which is consistent with recognizing the subject and verb relationship.\n",
        "\n",
        "3. \"Lies\" (third row) has the highest weight with itself (0.2388), and a notable weight with \"dog\" (0.1851) and \"rug\" (0.2110), reinforcing the connection between the subject, object and the action.\n",
        "\n",
        "4. \"Rug\" (sixth row), much like \"mat\" in the first sentence, shows a relatively high weight for \"dog\" (0.1854) and \"lies\" (0.2158), as expected.\n",
        "\n",
        "In both sentences, it is evident that the attention mechanism is picking up on the grammatical structure and the relationships between subjects, verbs, and objects. It emphasizes self-attention and relationships that make sense in the context of the sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation the Model"
      ],
      "metadata": {
        "id": "hXnStVipd1oK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-CjaKz4B8X_"
      },
      "source": [
        "## Preliminary Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkesXoMgB_Vc"
      },
      "source": [
        "First, ensure all required packages are installed and import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49ToAoDBCFLR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "rd5J2JpDH5HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to produce N identical layers\n",
        "def clones(module, N):\n",
        "  \"Produce N identical layers.\"\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "YKKM9QA6H2ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUoEXHACCq3"
      },
      "source": [
        "## Core Components of the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnLqaozUCP54"
      },
      "source": [
        "### Scaled Dot-Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCDwbr31CTcO"
      },
      "source": [
        "The core of the transformer model is the attention mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/attention.png\" width=\"180\" height=\"300\" alt=\"Attention Mechanism\">\n",
        "    <figcaption>Attention Mechanism</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "tVJwk5k_FibE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U38lQRe2CXRh"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout=None):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        # Initialize dropout for attention scores, if provided.\n",
        "        self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Dimension of the key vectors, used for scaling down the dot products.\n",
        "        d_k = query.size(-1)\n",
        "        # Compute the dot products between queries and keys for each batch and head,\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "        # Scale the results\n",
        "        scaled_score = scores / math.sqrt(d_k)\n",
        "        # Apply masking if provided (for attention to consider only certain positions).\n",
        "        if mask is not None:\n",
        "            scaled_score = scaled_score.masked_fill(mask == 0, -1e9)\n",
        "        # Apply softmax to obtain attention probabilities.\n",
        "        p_attn = F.softmax(scaled_score, dim=-1)\n",
        "        # Optionally apply dropout to attention scores.\n",
        "        if self.dropout is not None:\n",
        "            p_attn = self.dropout(p_attn)\n",
        "        # Weight the values by the computed attention probabilities.\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ScaledDotProductAttention` module implements the attention mechanism that is at the heart of the Transformer model's effectiveness. Its roles and effects on the structure are as follows:\n",
        "\n",
        "- **Attention Calculation**: This module computes the attention scores by taking the dot product of the query with the key. It scales the dot product by the square root of the dimensionality of the key to prevent extremely small gradients when the dimensionality is large. This scaling helps stabilize gradients during training.\n",
        "\n",
        "- **Masking**: The optional mask allows the model to selectively ignore certain positions within the input sequence. This is particularly useful for masking out padding tokens in the input sequence or for preventing the model from peeking at future tokens when processing sequences in an autoregressive manner (e.g., during training a language model).\n",
        "\n",
        "- **Dropout**: Applying dropout to the attention scores is a regularization technique that helps prevent overfitting by randomly zeroing out some of the scores before computing the final attention weights.\n",
        "\n",
        "- **Softmax**: The softmax function converts the attention scores into probabilities, ensuring that they are non-negative and sum up to 1. This step allows the model to essentially \"focus\" on the most relevant parts of the input.\n",
        "\n",
        "- **Output**: The module outputs the weighted sum of the value vectors, scaled by the attention probabilities, which is then used in subsequent layers of the model. It also returns the attention probabilities themselves, which can be useful for analysis or for visualizing the model's attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "FK4gLRowVdWH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDmzVYO7Caqs"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pG3peUkCdWE"
      },
      "source": [
        "Allows the model to jointly attend to information from different representation subspaces."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/mutl-head-attention.png\" width=\"180\" height=\"300\" alt=\"Multi Head Attention\">\n",
        "    <figcaption>Multi Head Attention</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "ZE4m9LylFjIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RL7T6pwCfnb"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        # Ensure the model dimension is divisible by the number of heads.\n",
        "        assert d_model % h == 0\n",
        "        # Dimension of each attention head.\n",
        "        self.d_k = d_model // h\n",
        "        # Number of attention heads.\n",
        "        self.h = h\n",
        "        # Linear layers for query, key, value, and output transformation.\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        # Scaled dot-product attention mechanism.\n",
        "        self.attn = ScaledDotProductAttention(dropout)\n",
        "        # Dropout layer.\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Optionally expand mask for all heads if provided.\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        # Batch size.\n",
        "        nbatches = query.size(0)\n",
        "        # Apply linear projections in batch from d_model => h x d_k,\n",
        "        # then transpose for attention computation.\n",
        "        query, key, value = [\n",
        "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "            for l, x in zip(self.linears, (query, key, value))\n",
        "        ]\n",
        "        # Apply attention on all the projected vectors in batch.\n",
        "        x, attn_weights = self.attn(query, key, value, mask=mask)\n",
        "        # \"Concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `MultiHeadedAttention` module plays a crucial role in the Transformer architecture by enabling the model to focus on different parts of the input sequence simultaneously. Here's how it fits into the overall structure and its impact:\n",
        "\n",
        "- **Parallel Attention Heads**: This module divides the model's attention mechanism into multiple \"heads,\" allowing the model to jointly attend to information from different representation subspaces at different positions. This parallelization helps capture more complex dependencies in the data.\n",
        "\n",
        "- **Query, Key, Value Transformations**: Before computing attention, the module transforms the input queries, keys, and values through separate linear layers. This transformation is critical for allowing each attention head to learn different aspects of the data.\n",
        "\n",
        "- **Scaled Dot-Product Attention**: Each head uses scaled dot-product attention to compute how much focus to put on other parts of the input sequence. The scaling factor (the square root of the dimension of the key vectors) helps stabilize the gradients during training.\n",
        "\n",
        "- **Masking Support**: The module supports masking out certain positions within the input sequence (e.g., for ignoring padding tokens or for enforcing causality in autoregressive models), which is essential for flexible sequence processing.\n",
        "\n",
        "- **Output Transformation**: After computing the attention in all heads, the module combines the outputs and passes them through another linear transformation to produce the final result. This step is crucial for reintegrating the information learned by each head."
      ],
      "metadata": {
        "id": "CoOLEfrbiR_X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-vmKmdLCkkd"
      },
      "source": [
        "### Position-wise Feed-Forward Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3pxg70tCncl"
      },
      "source": [
        "Each layer in the encoder and decoder contains a fully connected feed-forward network."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `PositionwiseFeedForward` class is a component of the Transformer architecture, designed to process the output of the attention mechanism.\n",
        "\n",
        "The PositionwiseFeedForward network plays a crucial role in adding depth and complexity to the Transformer model. While the attention mechanism allows the model to focus on different parts of the input sequence, the feedforward network enables complex transformations of the attention-modulated representations. This combination of attention and position-wise feedforward processing allows the Transformer to effectively capture the intricacies of sequential data.\n",
        "\n",
        "\n",
        "Here's a breakdown of its structure and function:\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "    - **d_model**: This parameter represents the dimensionality of the input and output of this feedforward network. It's consistent with the model's overall dimensionality, ensuring that the input and output of the feedforward network can seamlessly integrate with other parts of the Transformer.\n",
        "\n",
        "    - **d_ff**: This is the dimensionality of the hidden layer within the feedforward network. Typically, `d_ff` is significantly larger than `d_model`, allowing the network to capture more complex interactions in the data. This expansion and subsequent projection back to `d_model` is a common technique for increasing the expressiveness of neural networks.\n",
        "\n",
        "    - **dropout**: Specifies the dropout rate for regularization. Dropout is applied to the output of the first activation function.\n",
        "\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "    - The input `x` is first passed through a linear transformation (`self.w_1`), increasing its dimensionality from `d_model` to `d_ff`. This is followed by a ReLU activation function to introduce non-linearity, enabling the network to model complex relationships.\n",
        "\n",
        "    - A dropout is applied after the ReLU function, randomly setting a fraction of its inputs to zero, which helps in regularization.\n",
        "\n",
        "    - Finally, the output from the dropout layer is passed through another linear transformation (`self.w_2`) to project it back to the original dimensionality (`d_model`). This ensures that the output of the feedforward network can be fed directly into subsequent layers or components of the Transformer."
      ],
      "metadata": {
        "id": "rVDme49TuK1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RowBB9YlCqiY"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d32j_xZuCt95"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfvJ6aQHCxl8"
      },
      "source": [
        "\n",
        "Convert input tokens and output tokens to vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Embeddings` are the entry point of textual (or any token-based) data into the Transformer model, transforming discrete token IDs into dense vectors that can carry semantic information. These vectors are then processed through the model's subsequent layers. The scaling factor $(\\sqrt{d_{model}})$ is applied to give the embeddings a similar scale to the model's internal activations, aiming to facilitate optimization.\n",
        "\n",
        "\n",
        "Here's a breakdown of its components and functionality:\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "    - **d_model**: This parameter represents the size of the embedding vectors. In the context of Transformers, `d_model` is a fixed-size dimension that all embeddings and subsequent layers adhere to, ensuring uniformity across the model's architecture. It's a critical hyperparameter that influences the model's capacity to encode information.\n",
        "\n",
        "    - **vocab**: This is the size of the vocabulary, representing the total number of unique tokens that the model can recognize. The `vocab` size determines the number of rows in the embedding matrix, where each row corresponds to a specific token in the vocabulary.\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "    - The `forward` method takes an input tensor `x` containing token IDs and returns their corresponding embeddings. The embeddings are scaled by the square root of `d_model`, a normalization step recommended in the original Transformer paper to reduce the variance of the embeddings, helping stabilize the learning process.\n"
      ],
      "metadata": {
        "id": "79ldd0-KvgC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TcjR--JC0Ix"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding"
      ],
      "metadata": {
        "id": "7a28krc2kCeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence."
      ],
      "metadata": {
        "id": "tuVkgZT2kJPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `PositionalEncoding` class is a component designed to add information about the position of tokens within a sequence to the embeddings in a Transformer model. Since the Transformer architecture relies on attention mechanisms that do not inherently understand sequence order (unlike RNNs or LSTMs), positional encodings are crucial for enabling the model to consider the order of tokens when processing text or other sequential data. Without them, the model would treat sequences as unordered sets of tokens, significantly reducing its ability to perform tasks that rely on the sequential nature of the data, such as language understanding, translation, and generation.\n",
        "\n",
        "\n",
        "Here's a detailed explanation:\n",
        "\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "    - **d_model**: The dimensionality of the model's embeddings, which is the size of the vectors representing each token. Positional encodings must match this dimensionality to be added to the token embeddings.\n",
        "\n",
        "    - **dropout**: The dropout rate for regularization. After adding positional encodings to the token embeddings, dropout is applied to the combined embeddings to prevent overfitting.\n",
        "\n",
        "    - **max_len**: The maximum length of the input sequences that the model can handle. This parameter defines the size of the positional encoding matrix, ensuring that every position in a sequence, up to `max_len`, has a unique positional encoding vector.\n",
        "\n",
        "    - **pe (Positional Encoding Matrix)**: A precomputed matrix where each row represents the positional encoding for a specific position in a sequence. The encodings alternate between sine and cosine functions across different dimensions, allowing the model to distinguish positions based on these patterns. The use of sine and cosine functions helps the model to learn to attend to relative positions since the encoding for a position is a function of its distance from other positions.\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "    - The method receives the token embeddings `x` and adds the positional encodings to them based on the length of the input sequence. The positional encoding vectors are not learned but are a fixed function of the position. Adding these encodings to the embeddings allows the model to consider the position of tokens when processing sequences.\n",
        "    \n",
        "    - The result is then passed through a dropout layer for regularization before being returned."
      ],
      "metadata": {
        "id": "NsAZK_KFxlI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Precompute the positional encodings in log space.\n",
        "        # pe is the positional encoding matrix where each row corresponds to a position\n",
        "        # and each column corresponds to a dimension in the d_model space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)  # Generate a column vector of positions.\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )  # Generate the divisors for the sine and cosine functions.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices in the dimensions.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices.\n",
        "        pe = pe.unsqueeze(0)  # Add a batch dimension.\n",
        "        self.register_buffer(\"pe\", pe)  # Register pe as a buffer that is not a model parameter.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to each embedding vector in the batch.\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)  # Ensures gradients are not computed for pe.\n",
        "        return self.dropout(x)  # Apply dropout and return the result.\n"
      ],
      "metadata": {
        "id": "AGf73BmYkNl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalisation"
      ],
      "metadata": {
        "id": "hOZYfN8XiVv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We employ a residual connection around each of the two sub-layers, followed by layer normalization"
      ],
      "metadata": {
        "id": "W59ygv8aiaEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Transformer model, layer normalization is applied within both the encoder and decoder components.\n",
        "\n",
        "Layer Normalization works by normalizing the inputs across the features for each data sample in a batch. It's particularly effective in recurrent neural networks and has been beneficial in Transformer models. By normalizing the inputs, it helps control the scale of gradients, making the optimization landscape smoother, which often leads to faster training and better generalization.\n",
        "\n",
        "Here's a detailed explanation:\n",
        "\n",
        "- **Learnable Parameters (`self.gamma` and `self.beta`)**: Unlike batch normalization, which normalizes across the batch dimension, layer normalization performs normalization for each sample independently and uses learnable parameters to scale (`gamma`) and shift (`beta`) the normalized values.\n",
        "\n",
        "- **Epsilon (`self.eps`)**: A small constant added to the standard deviation to avoid division by zero.\n",
        "\n",
        "- **Normalization Process**: The forward method computes the mean and standard deviation for each input sample across its features. It then normalizes the inputs by subtracting the mean and dividing by the standard deviation, adjusted by `eps`. Finally, it applies the learned scaling and shifting parameters to each feature."
      ],
      "metadata": {
        "id": "K4zhkCyr05Eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        # Learnable Parameters to scale and shift the normalized values.\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps  # Small epsilon value to prevent division by zero in normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate the mean of the features.\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        # Calculate the standard deviation of the features.\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        # Normalize the features. The normalization formula is:\n",
        "        # normalized_x = gamma * (x - mean) / (std + epsilon) + beta\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "WO24_zkvieI3"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sublayer Connection"
      ],
      "metadata": {
        "id": "2fzoW6xaiq0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `SublayerConnection` module is designed to wrap around any sub-layer of the Transformer model, such as a self-attention layer or a position-wise feedforward network, to incorporate a residual connection and layer normalization around that sub-layer. This is critical for the model's ability to train deeply.\n",
        "\n",
        "- **Layer Normalization**: The input to the sub-layer is first normalized. Normalization stabilizes the learning process by ensuring that the input distributions to layers do not shift dramatically.\n",
        "\n",
        "- **Sub-layer Functionality**: The `sublayer` argument represents any function or module that takes the normalized input and produces an output of the same dimension (`d_model`). This design allows the `SublayerConnection` module to be agnostic to the specific type of sub-layer it is augmenting.\n",
        "\n",
        "- **Dropout and Residual Connection**: Dropout is applied to the output of the sub-layer for regularization. The dropout-enhanced output is then added to the original input `x` to form a residual connection. This connection allows gradients to flow directly through the network, mitigating the vanishing gradient problem and enabling the effective training of deep networks.\n",
        "\n",
        "- **Output Dimension Consistency**: The requirement that all sub-layers and embedding layers produce outputs of dimension `d_model` ensures consistency and simplicity in the model architecture. This uniformity is crucial for stacking multiple layers and for the effectiveness of residual connections."
      ],
      "metadata": {
        "id": "eD8yehcD3pkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is, the output of each sub-layer is\n",
        "\n",
        "$$\n",
        "LayerNorm(x+Sublayer(x))\n",
        "$$\n",
        "\n",
        "where Sublayer(x) is the function implemented by the sub-layer itself.\n",
        "\n",
        "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model} = 512$\n"
      ],
      "metadata": {
        "id": "4LYpg6GUivNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        # Normalizes the input features across the features dimension.\n",
        "        self.norm = LayerNorm(size)\n",
        "        # Dropout module\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # Normalize the input, pass through the sublayer, apply dropout,\n",
        "        # then add the input x for the residual connection.\n",
        "        # This ensures that the input can flow directly (or with minimal\n",
        "        # modification) through the layers of the network,\n",
        "        # preventing the vanishing gradients problem in deep networks.\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "8vrQ0kQNjQqc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator"
      ],
      "metadata": {
        "id": "BiVPrXrFkd4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Generator` class in a Transformer model architecture is the final step in the model's decoder that maps the decoder output to a probability distribution over the vocabulary.\n",
        "\n",
        "In a typical sequence generation task, this `Generator` module's output can be used during training to calculate the loss by comparing the predicted log probabilities against the ground truth. During inference, the logits (before the softmax) are often used with decoding strategies like greedy decoding, beam search, or sampling to select the next token in the sequence.\n",
        "\n",
        "Here's detailed explanations:\n",
        "\n",
        "- **Projection**: The linear layer (`nn.Linear`) projects the high-dimensional decoder outputs (which encode the semantic information of the input sequence and the so-far generated output sequence) down to the vocabulary space. The output of this layer is a set of logits, with one logit per vocabulary token, for each position in the input sequence.\n",
        "\n",
        "- **Log Softmax**: The softmax function is commonly used to convert logits to probabilities, ensuring that the output values are non-negative and sum to one, thus forming a valid probability distribution. The use of `log_softmax` instead of plain `softmax` is for numerical stability, especially useful in the context of calculating the loss during training (e.g., when using the Negative Log Likelihood loss)."
      ],
      "metadata": {
        "id": "JBhhYn9-5-GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        # Linear transformation that projects from model dimension (d_model) to vocabulary size.\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the linear transformation followed by log softmax.\n",
        "\n",
        "        Parameters:\n",
        "        - x: The input tensor with shape [batch size, sequence length, d_model],\n",
        "             representing the output from the Transformer's final decoder layer.\n",
        "\n",
        "        Returns:\n",
        "        - Log probabilities of each token in the vocabulary for each position\n",
        "          in the sequence and for each sequence in the batch.\n",
        "        \"\"\"\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "1ggjoNxokfyH"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "B-Yu4_AjhtXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder is composed of a stack of $N = 6$ identical layers."
      ],
      "metadata": {
        "id": "mP-z_BEeiAct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/encoder.png\" width=\"200\" height=\"400\" alt=\"Encoder\">\n",
        "    <figcaption>Encoder</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "fhAHHKEgnkOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Encoder` and `EncoderLayer` classes are fundamental components of the Transformer model's encoder architecture. The encoder's role is to process the input sequence and generate a set of representations that capture the semantics of the input sequence, considering both the content and the context around each token. These representations are then used by the decoder to generate the output sequence.\n",
        "\n",
        "Here's a breakdown of these components with detailed comments:\n",
        "\n",
        "\n",
        "- **Encoder**: The encoder takes the entire input sequence and transforms it into a continuous representation that holds the contextual information of every token. This is achieved through a series of encoder layers that each perform self-attention and position-wise feed-forward computations.\n",
        "\n",
        "- **EncoderLayer**: Each encoder layer first processes its input with a self-attention mechanism, allowing each position in the input to attend over all positions in the input sequence to capture contextual information. It then applies a position-wise feed-forward network to each position independently. Both the self-attention and feed-forward networks are wrapped with residual connections and layer normalization (encapsulated by the `SublayerConnection` module), which helps stabilize training and allows for deeper models."
      ],
      "metadata": {
        "id": "E6CENIpj8VRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Core encoder is a stack of N layers.\n",
        "    The encoder processes the input sequence in a series of steps,\n",
        "    each performed by an EncoderLayer.\n",
        "    All layers in the encoder are identical but do not share weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Clones the given layer N times to create a stack of N identical layers.\n",
        "        self.layers = clones(layer, N)\n",
        "        # Applies layer normalization to the output of the last encoder layer.\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Pass the input (and mask) through each layer in turn.\n",
        "\n",
        "        Parameters:\n",
        "        - x: The input sequence to the encoder.\n",
        "        - mask: A mask tensor indicating which positions are valid\n",
        "        (e.g., to exclude padding positions from attention computations).\n",
        "\n",
        "        Returns:\n",
        "        - The output of the last layer normalization, representing\n",
        "        the encoder's final representations of the input sequence.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "hdxtR0eziN_V"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder is made up of self-attention and feed-forward networks.\n",
        "    This design allows each token in the input sequence to attend\n",
        "    to all tokens in the same sequence (self-attention),\n",
        "    followed by position-wise feed-forward processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # The self-attention mechanism.\n",
        "        self.self_attn = self_attn\n",
        "        # The position-wise feed-forward network.\n",
        "        self.feed_forward = feed_forward\n",
        "        # Two instances of SublayerConnection, which apply residual connections\n",
        "        # and layer normalization around the sub-layers.\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        # The size of the input and output dimension of the layer.\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Process the input x through the self-attention and feed-forward networks sequentially,\n",
        "        with each being wrapped in a SublayerConnection to include residual connections and normalization.\n",
        "\n",
        "        Parameters:\n",
        "        - x: The input to the encoder layer.\n",
        "        - mask: A mask tensor for the self-attention mechanism to ignore certain positions.\n",
        "\n",
        "        Returns:\n",
        "        - The output of the encoder layer, after processing by self-attention and feed-forward networks.\n",
        "        \"\"\"\n",
        "        # Apply self-attention sub-layer, with residual connection and normalization.\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        # Apply feed-forward sub-layer, with residual connection and normalization.\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "WxIStTI0hwJl"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Layer"
      ],
      "metadata": {
        "id": "Bw5D_aSLjca1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is also composed of a stack of $N=6$ identical layers."
      ],
      "metadata": {
        "id": "uYFfrA5mjgnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/decoder.png\" width=\"200\" height=\"480\" alt=\"Decoder\">\n",
        "    <figcaption>Decoder</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "7y1R8Sx6nlc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Decoder` and `DecoderLayer` classes are central components of the Transformer model's decoder side, designed for sequence generation tasks. The decoder processes the encoder's output (referred to as \"memory\" here) along with its own input to generate the final output sequence.\n",
        "\n",
        "Here's a detailed breakdown with comments for clarity:\n",
        "\n",
        "- **Decoder**: Similar to the encoder, the decoder is composed of a stack of layers. However, the decoder layers also include cross-attention mechanisms that allow the decoder to focus on relevant parts of the input sequence as it generates each token of the output sequence. The decoder uses masking to ensure that predictions for a given position can only depend on known outputs at positions before it, not future positions.\n",
        "\n",
        "- **DecoderLayer**: Each decoder layer has three main components:\n",
        "    - **Self-Attention Layer**: Allows each position in the decoder to attend to all positions up to and including that position in the decoder sequence.\n",
        "    - **Cross-Attention Layer**: Allows each position in the decoder to attend to all positions in the encoder sequence, integrating information from the input sequence.\n",
        "    - **Feed-Forward Network**: Processes each position of the decoder output in isolation, further transforming the data after integrating information from the encoder and previous decoder outputs."
      ],
      "metadata": {
        "id": "hA8Gmz_g-SPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic N layer decoder with masking.\n",
        "\n",
        "    The decoder is responsible for generating the output sequence by processing\n",
        "    the encoder output and the previous decoder outputs. It includes mechanisms\n",
        "    for self-attention within the decoder and cross-attention to the encoder outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Stack of N identical layers.\n",
        "        self.layers = clones(layer, N)\n",
        "        # Layer normalization applied to the output of the final layer.\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Pass the input through each layer in turn.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input to the decoder, typically the previous output.\n",
        "        - memory: Output from the encoder to be used in cross-attention.\n",
        "        - src_mask: Mask for the encoder output.\n",
        "        - tgt_mask: Mask for the decoder input to prevent attending to future tokens.\n",
        "\n",
        "        Returns:\n",
        "        - The output of the decoder after processing through N layers and normalization.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "qb1N73B4jme_"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
      ],
      "metadata": {
        "id": "q2RiKR4Ajp6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder is made of self-attention, source-attention, and feed forward networks.\n",
        "\n",
        "    Each layer in the decoder includes two attention mechanisms: one for attending to\n",
        "    the previous decoder outputs (self-attention) and one for attending to the encoder\n",
        "    outputs (source-attention). This is followed by a position-wise feed-forward network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size  # Size of the input dimension.\n",
        "        # Self-attention mechanism for attending to previous decoder outputs.\n",
        "        self.self_attn = self_attn\n",
        "        # Source-attention mechanism for attending to encoder outputs.\n",
        "        self.src_attn = src_attn\n",
        "        # Position-wise feed-forward network.\n",
        "        self.feed_forward = feed_forward\n",
        "        # Residual connections around each of the sub-layers.\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Process input through self-attention, source-attention, and feed-forward layers.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input to the decoder layer.\n",
        "        - memory: Encoder outputs.\n",
        "        - src_mask: Mask for the encoder outputs.\n",
        "        - tgt_mask: Mask for the decoder inputs to ensure causality.\n",
        "\n",
        "        Returns:\n",
        "        - The output of the decoder layer after processing.\n",
        "        \"\"\"\n",
        "        m = memory\n",
        "        # Apply self-attention with causality mask to prevent forward-looking.\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        # Apply source-attention, attending to encoder outputs.\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        # Apply the feed-forward network.\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "Yu03L_ARjwfU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subsequent Mask"
      ],
      "metadata": {
        "id": "9J6m98n_noYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
      ],
      "metadata": {
        "id": "bJuTCAocnsvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
        "        torch.uint8\n",
        "    )\n",
        "    return subsequent_mask == 0"
      ],
      "metadata": {
        "id": "Q3n9O22Jn7RU"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v85Es50DHEL"
      },
      "source": [
        "## Full Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGDK_at2HtYa"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/overall-architecture-of-transformers.png\" width=\"400\" height=\"400\" alt=\"Transformers Architecture\">\n",
        "    <figcaption>Transformers Architecture</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSmgcW5mDKOz"
      },
      "source": [
        "Now, let's put together the main Transformer architecture combining the encoder, decoder, and other components previously described."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "R0ijbTIEDOAl"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        c = copy.deepcopy\n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
        "        self.decoder = Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N)\n",
        "        self.src_embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))\n",
        "        self.tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))\n",
        "        self.generator = Generator(d_model, tgt_vocab)\n",
        "\n",
        "        # Parameter Initialization\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference Test"
      ],
      "metadata": {
        "id": "Adac3Ouqk20_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_test():\n",
        "    # Adjust parameters as needed\n",
        "    test_model = Transformer(src_vocab=11, tgt_vocab=11, N=2, d_model=512, d_ff=2048, h=8, dropout=0.1)\n",
        "    test_model.eval()\n",
        "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
        "    src_mask = torch.ones(1, 1, 10)\n",
        "    memory = test_model.encode(src, src_mask)\n",
        "    ys = torch.zeros(1, 1).type_as(src)\n",
        "\n",
        "    for i in range(9):\n",
        "        out = test_model.decode(\n",
        "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
        "        )\n",
        "        prob = test_model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "        ys = torch.cat(\n",
        "            [ys, torch.tensor([[next_word]]).type_as(src.data)], dim=1\n",
        "        )\n",
        "\n",
        "    print(\"Example Untrained Model Prediction:\", ys)\n",
        "\n",
        "def run_tests():\n",
        "    for _ in range(10):\n",
        "        inference_test()\n",
        "\n",
        "# Uncomment to run the test\n",
        "run_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6702Kmgk-au",
        "outputId": "a97bd502-b58c-4eac-9bfb-773c9c56a132"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Untrained Model Prediction: tensor([[0, 2, 2, 2, 2, 2, 2, 2, 2, 3]])\n",
            "Example Untrained Model Prediction: tensor([[0, 1, 1, 1, 0, 6, 7, 5, 6, 1]])\n",
            "Example Untrained Model Prediction: tensor([[0, 5, 3, 5, 0, 5, 3, 5, 0, 5]])\n",
            "Example Untrained Model Prediction: tensor([[0, 8, 5, 6, 5, 6, 2, 9, 2, 9]])\n",
            "Example Untrained Model Prediction: tensor([[ 0,  7,  3, 10,  3, 10,  7,  8,  3, 10]])\n",
            "Example Untrained Model Prediction: tensor([[0, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n",
            "Example Untrained Model Prediction: tensor([[ 0,  7,  1,  5,  8, 10,  1,  5,  7,  1]])\n",
            "Example Untrained Model Prediction: tensor([[ 0,  4,  2,  8,  4,  2, 10, 10, 10, 10]])\n",
            "Example Untrained Model Prediction: tensor([[0, 7, 9, 7, 7, 9, 7, 9, 7, 2]])\n",
            "Example Untrained Model Prediction: tensor([[ 0,  9,  5,  2, 10,  2, 10,  9,  5,  9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "cBt2r9g3eDbg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEgn4rM5DRxB"
      },
      "source": [
        "## Training Infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcGRhZH_DVA2"
      },
      "source": [
        "Below is a simple training loop and data generation for a copy task, which will serve as our \"simple task\". For real data, you would replace the data generation and possibly the training loop to suit your dataset and task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz76xVVpDYo1"
      },
      "outputs": [],
      "source": [
        "def train(model, data_generator, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(data_generator):\n",
        "            src = batch[0]\n",
        "            tgt = batch[1][:, :-1]\n",
        "            tgt_y = batch[1][:, 1:]\n",
        "            src_mask = (src != 0).unsqueeze(-2)\n",
        "            tgt_mask = make_std_mask(tgt, 0)\n",
        "            out = model(src, tgt, src_mask, tgt_mask)\n",
        "            loss = (out - tgt_y).mean()  # Simplified loss calculation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch} Loss {total_loss / len(data_generator)}\")\n",
        "\n",
        "def data_gen(V, batch, nbatches):\n",
        "    \"Generate random data for a src-tgt copy task.\"\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        data[:, 0] = 1  # Start token\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        yield Batch(src, tgt, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrOQYwLKDcJB"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9P96Sv-DgMo"
      },
      "source": [
        "Let's put it all together and run the training for our Transformer model on the simple copy task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGdyMGH4DjKM"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "V = 11  # Vocabulary size\n",
        "N = 6  # Number of layers\n",
        "d_model = 512  # Embedding dimension\n",
        "d_ff = 2048  # Feed-forward dimension\n",
        "h = 8  # Number of heads\n",
        "\n",
        "# Instantiate model\n",
        "model = Transformer(V, V, N=N, d_model=d_model, d_ff=d_ff, h=h)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Data generation\n",
        "gen = data_gen(V, 30, 20)\n",
        "\n",
        "# Training\n",
        "train(model, gen, optimizer, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P466rmc9DrTE"
      },
      "source": [
        "# Applying the Model to Real Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nzCLBiyDAi5"
      },
      "source": [
        "\n",
        "This implementation is quite high-level and abstracts away many details for brevity. When working with real-world datasets, consider using existing frameworks like Hugging Face's Transformers, which provide pre-implemented versions of Transformer models and utilities for processing data, or adjust the data loading and processing parts of this guide to fit your specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKW0UC-pBdlf"
      },
      "source": [
        "# Reference\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbIDMOtCBf3n"
      },
      "source": [
        "- Huang, A., Subramanian, S., Sum, J., Almubarak, K., & Biderman, S. (2022). *The Annotated Transformer*. Original by Sasha Rush. Retrieved from [https://nlp.seas.harvard.edu/annotated-transformer/](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "\n",
        "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). *Attention is All You Need*. In Advances in Neural Information Processing Systems (NIPS). Retrieved from [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "- The Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}