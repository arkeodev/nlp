{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Simple_Transformers/simple_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuU9A_QV-7PE"
      },
      "source": [
        "# Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x26jAFlT_tuR"
      },
      "source": [
        "To implement the transformer architecture based on the description you've provided, let's break it down into its components and outline how each can be coded using PyTorch. The transformer model, as described in the paper \"Attention is All You Need\", consists of several key components: an Encoder and Decoder architecture, with each having multiple layers of self-attention and feed-forward neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEx3cKI_BOfd"
      },
      "source": [
        "This guide and explanation of the Transformer architecture draws extensively from \"The Annotated Transformer\" by Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman, building upon the original work by Sasha Rush. This comprehensive article offers a detailed line-by-line implementation of the Transformer model, providing insights into its inner workings and illustrating the theoretical concepts with practical code examples. It serves as an invaluable resource for understanding and implementing the Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Complexity Comparison"
      ],
      "metadata": {
        "id": "sKdKEQX3E6EY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/time-complexity-of-transformers.png\" width=\"1000\" height=\"300\" alt=\"Time complexity\">\n",
        "    <figcaption>Time complexity</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "3zLRb49eETsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table from Vaswani et al. (2017), which is from the seminal paper \"Attention is All You Need,\" compares different neural network layer types across three dimensions: complexity per layer, number of sequential operations, and maximum path length:\n",
        "\n",
        "- **Self-Attention Layer**:\n",
        "  - Computational complexity per layer is \\( O(n^2 \\cdot d) \\), meaning it scales quadratically with the sequence length and linearly with the dimensionality of the representation.\n",
        "  - Sequential operations are constant (\\( O(1) \\)), allowing for high parallelization during training and inference.\n",
        "  - Maximum path length is also constant (\\( O(1) \\)), ensuring that long-range dependencies can be learned effectively as any position in the sequence can directly attend to any other position.\n",
        "\n",
        "- **Recurrent Layer**:\n",
        "  - Complexity is \\( O(n \\cdot d^2) \\), which scales linearly with the sequence length, making it less efficient for longer sequences compared to self-attention.\n",
        "  - Requires \\( O(n) \\) sequential operations, limiting parallelization and potentially increasing training time.\n",
        "  - Has a linear maximum path length (\\( O(n) \\)), which can make learning long-range dependencies more challenging.\n",
        "\n",
        "- **Convolutional Layer**:\n",
        "  - Shows a complexity of \\( O(k \\cdot n \\cdot d^2) \\), where \\( k \\) is the kernel size, suggesting an increase in computation if larger receptive fields are needed.\n",
        "  - Has constant sequential operations (\\( O(1) \\)), similar to self-attention, facilitating parallelization.\n",
        "  - Exhibits a logarithmic maximum path length (\\( O(\\log_k(n)) \\)), improving the learning of dependencies over recurrent layers but potentially less effective than self-attention for very long sequences.\n",
        "\n",
        "- **Restricted Self-Attention Layer (or Local Attention)**:\n",
        "  - Complexity per layer is reduced to \\( O(r \\cdot n \\cdot d) \\) by restricting attention to a window of \\( r \\) surrounding tokens, balancing computational efficiency and the ability to capture dependencies.\n",
        "  - Maintains constant sequential operations (\\( O(1) \\)), benefiting parallelization.\n",
        "  - Features a maximum path length of \\( O(n/r) \\), indicating a compromise in the ability to learn dependencies as the sequence length increases compared to full self-attention."
      ],
      "metadata": {
        "id": "bTfRpR-CEUWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dive into the self-attention mechanism, which forms the core component of transformers."
      ],
      "metadata": {
        "id": "s8xn6oKFFVj5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOS3epRYH6BU"
      },
      "source": [
        "## Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism is what allows Transformers to process data in parallel. It assigns a weight to each element in the input sequence, based on how relevant each element is to every other element."
      ],
      "metadata": {
        "id": "fUuZ62MidIOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of Self Attention"
      ],
      "metadata": {
        "id": "Heh5NsV5dKoG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF36kRWPH-y2"
      },
      "source": [
        "Self-attention can be described with three main components: Queries, Keys, and Values.\n",
        "\n",
        "- **Queries**: A set of vectors that is matched against the keys to decide the most important elements in the sequence.\n",
        "- **Keys**: Vectors that are paired with values; they are used to extract the information that queries look for.\n",
        "- **Values**: Vectors that contain the actual information of each element in the sequence that is extracted based on the weightage from the keys.\n",
        "\n",
        "Imagine you are in a library with a huge collection of books (the sequence), and you are looking for information on a specific topic.\n",
        "\n",
        "- The **query** is like your question about the topic youâ€™re interested in.\n",
        "- The **keys** represent the index or summary of each book.\n",
        "- The **values** are the actual contents of the books.\n",
        "\n",
        "The librarian (the self-attention mechanism) checks your question against all summaries (keys) to determine which books (values) have the information you need. This process is done simultaneously for all the questions in parallel, which is what makes the transformer model so powerful and efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative Attention Approaches"
      ],
      "metadata": {
        "id": "9D1aD6_XVi2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Additive Attention**: While the scaled dot-product attention is widely used, alternative attention mechanisms exist, such as additive (or Bahdanau) attention, which uses a feed-forward network to compute attention scores instead of dot products. Each variant has its own advantages and use cases.\n",
        "\n",
        "- **Multi-Head Attention**: The Transformer model extends the basic scaled dot-product attention mechanism through the use of multi-head attention, where the model runs several attention processes in parallel. This approach allows the model to simultaneously attend to information from different representation subspaces at different positions, improving the model's ability to capture various types of relationships in the data.\n",
        "\n",
        "- **Self-Attention vs. Cross-Attention**: The module as described implements self-attention, where queries, keys, and values all come from the same source. In cross-attention settings, keys and values can come from a different source than queries, enabling interactions between different sequences (e.g., between an encoder and a decoder in a sequence-to-sequence model)."
      ],
      "metadata": {
        "id": "uOdKUvIgVorf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mathemetical Foundation of Attention"
      ],
      "metadata": {
        "id": "xJXcjF_aOU8s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u283a318IBSi"
      },
      "source": [
        "The following formula represents the scaled dot-product attention, which is the foundation of self-attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Components and Dimensions:**\n",
        "\n",
        "- $( Q )$: Queries matrix of dimension $( m \\times d_k )$, where $( m )$ is the number of queries and $( d_k )$ is the dimension of each query vector.\n",
        "- $( K )$: Keys matrix of dimension $( n \\times d_k )$, where $( n )$ is the number of keys (and values) and $( d_k )$ is the dimension of each key vector.\n",
        "- $( V )$: Values matrix of dimension $( n \\times d_v )$, where $( n )$ is the number of values and $( d_v )$ is the dimension of each value vector.\n",
        "- $( d_k )$: Dimension of the keys (and queries), used for scaling the dot product scores."
      ],
      "metadata": {
        "id": "5sjro7b7ZPaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Dot Product of $( Q )$ and $( K^T )$**\n",
        "\n",
        "- $( QK^T )$: Since $( Q )$ is $( m \\times d_k )$ and $( K )$ is $( n \\times d_k )$, when we take the transpose of $( K )$ to get $( K^T )$ which is $( d_k \\times n )$, the resulting dot product $( QK^T )$ will be a matrix of dimension $( m \\times n )$. Each element of this matrix represents a score reflecting the relevance of a query to a key.\n",
        "\n",
        "- This step calculates the similarity between all queries and all keys, serving as the basis for determining how the values should be weighted according to each query."
      ],
      "metadata": {
        "id": "UrLWhqd2ZZNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Scaling by $( \\sqrt{d_k} )$**\n",
        "\n",
        "- The scores in the $( m \\times n )$ matrix are scaled down by $( \\sqrt{d_k} )$, which doesn't change the dimensions of the matrix but normalizes the scores to prevent them from becoming too large.\n",
        "\n",
        "- Scaling helps maintain numerical stability, particularly in the softmax step, by ensuring the scores don't lead to extremely small gradients when the dimensionality $( d_k )$ is large."
      ],
      "metadata": {
        "id": "W45x5F9Rap3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Softmax Function**\n",
        "\n",
        "- The softmax function is applied across the rows of the scaled $( m \\times n )$ matrix. The dimensionality of the output remains $( m \\times n )$, but now each row sums to 1, representing probabilities.\n",
        "\n",
        "- Converting the scores to probabilities allows the model to probabilistically decide which keys (and thus, which values) are most relevant to each query, enabling a soft selection mechanism."
      ],
      "metadata": {
        "id": "yFLfIefQaz_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Multiplication by $( V )$**\n",
        "\n",
        "- The softmax output, still $( m \\times n )$, is then multiplied by $( V )$, which is $( n \\times d_v )$. The resulting matrix will have dimensions $( m \\times d_v )$, representing the final output of the attention mechanism.\n",
        "\n",
        "- This step effectively combines the values based on their relevance to each query, as determined by the weighted scores. The final dimension $( m \\times d_v )$ corresponds to the weighted sum of values for each query, now ready for further processing in the model.\n"
      ],
      "metadata": {
        "id": "J32Z64AEYzDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Implementation"
      ],
      "metadata": {
        "id": "jEdINcjFYuY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5Lo6pNiPIJ3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265ea9a5-2c90-4b0f-e7ec-c8e6b39ff10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights for the first sentence:\n",
            "[[0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.1320772  0.20914044 0.20045296 0.1320772  0.1320772  0.194175  ]\n",
            " [0.11958619 0.18149541 0.25215275 0.11958619 0.11958619 0.20759326]\n",
            " [0.15299844 0.15299844 0.15299844 0.21788799 0.15299844 0.17011824]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12397355 0.18226132 0.21520941 0.13784561 0.12397355 0.21673656]]\n",
            "Attention weights for the second sentence:\n",
            "[[0.20789086 0.14701445 0.1464956  0.14546337 0.20715715 0.14597857]\n",
            " [0.13342497 0.19895022 0.20393542 0.13201726 0.13201726 0.19965486]\n",
            " [0.12073931 0.18519945 0.23888728 0.12420309 0.11988857 0.2110823 ]\n",
            " [0.15216063 0.15216063 0.15763656 0.21669485 0.15216063 0.16918669]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12305363 0.18544201 0.21589025 0.13633987 0.12261934 0.21665489]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x over the specified axis.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
        "\n",
        "# Define the scaled dot product attention function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "    dk = K.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "    attention_weights = softmax(scaled_attention_logits)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Let's consider two sentences for our demonstration\n",
        "sentences = [\"The cat sits on the mat\", \"A dog lies on the rug\"]\n",
        "\n",
        "# Define embeddings for each unique word in the sentences\n",
        "# Embeddings are crafted so that similar words have closer embeddings\n",
        "word_embeddings = {\n",
        "    \"the\": np.array([1, 0, 0, 0, 0, 0, 0, 0]),\n",
        "    \"cat\": np.array([0, 1, 0, 0, 0.1, 0.2, 0.3, 0.4]),\n",
        "    \"sits\": np.array([0, 0.9, 1, 0, 0.2, 0.1, 0.4, 0.3]),\n",
        "    \"on\": np.array([0, 0, 0, 1, 0, 0, 0, 0]),\n",
        "    \"mat\": np.array([0, 0.8, 0.6, 0.3, 0, 0.6, 0.3, 0.2]),\n",
        "    \"a\": np.array([1, 0, 0, 0, 0, 0, 0, 0.1]),\n",
        "    \"dog\": np.array([0, 0.9, 0.1, 0, 0, 0.3, 0.4, 0.3]),\n",
        "    \"lies\": np.array([0, 1, 0.8, 0.1, 0.3, 0.1, 0.4, 0.2]),\n",
        "    \"rug\": np.array([0, 0.9, 0.6, 0.3, 0, 0.5, 0.3, 0.1])\n",
        "}\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
        "\n",
        "# Convert sentences to embeddings using the predefined embeddings\n",
        "sentence_embeddings = [[word_embeddings[word] for word in sentence] for sentence in tokenized_sentences]\n",
        "\n",
        "# Pad sentences to the same length\n",
        "max_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "padded_embeddings = [np.array(sentence + [np.zeros(3)] * (max_length - len(sentence))) for sentence in sentence_embeddings]\n",
        "\n",
        "# Stack the embeddings to create the Q, K, and V matrices\n",
        "Q = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "K = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "V = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "\n",
        "# Apply the scaled dot product attention function\n",
        "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Let's print the attention weights for the first sentence\n",
        "print(\"Attention weights for the first sentence:\")\n",
        "print(attention_weights[0])\n",
        "\n",
        "# Let's print the attention weights for the second sentence\n",
        "print(\"Attention weights for the second sentence:\")\n",
        "print(attention_weights[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RskdI0JuIPZ9"
      },
      "source": [
        "**For the first sentence \"The cat sits on the mat\":**\n",
        "\n",
        "1. The attention weights for the word \"the\" (first row) seem to be distributed across \"the\", \"sits\", and \"mat\" with higher weights (0.2079) compared to \"cat\" and \"on\" (0.1460). This might indicate that the model perceives a stronger association between \"the\" and the action (\"sits\") and the object (\"mat\") of the sentence.\n",
        "\n",
        "2. The word \"cat\" (second row) has the highest attention weight when paired with itself (0.2091) and with \"sits\" (0.2004), which could suggest that the model identifies \"cat\" and the action it's performing as key components of the sentence.\n",
        "\n",
        "3. The word, \"sits\" (third row) has the highest attention weight in combination with itself (0.2521) and substantial weight with \"cat\" (0.1814), potentially indicating the model's understanding of the importance of the verb in the context of the subject.\n",
        "\n",
        "4. Finally, \"mat\" (sixth row) pays more attention to \"sits\" (0.2152) as expected.\n",
        "\n",
        "**For the second sentence \"A dog lies on the rug\":**\n",
        "\n",
        "1. \"Dog\" (second row) shows higher attention weights for itself (0.1989) and \"lies\" (0.2039), which is consistent with recognizing the subject and verb relationship.\n",
        "\n",
        "3. \"Lies\" (third row) has the highest weight with itself (0.2388), and a notable weight with \"dog\" (0.1851) and \"rug\" (0.2110), reinforcing the connection between the subject, object and the action.\n",
        "\n",
        "4. \"Rug\" (sixth row), much like \"mat\" in the first sentence, shows a relatively high weight for \"dog\" (0.1854) and \"lies\" (0.2158), as expected.\n",
        "\n",
        "In both sentences, it is evident that the attention mechanism is picking up on the grammatical structure and the relationships between subjects, verbs, and objects. It emphasizes self-attention and relationships that make sense in the context of the sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation the Model"
      ],
      "metadata": {
        "id": "hXnStVipd1oK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-CjaKz4B8X_"
      },
      "source": [
        "## Preliminary Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkesXoMgB_Vc"
      },
      "source": [
        "First, ensure all required packages are installed and import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "49ToAoDBCFLR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "rd5J2JpDH5HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to produce N identical layers\n",
        "def clones(module, N):\n",
        "  \"Produce N identical layers.\"\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "YKKM9QA6H2ue"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUoEXHACCq3"
      },
      "source": [
        "## Core Components of the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnLqaozUCP54"
      },
      "source": [
        "### Scaled Dot-Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCDwbr31CTcO"
      },
      "source": [
        "The core of the transformer model is the attention mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/attention.png\" width=\"180\" height=\"300\" alt=\"Attention Mechanism\">\n",
        "    <figcaption>Attention Mechanism</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "tVJwk5k_FibE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "U38lQRe2CXRh"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout=None):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        # Initialize dropout for attention scores, if provided.\n",
        "        self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Dimension of the key vectors, used for scaling down the dot products.\n",
        "        d_k = query.size(-1)\n",
        "        # Compute the dot products between queries and keys for each batch and head,\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "        # Scale the results\n",
        "        scaled_score = scores / math.sqrt(d_k)\n",
        "        # Apply masking if provided (for attention to consider only certain positions).\n",
        "        if mask is not None:\n",
        "            scaled_score = scaled_score.masked_fill(mask == 0, -1e9)\n",
        "        # Apply softmax to obtain attention probabilities.\n",
        "        p_attn = F.softmax(scaled_score, dim=-1)\n",
        "        # Optionally apply dropout to attention scores.\n",
        "        if self.dropout is not None:\n",
        "            p_attn = self.dropout(p_attn)\n",
        "        # Weight the values by the computed attention probabilities.\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ScaledDotProductAttention` module implements the attention mechanism that is at the heart of the Transformer model's effectiveness. Its roles and effects on the structure are as follows:\n",
        "\n",
        "- **Attention Calculation**: This module computes the attention scores by taking the dot product of the query with the key. It scales the dot product by the square root of the dimensionality of the key to prevent extremely small gradients when the dimensionality is large. This scaling helps stabilize gradients during training.\n",
        "\n",
        "- **Masking**: The optional mask allows the model to selectively ignore certain positions within the input sequence. This is particularly useful for masking out padding tokens in the input sequence or for preventing the model from peeking at future tokens when processing sequences in an autoregressive manner (e.g., during training a language model).\n",
        "\n",
        "- **Dropout**: Applying dropout to the attention scores is a regularization technique that helps prevent overfitting by randomly zeroing out some of the scores before computing the final attention weights.\n",
        "\n",
        "- **Softmax**: The softmax function converts the attention scores into probabilities, ensuring that they are non-negative and sum up to 1. This step allows the model to essentially \"focus\" on the most relevant parts of the input.\n",
        "\n",
        "- **Output**: The module outputs the weighted sum of the value vectors, scaled by the attention probabilities, which is then used in subsequent layers of the model. It also returns the attention probabilities themselves, which can be useful for analysis or for visualizing the model's attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "FK4gLRowVdWH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDmzVYO7Caqs"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pG3peUkCdWE"
      },
      "source": [
        "Allows the model to jointly attend to information from different representation subspaces."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/mutl-head-attention.png\" width=\"180\" height=\"300\" alt=\"Multi Head Attention\">\n",
        "    <figcaption>Multi Head Attention</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "ZE4m9LylFjIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "4RL7T6pwCfnb"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = ScaledDotProductAttention(dropout)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        query, key, value = [\n",
        "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "            for l, x in zip(self.linears, (query, key, value))\n",
        "        ]\n",
        "        x, attn_weights = self.attn(query, key, value, mask=mask)\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-vmKmdLCkkd"
      },
      "source": [
        "### Position-wise Feed-Forward Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3pxg70tCncl"
      },
      "source": [
        "Each layer in the encoder and decoder contains a fully connected feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RowBB9YlCqiY"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d32j_xZuCt95"
      },
      "source": [
        "### Embeddings and Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfvJ6aQHCxl8"
      },
      "source": [
        "\n",
        "Convert input tokens and output tokens to vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_TcjR--JC0Ix"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding"
      ],
      "metadata": {
        "id": "7a28krc2kCeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence."
      ],
      "metadata": {
        "id": "tuVkgZT2kJPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "AGf73BmYkNl8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalisation"
      ],
      "metadata": {
        "id": "hOZYfN8XiVv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We employ a residual connection around each of the two sub-layers, followed by layer normalization"
      ],
      "metadata": {
        "id": "W59ygv8aiaEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "metadata": {
        "id": "WO24_zkvieI3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sublayer Connection"
      ],
      "metadata": {
        "id": "2fzoW6xaiq0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is, the output of each sub-layer is\n",
        "\n",
        "$$\n",
        "LayerNorm(x+Sublayer(x))\n",
        "$$\n",
        "\n",
        "where Sublayer(x) is the function implemented by the sub-layer itself. We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
        "\n",
        "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_model = 512$\n"
      ],
      "metadata": {
        "id": "4LYpg6GUivNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "8vrQ0kQNjQqc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator"
      ],
      "metadata": {
        "id": "BiVPrXrFkd4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "1ggjoNxokfyH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "B-Yu4_AjhtXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder is composed of a stack of $N = 6$ identical layers."
      ],
      "metadata": {
        "id": "mP-z_BEeiAct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "hdxtR0eziN_V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "WxIStTI0hwJl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Layer"
      ],
      "metadata": {
        "id": "Bw5D_aSLjca1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is also composed of a stack of $N=6$ identical layers."
      ],
      "metadata": {
        "id": "uYFfrA5mjgnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "qb1N73B4jme_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
      ],
      "metadata": {
        "id": "q2RiKR4Ajp6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "Yu03L_ARjwfU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subsequent Mask"
      ],
      "metadata": {
        "id": "9J6m98n_noYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
      ],
      "metadata": {
        "id": "bJuTCAocnsvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
        "        torch.uint8\n",
        "    )\n",
        "    return subsequent_mask == 0"
      ],
      "metadata": {
        "id": "Q3n9O22Jn7RU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v85Es50DHEL"
      },
      "source": [
        "## Full Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGDK_at2HtYa"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/overall-architecture-of-transformers.png\" width=\"400\" height=\"400\" alt=\"Transformers Architecture\">\n",
        "    <figcaption>Transformers Architecture</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSmgcW5mDKOz"
      },
      "source": [
        "Now, let's put together the main Transformer architecture combining the encoder, decoder, and other components previously described."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "R0ijbTIEDOAl"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        c = copy.deepcopy\n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
        "        self.decoder = Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N)\n",
        "        self.src_embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))\n",
        "        self.tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))\n",
        "        self.generator = Generator(d_model, tgt_vocab)\n",
        "\n",
        "        # Parameter Initialization\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference Test"
      ],
      "metadata": {
        "id": "Adac3Ouqk20_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_test():\n",
        "    # Adjust parameters as needed\n",
        "    test_model = Transformer(src_vocab=11, tgt_vocab=11, N=2, d_model=512, d_ff=2048, h=8, dropout=0.1)\n",
        "    test_model.eval()\n",
        "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
        "    src_mask = torch.ones(1, 1, 10)\n",
        "    memory = test_model.encode(src, src_mask)\n",
        "    ys = torch.zeros(1, 1).type_as(src)\n",
        "\n",
        "    for i in range(9):\n",
        "        out = test_model.decode(\n",
        "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
        "        )\n",
        "        prob = test_model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "        ys = torch.cat(\n",
        "            [ys, torch.tensor([[next_word]]).type_as(src.data)], dim=1\n",
        "        )\n",
        "\n",
        "    print(\"Example Untrained Model Prediction:\", ys)\n",
        "\n",
        "def run_tests():\n",
        "    for _ in range(10):\n",
        "        inference_test()\n",
        "\n",
        "# Uncomment to run the test\n",
        "run_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6702Kmgk-au",
        "outputId": "e13fdc7f-b82c-429f-ce95-a5c2e3af0f52"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Untrained Model Prediction: tensor([[ 0,  1,  3,  5,  0,  1,  3,  5,  0, 10]])\n",
            "Example Untrained Model Prediction: tensor([[0, 4, 2, 2, 4, 2, 2, 2, 2, 4]])\n",
            "Example Untrained Model Prediction: tensor([[0, 7, 2, 8, 8, 8, 8, 8, 8, 8]])\n",
            "Example Untrained Model Prediction: tensor([[0, 9, 2, 0, 7, 2, 0, 7, 2, 6]])\n",
            "Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "Example Untrained Model Prediction: tensor([[0, 7, 6, 6, 7, 6, 6, 6, 6, 6]])\n",
            "Example Untrained Model Prediction: tensor([[0, 8, 5, 5, 5, 5, 5, 5, 5, 5]])\n",
            "Example Untrained Model Prediction: tensor([[ 0, 10,  1,  1,  1,  9,  8,  5,  8,  5]])\n",
            "Example Untrained Model Prediction: tensor([[0, 8, 3, 9, 9, 9, 9, 9, 9, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "cBt2r9g3eDbg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEgn4rM5DRxB"
      },
      "source": [
        "## Training Infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcGRhZH_DVA2"
      },
      "source": [
        "Below is a simple training loop and data generation for a copy task, which will serve as our \"simple task\". For real data, you would replace the data generation and possibly the training loop to suit your dataset and task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz76xVVpDYo1"
      },
      "outputs": [],
      "source": [
        "def train(model, data_generator, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(data_generator):\n",
        "            src = batch[0]\n",
        "            tgt = batch[1][:, :-1]\n",
        "            tgt_y = batch[1][:, 1:]\n",
        "            src_mask = (src != 0).unsqueeze(-2)\n",
        "            tgt_mask = make_std_mask(tgt, 0)\n",
        "            out = model(src, tgt, src_mask, tgt_mask)\n",
        "            loss = (out - tgt_y).mean()  # Simplified loss calculation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch} Loss {total_loss / len(data_generator)}\")\n",
        "\n",
        "def data_gen(V, batch, nbatches):\n",
        "    \"Generate random data for a src-tgt copy task.\"\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        data[:, 0] = 1  # Start token\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        yield Batch(src, tgt, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrOQYwLKDcJB"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9P96Sv-DgMo"
      },
      "source": [
        "Let's put it all together and run the training for our Transformer model on the simple copy task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGdyMGH4DjKM"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "V = 11  # Vocabulary size\n",
        "N = 6  # Number of layers\n",
        "d_model = 512  # Embedding dimension\n",
        "d_ff = 2048  # Feed-forward dimension\n",
        "h = 8  # Number of heads\n",
        "\n",
        "# Instantiate model\n",
        "model = Transformer(V, V, N=N, d_model=d_model, d_ff=d_ff, h=h)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Data generation\n",
        "gen = data_gen(V, 30, 20)\n",
        "\n",
        "# Training\n",
        "train(model, gen, optimizer, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P466rmc9DrTE"
      },
      "source": [
        "# Applying the Model to Real Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nzCLBiyDAi5"
      },
      "source": [
        "\n",
        "This implementation is quite high-level and abstracts away many details for brevity. When working with real-world datasets, consider using existing frameworks like Hugging Face's Transformers, which provide pre-implemented versions of Transformer models and utilities for processing data, or adjust the data loading and processing parts of this guide to fit your specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKW0UC-pBdlf"
      },
      "source": [
        "# Reference\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbIDMOtCBf3n"
      },
      "source": [
        "- Huang, A., Subramanian, S., Sum, J., Almubarak, K., & Biderman, S. (2022). *The Annotated Transformer*. Original by Sasha Rush. Retrieved from [https://nlp.seas.harvard.edu/annotated-transformer/](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "\n",
        "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., & Polosukhin, I. (2017). *Attention is All You Need*. In Advances in Neural Information Processing Systems (NIPS). Retrieved from [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}