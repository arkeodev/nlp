{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Simple_Transformers/simple_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuU9A_QV-7PE"
      },
      "source": [
        "# Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x26jAFlT_tuR"
      },
      "source": [
        "To implement the transformer architecture based on the description you've provided, let's break it down into its components and outline how each can be coded using PyTorch. The transformer model, as described in the paper \"Attention is All You Need\", consists of several key components: an Encoder and Decoder architecture, with each having multiple layers of self-attention and feed-forward neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEx3cKI_BOfd"
      },
      "source": [
        "This guide and explanation of the Transformer architecture draws extensively from \"The Annotated Transformer\" by Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman, building upon the original work by Sasha Rush. This comprehensive article offers a detailed line-by-line implementation of the Transformer model, providing insights into its inner workings and illustrating the theoretical concepts with practical code examples. It serves as an invaluable resource for understanding and implementing the Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Complexity Comparison"
      ],
      "metadata": {
        "id": "sKdKEQX3E6EY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/time-complexity-of-transformers.png\" width=\"1000\" height=\"300\" alt=\"Time complexity\">\n",
        "    <figcaption>Time complexity</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "3zLRb49eETsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table from Vaswani et al. (2017), which is from the seminal paper \"Attention is All You Need,\" compares different neural network layer types across three dimensions: complexity per layer, number of sequential operations, and maximum path length:\n",
        "\n",
        "- **Self-Attention Layer**:\n",
        "  - Computational complexity per layer is $ O(n^2 \\cdot d) $, meaning it scales quadratically with the sequence length and linearly with the dimensionality of the representation.\n",
        "  - Sequential operations are constant $ O(1) $, allowing for high parallelization during training and inference.\n",
        "  - Maximum path length is also constant $ O(1) $, ensuring that long-range dependencies can be learned effectively as any position in the sequence can directly attend to any other position.\n",
        "\n",
        "- **Recurrent Layer**:\n",
        "  - Complexity is $ O(n \\cdot d^2) $, which scales linearly with the sequence length, making it less efficient for longer sequences compared to self-attention.\n",
        "  - Requires $ O(n) $ sequential operations, limiting parallelization and potentially increasing training time.\n",
        "  - Has a linear maximum path length $ O(n) $, which can make learning long-range dependencies more challenging.\n",
        "\n",
        "- **Convolutional Layer**:\n",
        "  - Shows a complexity of $ O(k \\cdot n \\cdot d^2) $, where $ k $ is the kernel size, suggesting an increase in computation if larger receptive fields are needed.\n",
        "  - Has constant sequential operations $ O(1) $, similar to self-attention, facilitating parallelization.\n",
        "  - Exhibits a logarithmic maximum path length $ O(\\log_k(n)) $, improving the learning of dependencies over recurrent layers but potentially less effective than self-attention for very long sequences.\n",
        "\n",
        "- **Restricted Self-Attention Layer (or Local Attention)**:\n",
        "  - Complexity per layer is reduced to $ O(r \\cdot n \\cdot d) $ by restricting attention to a window of $ r $ surrounding tokens, balancing computational efficiency and the ability to capture dependencies.\n",
        "  - Maintains constant sequential operations $ O(1) $, benefiting parallelization.\n",
        "  - Features a maximum path length of $ O(n/r) $, indicating a compromise in the ability to learn dependencies as the sequence length increases compared to full self-attention."
      ],
      "metadata": {
        "id": "bTfRpR-CEUWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dive into the self-attention mechanism, which forms the core component of transformers."
      ],
      "metadata": {
        "id": "s8xn6oKFFVj5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOS3epRYH6BU"
      },
      "source": [
        "## Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism is what allows Transformers to process data in parallel. It assigns a weight to each element in the input sequence, based on how relevant each element is to every other element."
      ],
      "metadata": {
        "id": "fUuZ62MidIOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of Self Attention"
      ],
      "metadata": {
        "id": "Heh5NsV5dKoG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF36kRWPH-y2"
      },
      "source": [
        "Self-attention can be described with three main components: Queries, Keys, and Values.\n",
        "\n",
        "- **Queries**: A set of vectors that is matched against the keys to decide the most important elements in the sequence.\n",
        "- **Keys**: Vectors that are paired with values; they are used to extract the information that queries look for.\n",
        "- **Values**: Vectors that contain the actual information of each element in the sequence that is extracted based on the weightage from the keys.\n",
        "\n",
        "Imagine you are in a library with a huge collection of books (the sequence), and you are looking for information on a specific topic.\n",
        "\n",
        "- The **query** is like your question about the topic you’re interested in.\n",
        "- The **keys** represent the index or summary of each book.\n",
        "- The **values** are the actual contents of the books.\n",
        "\n",
        "The librarian (the self-attention mechanism) checks your question against all summaries (keys) to determine which books (values) have the information you need. This process is done simultaneously for all the questions in parallel, which is what makes the transformer model so powerful and efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative Attention Approaches"
      ],
      "metadata": {
        "id": "9D1aD6_XVi2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Additive Attention**: While the scaled dot-product attention is widely used, alternative attention mechanisms exist, such as additive (or Bahdanau) attention, which uses a feed-forward network to compute attention scores instead of dot products. Each variant has its own advantages and use cases.\n",
        "\n",
        "- **Multi-Head Attention**: The Transformer model extends the basic scaled dot-product attention mechanism through the use of multi-head attention, where the model runs several attention processes in parallel. This approach allows the model to simultaneously attend to information from different representation subspaces at different positions, improving the model's ability to capture various types of relationships in the data.\n",
        "\n",
        "- **Self-Attention vs. Cross-Attention**: The module as described implements self-attention, where queries, keys, and values all come from the same source. In cross-attention settings, keys and values can come from a different source than queries, enabling interactions between different sequences (e.g., between an encoder and a decoder in a sequence-to-sequence model)."
      ],
      "metadata": {
        "id": "uOdKUvIgVorf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mathemetical Foundation of Attention"
      ],
      "metadata": {
        "id": "xJXcjF_aOU8s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u283a318IBSi"
      },
      "source": [
        "The following formula represents the scaled dot-product attention, which is the foundation of self-attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Components and Dimensions:**\n",
        "\n",
        "- $( Q )$: Queries matrix of dimension $( m \\times d_k )$, where $( m )$ is the number of queries and $( d_k )$ is the dimension of each query vector.\n",
        "- $( K )$: Keys matrix of dimension $( n \\times d_k )$, where $( n )$ is the number of keys (and values) and $( d_k )$ is the dimension of each key vector.\n",
        "- $( V )$: Values matrix of dimension $( n \\times d_v )$, where $( n )$ is the number of values and $( d_v )$ is the dimension of each value vector.\n",
        "- $( d_k )$: Dimension of the keys (and queries), used for scaling the dot product scores."
      ],
      "metadata": {
        "id": "5sjro7b7ZPaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Dot Product of $( Q )$ and $( K^T )$**\n",
        "\n",
        "- $( QK^T )$: Since $( Q )$ is $( m \\times d_k )$ and $( K )$ is $( n \\times d_k )$, when we take the transpose of $( K )$ to get $( K^T )$ which is $( d_k \\times n )$, the resulting dot product $( QK^T )$ will be a matrix of dimension $( m \\times n )$. Each element of this matrix represents a score reflecting the relevance of a query to a key.\n",
        "\n",
        "- This step calculates the similarity between all queries and all keys, serving as the basis for determining how the values should be weighted according to each query."
      ],
      "metadata": {
        "id": "UrLWhqd2ZZNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Scaling by $( \\sqrt{d_k} )$**\n",
        "\n",
        "- The scores in the $( m \\times n )$ matrix are scaled down by $( \\sqrt{d_k} )$, which doesn't change the dimensions of the matrix but normalizes the scores to prevent them from becoming too large.\n",
        "\n",
        "- Scaling helps maintain numerical stability, particularly in the softmax step, by ensuring the scores don't lead to extremely small gradients when the dimensionality $( d_k )$ is large."
      ],
      "metadata": {
        "id": "W45x5F9Rap3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Softmax Function**\n",
        "\n",
        "- The softmax function is applied across the rows of the scaled $( m \\times n )$ matrix. The dimensionality of the output remains $( m \\times n )$, but now each row sums to 1, representing probabilities.\n",
        "\n",
        "- Converting the scores to probabilities allows the model to probabilistically decide which keys (and thus, which values) are most relevant to each query, enabling a soft selection mechanism."
      ],
      "metadata": {
        "id": "yFLfIefQaz_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Multiplication by $( V )$**\n",
        "\n",
        "- The softmax output, still $( m \\times n )$, is then multiplied by $( V )$, which is $( n \\times d_v )$. The resulting matrix will have dimensions $( m \\times d_v )$, representing the final output of the attention mechanism.\n",
        "\n",
        "- This step effectively combines the values based on their relevance to each query, as determined by the weighted scores. The final dimension $( m \\times d_v )$ corresponds to the weighted sum of values for each query, now ready for further processing in the model.\n"
      ],
      "metadata": {
        "id": "J32Z64AEYzDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Implementation"
      ],
      "metadata": {
        "id": "jEdINcjFYuY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Lo6pNiPIJ3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58a1a39-4767-4894-a175-64a5f6a7e04c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights for the first sentence:\n",
            "[[0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.1320772  0.20914044 0.20045296 0.1320772  0.1320772  0.194175  ]\n",
            " [0.11958619 0.18149541 0.25215275 0.11958619 0.11958619 0.20759326]\n",
            " [0.15299844 0.15299844 0.15299844 0.21788799 0.15299844 0.17011824]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12397355 0.18226132 0.21520941 0.13784561 0.12397355 0.21673656]]\n",
            "Attention weights for the second sentence:\n",
            "[[0.20789086 0.14701445 0.1464956  0.14546337 0.20715715 0.14597857]\n",
            " [0.13342497 0.19895022 0.20393542 0.13201726 0.13201726 0.19965486]\n",
            " [0.12073931 0.18519945 0.23888728 0.12420309 0.11988857 0.2110823 ]\n",
            " [0.15216063 0.15216063 0.15763656 0.21669485 0.15216063 0.16918669]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12305363 0.18544201 0.21589025 0.13633987 0.12261934 0.21665489]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x over the specified axis.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
        "\n",
        "# Define the scaled dot product attention function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "    dk = K.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "    attention_weights = softmax(scaled_attention_logits)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Let's consider two sentences for our demonstration\n",
        "sentences = [\"The cat sits on the mat\", \"A dog lies on the rug\"]\n",
        "\n",
        "# Define embeddings for each unique word in the sentences\n",
        "# Embeddings are crafted so that similar words have closer embeddings\n",
        "word_embeddings = {\n",
        "    \"the\": np.array([1, 0, 0, 0, 0, 0, 0, 0]),\n",
        "    \"cat\": np.array([0, 1, 0, 0, 0.1, 0.2, 0.3, 0.4]),\n",
        "    \"sits\": np.array([0, 0.9, 1, 0, 0.2, 0.1, 0.4, 0.3]),\n",
        "    \"on\": np.array([0, 0, 0, 1, 0, 0, 0, 0]),\n",
        "    \"mat\": np.array([0, 0.8, 0.6, 0.3, 0, 0.6, 0.3, 0.2]),\n",
        "    \"a\": np.array([1, 0, 0, 0, 0, 0, 0, 0.1]),\n",
        "    \"dog\": np.array([0, 0.9, 0.1, 0, 0, 0.3, 0.4, 0.3]),\n",
        "    \"lies\": np.array([0, 1, 0.8, 0.1, 0.3, 0.1, 0.4, 0.2]),\n",
        "    \"rug\": np.array([0, 0.9, 0.6, 0.3, 0, 0.5, 0.3, 0.1])\n",
        "}\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
        "\n",
        "# Convert sentences to embeddings using the predefined embeddings\n",
        "sentence_embeddings = [[word_embeddings[word] for word in sentence] for sentence in tokenized_sentences]\n",
        "\n",
        "# Pad sentences to the same length\n",
        "max_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "padded_embeddings = [np.array(sentence + [np.zeros(3)] * (max_length - len(sentence))) for sentence in sentence_embeddings]\n",
        "\n",
        "# Stack the embeddings to create the Q, K, and V matrices\n",
        "Q = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "K = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "V = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "\n",
        "# Apply the scaled dot product attention function\n",
        "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Let's print the attention weights for the first sentence\n",
        "print(\"Attention weights for the first sentence:\")\n",
        "print(attention_weights[0])\n",
        "\n",
        "# Let's print the attention weights for the second sentence\n",
        "print(\"Attention weights for the second sentence:\")\n",
        "print(attention_weights[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RskdI0JuIPZ9"
      },
      "source": [
        "**For the first sentence \"The cat sits on the mat\":**\n",
        "\n",
        "1. The attention weights for the word \"the\" (first row) seem to be distributed across \"the\", \"sits\", and \"mat\" with higher weights (0.2079) compared to \"cat\" and \"on\" (0.1460). This might indicate that the model perceives a stronger association between \"the\" and the action (\"sits\") and the object (\"mat\") of the sentence.\n",
        "\n",
        "2. The word \"cat\" (second row) has the highest attention weight when paired with itself (0.2091) and with \"sits\" (0.2004), which could suggest that the model identifies \"cat\" and the action it's performing as key components of the sentence.\n",
        "\n",
        "3. The word, \"sits\" (third row) has the highest attention weight in combination with itself (0.2521) and substantial weight with \"cat\" (0.1814), potentially indicating the model's understanding of the importance of the verb in the context of the subject.\n",
        "\n",
        "4. Finally, \"mat\" (sixth row) pays more attention to \"sits\" (0.2152) as expected.\n",
        "\n",
        "**For the second sentence \"A dog lies on the rug\":**\n",
        "\n",
        "1. \"Dog\" (second row) shows higher attention weights for itself (0.1989) and \"lies\" (0.2039), which is consistent with recognizing the subject and verb relationship.\n",
        "\n",
        "3. \"Lies\" (third row) has the highest weight with itself (0.2388), and a notable weight with \"dog\" (0.1851) and \"rug\" (0.2110), reinforcing the connection between the subject, object and the action.\n",
        "\n",
        "4. \"Rug\" (sixth row), much like \"mat\" in the first sentence, shows a relatively high weight for \"dog\" (0.1854) and \"lies\" (0.2158), as expected.\n",
        "\n",
        "In both sentences, it is evident that the attention mechanism is picking up on the grammatical structure and the relationships between subjects, verbs, and objects. It emphasizes self-attention and relationships that make sense in the context of the sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation the Model"
      ],
      "metadata": {
        "id": "hXnStVipd1oK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-CjaKz4B8X_"
      },
      "source": [
        "## Preliminary Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkesXoMgB_Vc"
      },
      "source": [
        "First, ensure all required packages are installed and import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "49ToAoDBCFLR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn.functional import pad\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import altair as alt\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "import os\n",
        "from os.path import exists\n",
        "import math\n",
        "import copy\n",
        "import itertools\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device globally\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "pqsjolHMbEC4",
        "outputId": "c8ffcda7-3d39-4613-fc2e-669bdc2e41eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "rd5J2JpDH5HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to produce N identical layers\n",
        "def clones(module, N):\n",
        "  \"Produce N identical layers.\"\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "YKKM9QA6H2ue"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUoEXHACCq3"
      },
      "source": [
        "## Core Components of the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnLqaozUCP54"
      },
      "source": [
        "### Scaled Dot-Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCDwbr31CTcO"
      },
      "source": [
        "The core of the transformer model is the attention mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/attention.png\" width=\"180\" height=\"300\" alt=\"Attention Mechanism\">\n",
        "    <figcaption>Attention Mechanism</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "tVJwk5k_FibE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U38lQRe2CXRh"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout=None):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        # Initialize dropout for attention scores, if provided.\n",
        "        self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Dimension of the key vectors, used for scaling down the dot products.\n",
        "        d_k = query.size(-1)\n",
        "        # Compute the dot products between queries and keys for each batch and head,\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "        # Scale the results\n",
        "        scaled_score = scores / math.sqrt(d_k)\n",
        "        # Apply masking if provided (for attention to consider only certain positions).\n",
        "        if mask is not None:\n",
        "            scaled_score = scaled_score.masked_fill(mask == 0, -1e9)\n",
        "        # Apply softmax to obtain attention probabilities.\n",
        "        p_attn = F.softmax(scaled_score, dim=-1)\n",
        "        # Optionally apply dropout to attention scores.\n",
        "        if self.dropout is not None:\n",
        "            p_attn = self.dropout(p_attn)\n",
        "        # Weight the values by the computed attention probabilities.\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ScaledDotProductAttention` module implements the attention mechanism that is at the heart of the Transformer model's effectiveness. Its roles and effects on the structure are as follows:\n",
        "\n",
        "- **Attention Calculation**: This module computes the attention scores by taking the dot product of the query with the key. It scales the dot product by the square root of the dimensionality of the key to prevent extremely small gradients when the dimensionality is large. This scaling helps stabilize gradients during training.\n",
        "\n",
        "- **Masking**: The optional mask allows the model to selectively ignore certain positions within the input sequence. This is particularly useful for masking out padding tokens in the input sequence or for preventing the model from peeking at future tokens when processing sequences in an autoregressive manner (e.g., during training a language model).\n",
        "\n",
        "- **Dropout**: Applying dropout to the attention scores is a regularization technique that helps prevent overfitting by randomly zeroing out some of the scores before computing the final attention weights.\n",
        "\n",
        "- **Softmax**: The softmax function converts the attention scores into probabilities, ensuring that they are non-negative and sum up to 1. This step allows the model to essentially \"focus\" on the most relevant parts of the input.\n",
        "\n",
        "- **Output**: The module outputs the weighted sum of the value vectors, scaled by the attention probabilities, which is then used in subsequent layers of the model. It also returns the attention probabilities themselves, which can be useful for analysis or for visualizing the model's attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "FK4gLRowVdWH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDmzVYO7Caqs"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pG3peUkCdWE"
      },
      "source": [
        "Allows the model to jointly attend to information from different representation subspaces."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/mutl-head-attention.png\" width=\"180\" height=\"300\" alt=\"Multi Head Attention\">\n",
        "    <figcaption>Multi Head Attention</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "ZE4m9LylFjIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4RL7T6pwCfnb"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        # Ensure the model dimension is divisible by the number of heads.\n",
        "        assert d_model % h == 0\n",
        "        # Dimension of each attention head.\n",
        "        self.d_k = d_model // h\n",
        "        # Number of attention heads.\n",
        "        self.h = h\n",
        "        # Linear layers for query, key, value, and output transformation.\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        # Scaled dot-product attention mechanism.\n",
        "        self.attn = ScaledDotProductAttention(dropout)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Expand mask for all heads if provided.\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        # Batch size\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # Instead of using a fixed size, calculate the sequence length dynamically\n",
        "        # based on the current shape of the query, key, and value tensors.\n",
        "        seq_len = query.size(1)\n",
        "\n",
        "        # Perform the linear operations and split into h heads,\n",
        "        # adjusting the sequence length dynamically.\n",
        "        query, key, value = [\n",
        "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "            for lin, x in zip(self.linears, (query, key, value))\n",
        "        ]\n",
        "\n",
        "        # Apply attention on all the projected vectors in batch.\n",
        "        x, attn_weights = self.attn(query, key, value, mask=mask)\n",
        "\n",
        "        # Concat using a view and apply a final linear.\n",
        "        # Adjust the operation to handle dynamic sequence length.\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "\n",
        "        return self.linears[-1](x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `MultiHeadedAttention` module plays a crucial role in the Transformer architecture by enabling the model to focus on different parts of the input sequence simultaneously. Here's how it fits into the overall structure and its impact:\n",
        "\n",
        "- **Parallel Attention Heads**: This module divides the model's attention mechanism into multiple \"heads,\" allowing the model to jointly attend to information from different representation subspaces at different positions. This parallelization helps capture more complex dependencies in the data.\n",
        "\n",
        "- **Query, Key, Value Transformations**: Before computing attention, the module transforms the input queries, keys, and values through separate linear layers. This transformation is critical for allowing each attention head to learn different aspects of the data.\n",
        "\n",
        "- **Scaled Dot-Product Attention**: Each head uses scaled dot-product attention to compute how much focus to put on other parts of the input sequence. The scaling factor (the square root of the dimension of the key vectors) helps stabilize the gradients during training.\n",
        "\n",
        "- **Masking Support**: The module supports masking out certain positions within the input sequence (e.g., for ignoring padding tokens or for enforcing causality in autoregressive models), which is essential for flexible sequence processing.\n",
        "\n",
        "- **Output Transformation**: After computing the attention in all heads, the module combines the outputs and passes them through another linear transformation to produce the final result. This step is crucial for reintegrating the information learned by each head."
      ],
      "metadata": {
        "id": "CoOLEfrbiR_X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-vmKmdLCkkd"
      },
      "source": [
        "### Position-wise Feed-Forward Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3pxg70tCncl"
      },
      "source": [
        "Each layer in the encoder and decoder contains a fully connected feed-forward network."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `PositionwiseFeedForward` class is a component of the Transformer architecture, designed to process the output of the attention mechanism.\n",
        "\n",
        "The PositionwiseFeedForward network plays a crucial role in adding depth and complexity to the Transformer model. While the attention mechanism allows the model to focus on different parts of the input sequence, the feedforward network enables complex transformations of the attention-modulated representations. This combination of attention and position-wise feedforward processing allows the Transformer to effectively capture the intricacies of sequential data.\n",
        "\n",
        "\n",
        "Here's a breakdown of its structure and function:\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "    - **d_model**: This parameter represents the dimensionality of the input and output of this feedforward network. It's consistent with the model's overall dimensionality, ensuring that the input and output of the feedforward network can seamlessly integrate with other parts of the Transformer.\n",
        "\n",
        "    - **d_ff**: This is the dimensionality of the hidden layer within the feedforward network. Typically, `d_ff` is significantly larger than `d_model`, allowing the network to capture more complex interactions in the data. This expansion and subsequent projection back to `d_model` is a common technique for increasing the expressiveness of neural networks.\n",
        "\n",
        "    - **dropout**: Specifies the dropout rate for regularization. Dropout is applied to the output of the first activation function.\n",
        "\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "    - The input `x` is first passed through a linear transformation (`self.w_1`), increasing its dimensionality from `d_model` to `d_ff`. This is followed by a ReLU activation function to introduce non-linearity, enabling the network to model complex relationships.\n",
        "\n",
        "    - A dropout is applied after the ReLU function, randomly setting a fraction of its inputs to zero, which helps in regularization.\n",
        "\n",
        "    - Finally, the output from the dropout layer is passed through another linear transformation (`self.w_2`) to project it back to the original dimensionality (`d_model`). This ensures that the output of the feedforward network can be fed directly into subsequent layers or components of the Transformer."
      ],
      "metadata": {
        "id": "rVDme49TuK1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RowBB9YlCqiY"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d32j_xZuCt95"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfvJ6aQHCxl8"
      },
      "source": [
        "\n",
        "Convert input tokens and output tokens to vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Embeddings` are the entry point of textual (or any token-based) data into the Transformer model, transforming discrete token IDs into dense vectors that can carry semantic information. These vectors are then processed through the model's subsequent layers. The scaling factor $(\\sqrt{d_{model}})$ is applied to give the embeddings a similar scale to the model's internal activations, aiming to facilitate optimization.\n",
        "\n",
        "\n",
        "Here's a breakdown of its components and functionality:\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "    - **d_model**: This parameter represents the size of the embedding vectors. In the context of Transformers, `d_model` is a fixed-size dimension that all embeddings and subsequent layers adhere to, ensuring uniformity across the model's architecture. It's a critical hyperparameter that influences the model's capacity to encode information.\n",
        "\n",
        "    - **vocab**: This is the size of the vocabulary, representing the total number of unique tokens that the model can recognize. The `vocab` size determines the number of rows in the embedding matrix, where each row corresponds to a specific token in the vocabulary.\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "    - The `forward` method takes an input tensor `x` containing token IDs and returns their corresponding embeddings. The embeddings are scaled by the square root of `d_model`, a normalization step recommended in the original Transformer paper to reduce the variance of the embeddings, helping stabilize the learning process.\n"
      ],
      "metadata": {
        "id": "79ldd0-KvgC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_TcjR--JC0Ix"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x.long()) * math.sqrt(self.d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embedding"
      ],
      "metadata": {
        "id": "7a28krc2kCeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence."
      ],
      "metadata": {
        "id": "tuVkgZT2kJPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `PositionalEncoding` class is a component designed to add information about the position of tokens within a sequence to the embeddings in a Transformer model. Since the Transformer architecture relies on attention mechanisms that do not inherently understand sequence order (unlike RNNs or LSTMs), positional encodings are crucial for enabling the model to consider the order of tokens when processing text or other sequential data. Without them, the model would treat sequences as unordered sets of tokens, significantly reducing its ability to perform tasks that rely on the sequential nature of the data, such as language understanding, translation, and generation.\n",
        "\n",
        "\n",
        "Here's a detailed explanation:\n",
        "\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "    - **d_model**: The dimensionality of the model's embeddings, which is the size of the vectors representing each token. Positional encodings must match this dimensionality to be added to the token embeddings.\n",
        "\n",
        "    - **dropout**: The dropout rate for regularization. After adding positional encodings to the token embeddings, dropout is applied to the combined embeddings to prevent overfitting.\n",
        "\n",
        "    - **max_len**: The maximum length of the input sequences that the model can handle. This parameter defines the size of the positional encoding matrix, ensuring that every position in a sequence, up to `max_len`, has a unique positional encoding vector.\n",
        "\n",
        "    - **pe (Positional Encoding Matrix)**: A precomputed matrix where each row represents the positional encoding for a specific position in a sequence. The encodings alternate between sine and cosine functions across different dimensions, allowing the model to distinguish positions based on these patterns. The use of sine and cosine functions helps the model to learn to attend to relative positions since the encoding for a position is a function of its distance from other positions.\n",
        "\n",
        "- **Forward Pass (`forward`)**:\n",
        "    - The method receives the token embeddings `x` and adds the positional encodings to them based on the length of the input sequence. The positional encoding vectors are not learned but are a fixed function of the position. Adding these encodings to the embeddings allows the model to consider the position of tokens when processing sequences.\n",
        "    \n",
        "    - The result is then passed through a dropout layer for regularization before being returned."
      ],
      "metadata": {
        "id": "NsAZK_KFxlI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Precompute the positional encodings in log space.\n",
        "        # pe is the positional encoding matrix where each row corresponds to a position\n",
        "        # and each column corresponds to a dimension in the d_model space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)  # Generate a column vector of positions.\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )  # Generate the divisors for the sine and cosine functions.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices in the dimensions.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices.\n",
        "        pe = pe.unsqueeze(0)  # Add a batch dimension.\n",
        "        self.register_buffer(\"pe\", pe)  # Register pe as a buffer that is not a model parameter.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to each embedding vector in the batch.\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)  # Ensures gradients are not computed for pe.\n",
        "        return self.dropout(x)  # Apply dropout and return the result.\n"
      ],
      "metadata": {
        "id": "AGf73BmYkNl8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalisation"
      ],
      "metadata": {
        "id": "hOZYfN8XiVv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We employ a residual connection around each of the two sub-layers, followed by layer normalization"
      ],
      "metadata": {
        "id": "W59ygv8aiaEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Transformer model, layer normalization is applied within both the encoder and decoder components.\n",
        "\n",
        "Layer Normalization works by normalizing the inputs across the features for each data sample in a batch. It's particularly effective in recurrent neural networks and has been beneficial in Transformer models. By normalizing the inputs, it helps control the scale of gradients, making the optimization landscape smoother, which often leads to faster training and better generalization.\n",
        "\n",
        "Here's a detailed explanation:\n",
        "\n",
        "- **Learnable Parameters (`self.gamma` and `self.beta`)**: Unlike batch normalization, which normalizes across the batch dimension, layer normalization performs normalization for each sample independently and uses learnable parameters to scale (`gamma`) and shift (`beta`) the normalized values.\n",
        "\n",
        "- **Epsilon (`self.eps`)**: A small constant added to the standard deviation to avoid division by zero.\n",
        "\n",
        "- **Normalization Process**: The forward method computes the mean and standard deviation for each input sample across its features. It then normalizes the inputs by subtracting the mean and dividing by the standard deviation, adjusted by `eps`. Finally, it applies the learned scaling and shifting parameters to each feature."
      ],
      "metadata": {
        "id": "K4zhkCyr05Eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        # Learnable Parameters to scale and shift the normalized values.\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps  # Small epsilon value to prevent division by zero in normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate the mean of the features.\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        # Calculate the standard deviation of the features.\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        # Normalize the features. The normalization formula is:\n",
        "        # normalized_x = gamma * (x - mean) / (std + epsilon) + beta\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "WO24_zkvieI3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sublayer Connection"
      ],
      "metadata": {
        "id": "2fzoW6xaiq0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `SublayerConnection` module is designed to wrap around any sub-layer of the Transformer model, such as a self-attention layer or a position-wise feedforward network, to incorporate a residual connection and layer normalization around that sub-layer. This is critical for the model's ability to train deeply.\n",
        "\n",
        "- **Layer Normalization**: The input to the sub-layer is first normalized. Normalization stabilizes the learning process by ensuring that the input distributions to layers do not shift dramatically.\n",
        "\n",
        "- **Sub-layer Functionality**: The `sublayer` argument represents any function or module that takes the normalized input and produces an output of the same dimension (`d_model`). This design allows the `SublayerConnection` module to be agnostic to the specific type of sub-layer it is augmenting.\n",
        "\n",
        "- **Dropout and Residual Connection**: Dropout is applied to the output of the sub-layer for regularization. The dropout-enhanced output is then added to the original input `x` to form a residual connection. This connection allows gradients to flow directly through the network, mitigating the vanishing gradient problem and enabling the effective training of deep networks.\n",
        "\n",
        "- **Output Dimension Consistency**: The requirement that all sub-layers and embedding layers produce outputs of dimension `d_model` ensures consistency and simplicity in the model architecture. This uniformity is crucial for stacking multiple layers and for the effectiveness of residual connections."
      ],
      "metadata": {
        "id": "eD8yehcD3pkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is, the output of each sub-layer is\n",
        "\n",
        "$$\n",
        "LayerNorm(x+Sublayer(x))\n",
        "$$\n",
        "\n",
        "where Sublayer(x) is the function implemented by the sub-layer itself.\n",
        "\n",
        "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model} = 512$\n"
      ],
      "metadata": {
        "id": "4LYpg6GUivNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        # Normalizes the input features across the features dimension.\n",
        "        self.norm = LayerNorm(size)\n",
        "        # Dropout module\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # Normalize the input, pass through the sublayer, apply dropout,\n",
        "        # then add the input x for the residual connection.\n",
        "        # This ensures that the input can flow directly (or with minimal\n",
        "        # modification) through the layers of the network,\n",
        "        # preventing the vanishing gradients problem in deep networks.\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "8vrQ0kQNjQqc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator"
      ],
      "metadata": {
        "id": "BiVPrXrFkd4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Generator` class in a Transformer model architecture is the final step in the model's decoder that maps the decoder output to a probability distribution over the vocabulary.\n",
        "\n",
        "In a typical sequence generation task, this `Generator` module's output can be used during training to calculate the loss by comparing the predicted log probabilities against the ground truth. During inference, the logits (before the softmax) are often used with decoding strategies like greedy decoding, beam search, or sampling to select the next token in the sequence.\n",
        "\n",
        "Here's detailed explanations:\n",
        "\n",
        "- **Projection**: The linear layer (`nn.Linear`) projects the high-dimensional decoder outputs (which encode the semantic information of the input sequence and the so-far generated output sequence) down to the vocabulary space. The output of this layer is a set of logits, with one logit per vocabulary token, for each position in the input sequence.\n",
        "\n",
        "- **Log Softmax**: The softmax function is commonly used to convert logits to probabilities, ensuring that the output values are non-negative and sum to one, thus forming a valid probability distribution. The use of `log_softmax` instead of plain `softmax` is for numerical stability, especially useful in the context of calculating the loss during training (e.g., when using the Negative Log Likelihood loss)."
      ],
      "metadata": {
        "id": "JBhhYn9-5-GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        # Linear transformation that projects from model dimension (d_model) to vocabulary size.\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the linear transformation followed by log softmax.\n",
        "\n",
        "        Parameters:\n",
        "        - x: The input tensor with shape [batch size, sequence length, d_model],\n",
        "             representing the output from the Transformer's final decoder layer.\n",
        "\n",
        "        Returns:\n",
        "        - Log probabilities of each token in the vocabulary for each position\n",
        "          in the sequence and for each sequence in the batch.\n",
        "        \"\"\"\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "1ggjoNxokfyH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "B-Yu4_AjhtXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder is composed of a stack of $N = 6$ identical layers."
      ],
      "metadata": {
        "id": "mP-z_BEeiAct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/encoder.png\" width=\"200\" height=\"400\" alt=\"Encoder\">\n",
        "    <figcaption>Encoder</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "fhAHHKEgnkOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Encoder` and `EncoderLayer` classes are fundamental components of the Transformer model's encoder architecture. The encoder's role is to process the input sequence and generate a set of representations that capture the semantics of the input sequence, considering both the content and the context around each token. These representations are then used by the decoder to generate the output sequence.\n",
        "\n",
        "Here's a breakdown of these components with detailed comments:\n",
        "\n",
        "\n",
        "- **Encoder**: The encoder takes the entire input sequence and transforms it into a continuous representation that holds the contextual information of every token. This is achieved through a series of encoder layers that each perform self-attention and position-wise feed-forward computations.\n",
        "\n",
        "- **EncoderLayer**: Each encoder layer first processes its input with a self-attention mechanism, allowing each position in the input to attend over all positions in the input sequence to capture contextual information. It then applies a position-wise feed-forward network to each position independently. Both the self-attention and feed-forward networks are wrapped with residual connections and layer normalization (encapsulated by the `SublayerConnection` module), which helps stabilize training and allows for deeper models."
      ],
      "metadata": {
        "id": "E6CENIpj8VRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Core encoder is a stack of N layers.\n",
        "    The encoder processes the input sequence in a series of steps,\n",
        "    each performed by an EncoderLayer.\n",
        "    All layers in the encoder are identical but do not share weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Clones the given layer N times to create a stack of N identical layers.\n",
        "        self.layers = clones(layer, N)\n",
        "        # Applies layer normalization to the output of the last encoder layer.\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Pass the input (and mask) through each layer in turn.\n",
        "\n",
        "        Parameters:\n",
        "        - x: The input sequence to the encoder.\n",
        "        - mask: A mask tensor indicating which positions are valid\n",
        "        (e.g., to exclude padding positions from attention computations).\n",
        "\n",
        "        Returns:\n",
        "        - The output of the last layer normalization, representing\n",
        "        the encoder's final representations of the input sequence.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "hdxtR0eziN_V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder is made up of self-attention and feed-forward networks.\n",
        "    This design allows each token in the input sequence to attend\n",
        "    to all tokens in the same sequence (self-attention),\n",
        "    followed by position-wise feed-forward processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # The self-attention mechanism.\n",
        "        self.self_attn = self_attn\n",
        "        # The position-wise feed-forward network.\n",
        "        self.feed_forward = feed_forward\n",
        "        # Two instances of SublayerConnection, which apply residual connections\n",
        "        # and layer normalization around the sub-layers.\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        # The size of the input and output dimension of the layer.\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Process the input x through the self-attention and feed-forward networks sequentially,\n",
        "        with each being wrapped in a SublayerConnection to include residual connections and normalization.\n",
        "\n",
        "        Parameters:\n",
        "        - x: The input to the encoder layer.\n",
        "        - mask: A mask tensor for the self-attention mechanism to ignore certain positions.\n",
        "\n",
        "        Returns:\n",
        "        - The output of the encoder layer, after processing by self-attention and feed-forward networks.\n",
        "        \"\"\"\n",
        "        # Apply self-attention sub-layer, with residual connection and normalization.\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        # Apply feed-forward sub-layer, with residual connection and normalization.\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "WxIStTI0hwJl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Layer"
      ],
      "metadata": {
        "id": "Bw5D_aSLjca1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is also composed of a stack of $N=6$ identical layers."
      ],
      "metadata": {
        "id": "uYFfrA5mjgnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/decoder.png\" width=\"200\" height=\"480\" alt=\"Decoder\">\n",
        "    <figcaption>Decoder</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "7y1R8Sx6nlc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Decoder` and `DecoderLayer` classes are central components of the Transformer model's decoder side, designed for sequence generation tasks. The decoder processes the encoder's output (referred to as \"memory\" here) along with its own input to generate the final output sequence.\n",
        "\n",
        "Here's a detailed breakdown with comments for clarity:\n",
        "\n",
        "- **Decoder**: Similar to the encoder, the decoder is composed of a stack of layers. However, the decoder layers also include cross-attention mechanisms that allow the decoder to focus on relevant parts of the input sequence as it generates each token of the output sequence. The decoder uses masking to ensure that predictions for a given position can only depend on known outputs at positions before it, not future positions.\n",
        "\n",
        "- **DecoderLayer**: Each decoder layer has three main components:\n",
        "    - **Self-Attention Layer**: Allows each position in the decoder to attend to all positions up to and including that position in the decoder sequence.\n",
        "    - **Cross-Attention Layer**: Allows each position in the decoder to attend to all positions in the encoder sequence, integrating information from the input sequence.\n",
        "    - **Feed-Forward Network**: Processes each position of the decoder output in isolation, further transforming the data after integrating information from the encoder and previous decoder outputs."
      ],
      "metadata": {
        "id": "hA8Gmz_g-SPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Generic N layer decoder with masking.\n",
        "\n",
        "    The decoder is responsible for generating the output sequence by processing\n",
        "    the encoder output and the previous decoder outputs. It includes mechanisms\n",
        "    for self-attention within the decoder and cross-attention to the encoder outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Stack of N identical layers.\n",
        "        self.layers = clones(layer, N)\n",
        "        # Layer normalization applied to the output of the final layer.\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Pass the input through each layer in turn.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input to the decoder, typically the previous output.\n",
        "        - memory: Output from the encoder to be used in cross-attention.\n",
        "        - src_mask: Mask for the encoder output.\n",
        "        - tgt_mask: Mask for the decoder input to prevent attending to future tokens.\n",
        "\n",
        "        Returns:\n",
        "        - The output of the decoder after processing through N layers and normalization.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "qb1N73B4jme_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
      ],
      "metadata": {
        "id": "q2RiKR4Ajp6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder is made of self-attention, source-attention, and feed forward networks.\n",
        "\n",
        "    Each layer in the decoder includes two attention mechanisms: one for attending to\n",
        "    the previous decoder outputs (self-attention) and one for attending to the encoder\n",
        "    outputs (source-attention). This is followed by a position-wise feed-forward network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size  # Size of the input dimension.\n",
        "        # Self-attention mechanism for attending to previous decoder outputs.\n",
        "        self.self_attn = self_attn\n",
        "        # Source-attention mechanism for attending to encoder outputs.\n",
        "        self.src_attn = src_attn\n",
        "        # Position-wise feed-forward network.\n",
        "        self.feed_forward = feed_forward\n",
        "        # Residual connections around each of the sub-layers.\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Process input through self-attention, source-attention, and feed-forward layers.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input to the decoder layer.\n",
        "        - memory: Encoder outputs.\n",
        "        - src_mask: Mask for the encoder outputs.\n",
        "        - tgt_mask: Mask for the decoder inputs to ensure causality.\n",
        "\n",
        "        Returns:\n",
        "        - The output of the decoder layer after processing.\n",
        "        \"\"\"\n",
        "        m = memory\n",
        "        # Apply self-attention with causality mask to prevent forward-looking.\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        # Apply source-attention, attending to encoder outputs.\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        # Apply the feed-forward network.\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "Yu03L_ARjwfU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subsequent Mask"
      ],
      "metadata": {
        "id": "9J6m98n_noYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `subsequent_mask` function is a crucial component in the Transformer model, specifically within the decoder. It is designed to prevent positions from \"peeking\" into the future by masking out subsequent positions during the self-attention calculation in the decoder layers. This masking strategy is vital for ensuring that the prediction for a given position in the output sequence can only depend on the known outputs at positions before it, thereby enforcing causality in the model's predictions. Here's an explanation of how the function works and its significance:\n",
        "\n",
        "\n",
        "- **Causality in Decoding**: In the context of sequence generation, causality ensures that the model cannot rely on future tokens when predicting the current token. This is essential for tasks like text generation, where the model generates one token at a time.\n",
        "\n",
        "- **Self-Attention Masking**: The Transformer model uses self-attention mechanisms to weigh the importance of different positions in the input data. For the decoder, the `subsequent_mask` is applied to the attention scores before the softmax step, effectively setting the scores for disallowed positions to negative infinity (after applying a log mask). This ensures that these positions do not contribute to the softmax output.\n",
        "\n",
        "- **Training vs. Inference**: During training, the Transformer model can process entire sequences in parallel, which greatly improves efficiency. The `subsequent_mask` enables this parallelization by ensuring that the model still respects the sequential nature of the data, even when processing many positions simultaneously. During inference, the model generates one token at a time, and the mask ensures that each prediction only uses appropriate past context."
      ],
      "metadata": {
        "id": "6xIK5jD2AoO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "    \"\"\"\n",
        "    Creates a mask to prevent attention to subsequent positions.\n",
        "\n",
        "    Parameters:\n",
        "    - size: The size of the sequence (length of the sequence).\n",
        "\n",
        "    Returns:\n",
        "    - A mask tensor of shape (1, size, size) where positions are filled with False (0)\n",
        "      for allowed attentions and True (1) for positions that should be masked.\n",
        "      This means that for any given row in the mask, positions to the right\n",
        "      (i.e., subsequent positions) are masked with True.\n",
        "    \"\"\"\n",
        "    # Create a square matrix of size `size` filled with ones on and above the diagonal.\n",
        "    subsequent_mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.uint8)\n",
        "    # Invert the mask: positions with ones become zeros (masked) and vice versa.\n",
        "    return subsequent_mask == 0"
      ],
      "metadata": {
        "id": "Q3n9O22Jn7RU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v85Es50DHEL"
      },
      "source": [
        "## Full Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGDK_at2HtYa"
      },
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Simple_Transformers/images/overall-architecture-of-transformers.png\" width=\"400\" height=\"400\" alt=\"Transformers Architecture\">\n",
        "    <figcaption>Transformers Architecture</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSmgcW5mDKOz"
      },
      "source": [
        "Now, let's put together the main Transformer architecture combining the encoder, decoder, and other components previously described."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1, device=None):\n",
        "        super(Transformer, self).__init__()\n",
        "         # Set device attribute early, before other initializations\n",
        "        self.device = device\n",
        "        # Utility for deep copying modules (used for encoder/decoder layers and components).\n",
        "        c = copy.deepcopy\n",
        "        # Set vocab sizes\n",
        "        self.src_vocab_size = src_vocab\n",
        "        self.tgt_vocab_size = tgt_vocab\n",
        "        # Multi-headed attention mechanism.\n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        # Position-wise feed-forward network.\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        # Positional encoding for input sequences.\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "        # Encoder consisting of N layers.\n",
        "        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
        "        # Decoder also consisting of N layers but with its own self-attention mechanism for masked attention.\n",
        "        self.decoder = Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N)\n",
        "        # Embedding layers for source and target sequences, with positional encodings added.\n",
        "        self.src_embed = nn.Sequential(Embeddings(d_model, self.src_vocab_size), c(position))\n",
        "        self.tgt_embed = nn.Sequential(Embeddings(d_model, self.tgt_vocab_size), c(position))\n",
        "        # Final linear and softmax layer to generate predictions.\n",
        "        self.generator = Generator(d_model, tgt_vocab)\n",
        "\n",
        "        # Initialize parameters with Xavier uniform distribution for better convergence.\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        # Encodes the source sequence and then decodes it into the target sequence.\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # Pass the source sequence and mask through the encoder.\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        # Decode the encoded source sequence (memory) into the target sequence.\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n"
      ],
      "metadata": {
        "id": "VBMqcFjqNyDf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference Test"
      ],
      "metadata": {
        "id": "Adac3Ouqk20_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple target vocabulary mapping\n",
        "tgt_vocab = {\n",
        "    '<pad>': 0,\n",
        "    '<bos>': 1,\n",
        "    '<eos>': 2,\n",
        "    '<unk>': 3,  # Tokens for unknown words\n",
        "    # Example token mappings (you would typically have more tokens based on your dataset)\n",
        "    'hello': 4,\n",
        "    'world': 5,\n",
        "    \"how\": 6,\n",
        "    \"are\": 7,\n",
        "    \"you\": 8,\n",
        "    \"yes\": 9,\n",
        "}\n",
        "\n",
        "# Define a similar source vocabulary for completeness (used when initializing the Transformer)\n",
        "src_vocab = {\n",
        "    '<pad>': 0,\n",
        "    '<bos>': 1,\n",
        "    '<eos>': 2,\n",
        "    '<unk>': 3,\n",
        "    # Source tokens\n",
        "    'hallo': 4,\n",
        "    'welt': 5,\n",
        "    \"wie\": 6,\n",
        "    \"bist\": 7,\n",
        "    \"du\": 8,\n",
        "    \"ja\": 9,\n",
        "}\n",
        "\n",
        "# Instantiate Transformer model with the vocab sizes\n",
        "test_model = Transformer(\n",
        "    src_vocab=len(src_vocab),\n",
        "    tgt_vocab=len(tgt_vocab),\n",
        "    N=2,\n",
        "    d_model=512,\n",
        "    d_ff=2048,\n",
        "    h=8,\n",
        "    dropout=0.1,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def inference_test(test_model, tgt_vocab):\n",
        "    test_model = test_model.to(device)\n",
        "    test_model.eval()\n",
        "\n",
        "    # Ensure src indices are within vocabulary range\n",
        "    src_indices = [min(index, len(src_vocab) - 1) for index in [1, 2, 3, 4, 5, 6, 7, 8, 9, 4]]\n",
        "    src = torch.LongTensor([src_indices]).to(device)\n",
        "\n",
        "    src_mask = torch.ones(1, 1, 10).to(device)\n",
        "    memory = test_model.encode(src, src_mask)\n",
        "    ys = torch.zeros(1, 1).type_as(src).fill_(tgt_vocab['<bos>']).to(device)\n",
        "\n",
        "    for _ in range(9):\n",
        "        tgt_mask = subsequent_mask(ys.size(1)).type_as(src.data)\n",
        "        out = test_model.decode(memory, src_mask, ys, tgt_mask)\n",
        "        out_probs = test_model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(out_probs, dim=1)\n",
        "        next_word = next_word.item()\n",
        "        ys = torch.cat([ys, torch.tensor([[next_word]]).type_as(src.data)], dim=1)\n",
        "\n",
        "    print(\"Example Untrained Model Prediction:\", ys)\n",
        "\n",
        "def run_tests():\n",
        "    # Run the inference test 10 times to demonstrate the generation process.\n",
        "    for _ in range(10):\n",
        "        inference_test(test_model, tgt_vocab)\n",
        "\n",
        "run_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6702Kmgk-au",
        "outputId": "a2a4062b-0068-458c-ba58-8300e307039c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n",
            "Example Untrained Model Prediction: tensor([[1, 7, 7, 6, 0, 0, 0, 7, 7, 6]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P466rmc9DrTE"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Download"
      ],
      "metadata": {
        "id": "bHU2nXg2KVi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch torchtext spacy -q"
      ],
      "metadata": {
        "id": "Yjobd6huKIqS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install portalocker -q"
      ],
      "metadata": {
        "id": "HKrjQ1ADjzRo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "fmOHDt3PKaxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy download en_core_web_sm -q\n",
        "! python -m spacy download de_core_news_sm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqQycHlWOJFW",
        "outputId": "9c940569-d0ec-40e7-d071-d15107fdf885"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Tokenization functions\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Vocabulary building function\n",
        "def build_vocab(tokenized_texts, min_freq=1):\n",
        "    # Count the frequency of tokens in the tokenized texts\n",
        "    counter = Counter(itertools.chain(*tokenized_texts))\n",
        "\n",
        "    # Filter tokens by min_freq and sort by frequency in descending order\n",
        "    vocab_tokens = sorted([token for token, freq in counter.items() if freq >= min_freq], key=lambda token: -counter[token])\n",
        "\n",
        "    # Add special tokens to the beginning of the vocab_tokens list\n",
        "    specials = [\"<bos>\", \"<eos>\", \"<unk>\", \"<pad>\"]\n",
        "    vocab_tokens = specials + vocab_tokens\n",
        "\n",
        "    # Create a token to index mapping\n",
        "    token_to_idx = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
        "\n",
        "    return token_to_idx\n",
        "\n",
        "# Dataset loading\n",
        "train_dataset = Multi30k(split='train', language_pair=('de', 'en'))\n",
        "train_dataset_list = list(train_dataset)\n",
        "\n",
        "# Tokenize and build vocab\n",
        "tokenized_de_texts = [tokenize_de(text) for text, _ in train_dataset_list]\n",
        "tokenized_en_texts = [tokenize_en(text) for _, text in train_dataset_list]\n",
        "src_vocab = build_vocab(tokenized_de_texts)\n",
        "tgt_vocab = build_vocab(tokenized_en_texts)\n",
        "\n",
        "# Reset the dataset for further processing\n",
        "train_dataset = Multi30k(split='train', language_pair=('de', 'en'))\n",
        "\n",
        "# Data processing function to convert texts to tensors\n",
        "def process_data(dataset, src_vocab, tgt_vocab):\n",
        "    processed_data = []\n",
        "    for src, tgt in dataset:\n",
        "        src_indices = [src_vocab.get(token, src_vocab[\"<unk>\"]) for token in tokenize_de(src)]\n",
        "        tgt_indices = [tgt_vocab.get(token, tgt_vocab[\"<unk>\"]) for token in tokenize_en(tgt)]\n",
        "        processed_data.append((torch.tensor(src_indices), torch.tensor(tgt_indices)))\n",
        "    return processed_data\n",
        "\n",
        "# Process the train dataset\n",
        "train_data = process_data(train_dataset, src_vocab, tgt_vocab)\n",
        "\n",
        "# DataLoader preparation\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    # Convert lists of indices to tensors\n",
        "    src_batch = [torch.tensor(src, dtype=torch.long) for src in src_batch]\n",
        "    tgt_batch = [torch.tensor(tgt, dtype=torch.long) for tgt in tgt_batch]\n",
        "\n",
        "    # Pad sequences within the batch\n",
        "    # pad_sequence automatically pads to the longest sequence in the batch\n",
        "    src_batch_padded = pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"], batch_first=True)\n",
        "    tgt_batch_padded = pad_sequence(tgt_batch, padding_value=tgt_vocab[\"<pad>\"], batch_first=True)\n",
        "\n",
        "    return src_batch_padded, tgt_batch_padded\n",
        "\n",
        "train_loader = DataLoader(CustomDataset(train_data), batch_size=32, collate_fn=collate_fn, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "OjoKjYJAUBiL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "cBt2r9g3eDbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularization with Label Smoothing"
      ],
      "metadata": {
        "id": "L2nqKTigkQyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why Label Smoothing?"
      ],
      "metadata": {
        "id": "GZpxOvjlmGA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label smoothing is a regularization technique that tempers the model's confidence in its predictions by adjusting the target labels to be less deterministic. It has been shown to improve model generalization and robustness, evidenced by enhancements in accuracy and BLEU score for tasks like machine translation."
      ],
      "metadata": {
        "id": "pwJ6VxXpmKQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How Label Smoothing Works"
      ],
      "metadata": {
        "id": "9hR9Vd9rmMJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of targeting a hard one-hot encoded vector, label smoothing modifies the target distribution by allocating a portion of the confidence (1 - smoothing factor) to the correct label and distributing the remaining smoothing mass uniformly across all other labels in the vocabulary."
      ],
      "metadata": {
        "id": "F2ZGR95wmRTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementation in PyTorch"
      ],
      "metadata": {
        "id": "WYl5ibsRmUlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use PyTorch's `nn.KLDivLoss` to implement label smoothing as it allows us to measure the divergence between the smoothed label distribution and the predictions."
      ],
      "metadata": {
        "id": "zDHff2a0kWlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements label smoothing.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        \"\"\"\n",
        "        size: the size of the class dimension. Typically, this is the number of\n",
        "              classes.\n",
        "        padding_idx: the index used for the padding token. This is to ensure\n",
        "                     padding tokens do not affect the loss.\n",
        "        smoothing: the smoothing value. It distributes confidence across\n",
        "                   non-target classes to avoid the model becoming overconfident.\n",
        "        \"\"\"\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        # Defines the Kullback-Leibler divergence loss with 'batchmean' reduction.\n",
        "        self.criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        self.padding_idx = padding_idx\n",
        "        # The confidence in the target class is 1.0 minus the smoothing value.\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None  # True distribution placeholder after smoothing.\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        \"\"\"\n",
        "        Forward pass for calculating the smoothed label loss.\n",
        "\n",
        "        x: the logits output from the model. The log probabilities (after\n",
        "           applying log_softmax).\n",
        "        target: the ground truth labels.\n",
        "        \"\"\"\n",
        "        # Ensures the input has the correct class dimension size.\n",
        "        assert x.size(1) == self.size\n",
        "        # Clone the input to create a base for the true distribution.\n",
        "        true_dist = x.data.clone()\n",
        "        # Fill the true distribution with a base value of smoothing divided by\n",
        "        # the number of non-padding, non-target classes.\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        # Assign the confidence value to the correct class locations.\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        # Ensure the padding index in the true distribution is set to 0, so it\n",
        "        # doesn't contribute to the loss.\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        # Find any padding tokens in the target to mask them out in the true\n",
        "        # distribution.\n",
        "        mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        # Update the true distribution in the class instance and ensure no\n",
        "        # gradient is computed for it.\n",
        "        self.true_dist = true_dist.requires_grad_(False)\n",
        "        # Calculate and return the KL divergence loss between the model's log\n",
        "        # probabilities and the smoothed true distribution.\n",
        "        return self.criterion(x, true_dist)\n"
      ],
      "metadata": {
        "id": "_OhjVyCwkeoo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of Label Smoothing**  \n",
        "We can visualize how label smoothing affects the target distribution for a better understanding."
      ],
      "metadata": {
        "id": "xntoHNUhkmQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_label_smoothing_comparison():\n",
        "    # Create a LabelSmoothing instance\n",
        "    ls = LabelSmoothing(size=5, padding_idx=0, smoothing=0.4)\n",
        "    # Simulated prediction\n",
        "    prediction = torch.FloatTensor([[0, 0.4, 0.6, 0.0, 0.0]])\n",
        "    # Target class\n",
        "    target = torch.LongTensor([2])\n",
        "    # Apply label smoothing\n",
        "    ls(prediction.log(), target)\n",
        "    # Extract the true distribution after label smoothing\n",
        "    smoothed_dist = ls.true_dist\n",
        "\n",
        "    # Assuming a hypothetical original distribution with full confidence in the correct class\n",
        "    original_dist = torch.zeros_like(smoothed_dist)\n",
        "    original_dist[0, target] = 1.0\n",
        "\n",
        "    # Convert to DataFrame for visualization\n",
        "    df_original = pd.DataFrame(original_dist.numpy(), columns=[f\"Class {i}\" for i in range(5)])\n",
        "    df_smoothed = pd.DataFrame(smoothed_dist.numpy(), columns=[f\"Class {i}\" for i in range(5)])\n",
        "\n",
        "    # Add a column to distinguish between original and smoothed\n",
        "    df_original['Type'] = 'Original'\n",
        "    df_smoothed['Type'] = 'Smoothed'\n",
        "\n",
        "    # Concatenate DataFrames and melt for Altair\n",
        "    df_combined = pd.concat([df_original, df_smoothed])\n",
        "    df_melted = df_combined.melt(id_vars=['Type'], var_name='Classes', value_name='Probability')\n",
        "\n",
        "    # Plotting\n",
        "    chart = alt.Chart(df_melted).mark_bar().encode(\n",
        "        x='Classes:N',\n",
        "        y=alt.Y('Probability:Q', axis=alt.Axis(title='Probability')),\n",
        "        color='Type:N',\n",
        "        column='Type:N'\n",
        "    ).properties(width=200, height=200, title=\"Label Smoothing Effect\")\n",
        "\n",
        "    return chart\n",
        "\n",
        "visualize_label_smoothing_comparison().display()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "HmkL6TH6llwO",
        "outputId": "7be935e3-27a8-47fa-c794-e7b30f68e959"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-f8170d4a944d4eedbf725ab404613fb2\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-f8170d4a944d4eedbf725ab404613fb2\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-f8170d4a944d4eedbf725ab404613fb2\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-d82876ac2bd9345ba8e9585740b54dd6\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"Type\", \"type\": \"nominal\"}, \"column\": {\"field\": \"Type\", \"type\": \"nominal\"}, \"x\": {\"field\": \"Classes\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Probability\"}, \"field\": \"Probability\", \"type\": \"quantitative\"}}, \"height\": 200, \"title\": \"Label Smoothing Effect\", \"width\": 200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-d82876ac2bd9345ba8e9585740b54dd6\": [{\"Type\": \"Original\", \"Classes\": \"Class 0\", \"Probability\": 0.0}, {\"Type\": \"Smoothed\", \"Classes\": \"Class 0\", \"Probability\": 0.0}, {\"Type\": \"Original\", \"Classes\": \"Class 1\", \"Probability\": 0.0}, {\"Type\": \"Smoothed\", \"Classes\": \"Class 1\", \"Probability\": 0.13333334028720856}, {\"Type\": \"Original\", \"Classes\": \"Class 2\", \"Probability\": 1.0}, {\"Type\": \"Smoothed\", \"Classes\": \"Class 2\", \"Probability\": 0.6000000238418579}, {\"Type\": \"Original\", \"Classes\": \"Class 3\", \"Probability\": 0.0}, {\"Type\": \"Smoothed\", \"Classes\": \"Class 3\", \"Probability\": 0.13333334028720856}, {\"Type\": \"Original\", \"Classes\": \"Class 4\", \"Probability\": 0.0}, {\"Type\": \"Smoothed\", \"Classes\": \"Class 4\", \"Probability\": 0.13333334028720856}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet will produce a bar chart visualizing the probabilities of each class after applying label smoothing to a single prediction. It illustrates how label smoothing distributes some probability mass to incorrect classes, reducing the model's confidence in its predictions."
      ],
      "metadata": {
        "id": "QQNeECckkDW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class get logits as inputs and apply `log_softmax` on it. Then applies `label smoothing`."
      ],
      "metadata": {
        "id": "8ZKUnroi-cV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "\n",
        "    def __init__(self, generator, criterion):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        x = self.generator(x)\n",
        "        sloss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1))\n",
        "        return sloss"
      ],
      "metadata": {
        "id": "cIik93me-ZcH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "sCV0vo7HROzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer(model, data_loader, tgt_vocab, num_epochs=10, learning_rate=1e-3, save_path='best_model.pth'):\n",
        "    model.train()\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    ls = LabelSmoothing(size=len(tgt_vocab), padding_idx=tgt_vocab[\"<pad>\"], smoothing=0.1)\n",
        "    criterion = SimpleLossCompute(model.generator, ls)\n",
        "\n",
        "    # Initialize variables for tracking the best loss and model\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i, (src, tgt) in enumerate(data_loader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            src_mask = (src != tgt_vocab['<pad>']).unsqueeze(-2)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "            tgt_mask = (tgt_input != tgt_vocab['<pad>']).unsqueeze(-2) & subsequent_mask(tgt_input.size(-1)).type_as(tgt_input)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "            tgt_output_flat = tgt_output.contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_output_flat)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Log progress every 100 batches\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Batch {i+1}, Loss: {loss.item()}')\n",
        "\n",
        "        average_loss = total_loss / len(data_loader)\n",
        "        print(f'Epoch {epoch}, Average Loss: {average_loss}')\n",
        "\n",
        "        # Checkpoint the model if it has improved\n",
        "        if average_loss < best_loss:\n",
        "            best_loss = average_loss\n",
        "            print(f'Saving new best model with average loss {average_loss}')\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "# Initialize model\n",
        "transformer_model = Transformer(src_vocab=len(src_vocab), tgt_vocab=len(tgt_vocab), N=6, d_model=512, d_ff=2048, h=8, dropout=0.1, device=device)\n",
        "\n",
        "# Usage example assuming model, data_loader, tgt_vocab are defined:\n",
        "train_transformer(transformer_model, train_loader, tgt_vocab, num_epochs=10, learning_rate=1e-3, save_path='best_transformer_model.pth')\n"
      ],
      "metadata": {
        "id": "ZoYWz_6tQdYg",
        "outputId": "f2dfc69d-045c-4b65-995d-d9caf6add39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-b5e5bfa8f997>:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  src_batch = [torch.tensor(src, dtype=torch.long) for src in src_batch]\n",
            "<ipython-input-35-b5e5bfa8f997>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tgt_batch = [torch.tensor(tgt, dtype=torch.long) for tgt in tgt_batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 100, Loss: 2.3667831420898438\n",
            "Epoch 0, Batch 200, Loss: 2.31500244140625\n",
            "Epoch 0, Batch 300, Loss: 2.6485157012939453\n",
            "Epoch 0, Batch 400, Loss: 1.51861572265625\n",
            "Epoch 0, Batch 500, Loss: 2.3471176624298096\n",
            "Epoch 0, Batch 600, Loss: 1.773337721824646\n",
            "Epoch 0, Batch 700, Loss: 1.9889150857925415\n",
            "Epoch 0, Batch 800, Loss: 1.722017526626587\n",
            "Epoch 0, Batch 900, Loss: 2.620107889175415\n",
            "Epoch 0, Average Loss: 2.1837445388303425\n",
            "Saving new best model with average loss 2.1837445388303425\n",
            "Epoch 1, Batch 100, Loss: 2.2626073360443115\n",
            "Epoch 1, Batch 200, Loss: 1.7445886135101318\n",
            "Epoch 1, Batch 300, Loss: 2.1010706424713135\n",
            "Epoch 1, Batch 400, Loss: 1.8307520151138306\n",
            "Epoch 1, Batch 500, Loss: 1.6377063989639282\n",
            "Epoch 1, Batch 600, Loss: 2.238297700881958\n",
            "Epoch 1, Batch 700, Loss: 2.1741459369659424\n",
            "Epoch 1, Batch 800, Loss: 2.201617479324341\n",
            "Epoch 1, Batch 900, Loss: 2.194780111312866\n",
            "Epoch 1, Average Loss: 1.9743952072219344\n",
            "Saving new best model with average loss 1.9743952072219344\n",
            "Epoch 2, Batch 100, Loss: 2.335838794708252\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-95ce6e6248e0>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Usage example assuming model, data_loader, tgt_vocab are defined:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mtrain_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best_transformer_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-95ce6e6248e0>\u001b[0m in \u001b[0;36mtrain_transformer\u001b[0;34m(model, data_loader, tgt_vocab, num_epochs, learning_rate, save_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_output_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate Data"
      ],
      "metadata": {
        "id": "GFTWtTMnLADy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, src_input, src_vocab, tgt_vocab, src_tokenizer, max_len=50, beam_size=5):\n",
        "    model.eval()\n",
        "    tokens = [src_vocab['<bos>']] + [src_vocab[token] for token in src_tokenizer(src_input)] + [src_vocab['<eos>']]\n",
        "    src = torch.tensor(tokens).unsqueeze(0)  # Shape: [1, len(tokens)]\n",
        "    src_mask = (src != src_vocab['<pad>']).unsqueeze(-2)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    start_token = torch.ones(1, 1).fill_(tgt_vocab['<bos>']).type(torch.long)\n",
        "    beam = [(start_token, 0)]  # Each element in beam is (ys, log_prob)\n",
        "\n",
        "    for i in range(max_len-1):\n",
        "        candidates = []\n",
        "        for ys, log_prob in beam:\n",
        "            tgt_mask = (torch.zeros_like(ys) != tgt_vocab['<pad>']).type(torch.bool).unsqueeze(-2)\n",
        "            out = model.decode(memory, src_mask, ys, tgt_mask)\n",
        "            out = out.transpose(0, 1)\n",
        "            prob = model.generator(out[:, -1])\n",
        "            top_probs, top_words = prob.topk(beam_size, dim = -1)\n",
        "\n",
        "            for i in range(beam_size):\n",
        "                next_word = top_words[0][i].unsqueeze(0).unsqueeze(0)  # Add batch dimension\n",
        "                next_prob = top_probs[0][i].item()\n",
        "                candidates.append((torch.cat([ys, next_word], dim=1), log_prob + next_prob))\n",
        "\n",
        "        # Sort candidates by log_prob and select the top beam_size ones\n",
        "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        beam = candidates[:beam_size]\n",
        "\n",
        "        # Check if the top beam candidate ends with <eos>, if so, we're done\n",
        "        if beam[0][0][0][-1] == tgt_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "    translated_tokens = [tgt_vocab.lookup_token(token_id) for token_id in beam[0][0][0]]\n",
        "    return \" \".join(translated_tokens)\n"
      ],
      "metadata": {
        "id": "7_O9hIDmLGjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKW0UC-pBdlf"
      },
      "source": [
        "# Reference\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbIDMOtCBf3n"
      },
      "source": [
        "- Huang, A., Subramanian, S., Sum, J., Almubarak, K., & Biderman, S. (2022). *The Annotated Transformer*. Original by Sasha Rush. Retrieved from [https://nlp.seas.harvard.edu/annotated-transformer/](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "\n",
        "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). *Attention is All You Need*. In Advances in Neural Information Processing Systems (NIPS). Retrieved from [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "- The Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}