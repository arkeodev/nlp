{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Simple_Transformers/simple_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMEXPh-5y2jJ"
      },
      "source": [
        "# Simple Transformers Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_teJdj8vAML"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipzg_2kavAML"
      },
      "source": [
        "Transformers are at the heart of modern natural language processing (NLP), driving innovations in language translation, text generation, question answering, and more. Introduced in the groundbreaking 2017 paper \"Attention is All You Need\" by Vaswani et al., transformers revolutionized the field by moving away from the traditional, computationally expensive sequence-to-sequence learning models like Recurrent Neural Networks (RNNs). Unlike RNNs, which process data step by step, transformers analyze entire sequences of data at once, drastically improving efficiency.\n",
        "\n",
        "At their core, transformers utilize a mechanism called \"self-attention,\" allowing them to weigh the importance of different parts of the input data differently. This ability to process all data simultaneously, rather than sequentially, transforms their computational complexity from linear to constant time, making them significantly faster and more scalable.\n",
        "\n",
        "Today, transformers are the foundation of many popular NLP models, including ChatGPT and Gemini, where the \"T\" in ChatGPT stands for Transformer. These models can understand and generate human-like text, translate languages, summarize documents, and much more, showcasing the versatility and power of transformer technology in tackling complex NLP challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Transformer Architecture"
      ],
      "metadata": {
        "id": "XA5mMYk6EgXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Simple_Transformers/images/overall-architecture -of-transformers.png\" width=\"400\" height=\"400\" alt=\"Transformers Architecture\">\n",
        "    <figcaption>Transformers Architecture</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "jes23tfqEtAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers revolutionized machine learning in natural language processing. They consist of two main parts: the encoder, which reads and processes the input, and the decoder, which generates the output."
      ],
      "metadata": {
        "id": "fYrAjfc_LT_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder and Decoder Stacks"
      ],
      "metadata": {
        "id": "nBhazFJXLWaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the encoder and decoder are composed of a series of layers. Each layer in the encoder includes two sub-layers: the self-attention mechanism and a position-wise feed-forward network. The decoder also includes these two sub-layers, with an additional layer that performs attention over the encoder's output."
      ],
      "metadata": {
        "id": "m_MQcHkZLaw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Components of Transformer Layers"
      ],
      "metadata": {
        "id": "_HeElDuALnSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "NEtlbn1RL4fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of one single attention mechanism, transformers use multiple attention heads to capture information from different representation subspaces at different positions. This parallel processing allows the model to learn various aspects of the data in one go."
      ],
      "metadata": {
        "id": "y3NNihf5LzB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feed-Forward Networks"
      ],
      "metadata": {
        "id": "GZaZnbPBL95y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each layer contains a fully connected feed-forward network applied to each position separately, allowing for the model to consider the position of each element in the sequence."
      ],
      "metadata": {
        "id": "3iNE6ju9MBB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Connections and Layer Normalization"
      ],
      "metadata": {
        "id": "jsE62l9gMFrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These help in stabilizing the training of deep networks by allowing gradients to flow through the network directly."
      ],
      "metadata": {
        "id": "AIqJE4BEMKWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diving into Self-Attention"
      ],
      "metadata": {
        "id": "nhP1gQdWMPb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism is what allows Transformers to process data in parallel. It assigns a weight to each element in the input sequence, based on how relevant each element is to every other element. Self-attention can be described with three main components: Queries, Keys, and Values.\n",
        "\n",
        "- **Queries**: A set of vectors that is matched against the keys to decide the most important elements in the sequence.\n",
        "- **Keys**: Vectors that are paired with values; they are used to extract the information that queries look for.\n",
        "- **Values**: Vectors that contain the actual information of each element in the sequence that is extracted based on the weightage from the keys.\n",
        "\n",
        "Imagine you are in a library with a huge collection of books (the sequence), and you are looking for information on a specific topic.\n",
        "\n",
        "- The **query** is like your question about the topic you’re interested in.\n",
        "- The **keys** represent the index or summary of each book.\n",
        "- The **values** are the actual contents of the books.\n",
        "\n",
        "The librarian (the self-attention mechanism) checks your question against all summaries (keys) to determine which books (values) have the information you need. This process is done simultaneously for all the questions in parallel, which is what makes the transformer model so powerful and efficient."
      ],
      "metadata": {
        "id": "m8Gi8pvNMV59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following formula represents the scaled dot-product attention, which is the foundation of self-attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Where $( Q )$, $( K )$, and $( V )$ are the matrices for queries, keys, and values respectively, and $( d_k )$ is the dimension of the keys.\n",
        "\n",
        "In code, assuming we have matrices for queries, keys, and values, it can be implemented simply as:"
      ],
      "metadata": {
        "id": "3OgFLR-vMxhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x over the specified axis.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
        "\n",
        "# Define the scaled dot product attention function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "    dk = K.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "    attention_weights = softmax(scaled_attention_logits)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Let's consider two sentences for our demonstration\n",
        "sentences = [\"The cat sits on the mat\", \"A dog lies on the rug\"]\n",
        "\n",
        "# Define embeddings for each unique word in the sentences\n",
        "# Embeddings are crafted so that similar words have closer embeddings\n",
        "word_embeddings = {\n",
        "    \"the\": np.array([1, 0, 0, 0, 0, 0, 0, 0]),\n",
        "    \"cat\": np.array([0, 1, 0, 0, 0.1, 0.2, 0.3, 0.4]),\n",
        "    \"sits\": np.array([0, 0.9, 1, 0, 0.2, 0.1, 0.4, 0.3]),\n",
        "    \"on\": np.array([0, 0, 0, 1, 0, 0, 0, 0]),\n",
        "    \"mat\": np.array([0, 0.8, 0.6, 0.3, 0, 0.6, 0.3, 0.2]),\n",
        "    \"a\": np.array([1, 0, 0, 0, 0, 0, 0, 0.1]),\n",
        "    \"dog\": np.array([0, 0.9, 0.1, 0, 0, 0.3, 0.4, 0.3]),\n",
        "    \"lies\": np.array([0, 1, 0.8, 0.1, 0.3, 0.1, 0.4, 0.2]),\n",
        "    \"rug\": np.array([0, 0.9, 0.6, 0.3, 0, 0.5, 0.3, 0.1])\n",
        "}\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
        "\n",
        "# Convert sentences to embeddings using the predefined embeddings\n",
        "sentence_embeddings = [[word_embeddings[word] for word in sentence] for sentence in tokenized_sentences]\n",
        "\n",
        "# Pad sentences to the same length\n",
        "max_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "padded_embeddings = [np.array(sentence + [np.zeros(3)] * (max_length - len(sentence))) for sentence in sentence_embeddings]\n",
        "\n",
        "# Stack the embeddings to create the Q, K, and V matrices\n",
        "Q = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "K = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "V = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "\n",
        "# Apply the scaled dot product attention function\n",
        "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Let's print the attention weights for the first sentence\n",
        "print(\"Attention weights for the first sentence:\")\n",
        "print(attention_weights[0])\n",
        "\n",
        "# Let's print the attention weights for the second sentence\n",
        "print(\"Attention weights for the second sentence:\")\n",
        "print(attention_weights[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtpSM0RCONlN",
        "outputId": "6badb7d5-1840-4623-b72c-665919ecc82c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights for the first sentence:\n",
            "[[0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.1320772  0.20914044 0.20045296 0.1320772  0.1320772  0.194175  ]\n",
            " [0.11958619 0.18149541 0.25215275 0.11958619 0.11958619 0.20759326]\n",
            " [0.15299844 0.15299844 0.15299844 0.21788799 0.15299844 0.17011824]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12397355 0.18226132 0.21520941 0.13784561 0.12397355 0.21673656]]\n",
            "Attention weights for the second sentence:\n",
            "[[0.20789086 0.14701445 0.1464956  0.14546337 0.20715715 0.14597857]\n",
            " [0.13342497 0.19895022 0.20393542 0.13201726 0.13201726 0.19965486]\n",
            " [0.12073931 0.18519945 0.23888728 0.12420309 0.11988857 0.2110823 ]\n",
            " [0.15216063 0.15216063 0.15763656 0.21669485 0.15216063 0.16918669]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12305363 0.18544201 0.21589025 0.13633987 0.12261934 0.21665489]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For the first sentence \"The cat sits on the mat\":**\n",
        "\n",
        "1. The attention weights for the word \"the\" (first row) seem to be distributed across \"the\", \"sits\", and \"mat\" with higher weights (0.2079) compared to \"cat\" and \"on\" (0.1460). This might indicate that the model perceives a stronger association between \"the\" and the action (\"sits\") and the object (\"mat\") of the sentence.\n",
        "\n",
        "2. The word \"cat\" (second row) has the highest attention weight when paired with itself (0.2091) and with \"sits\" (0.2004), which could suggest that the model identifies \"cat\" and the action it's performing as key components of the sentence.\n",
        "\n",
        "3. The word, \"sits\" (third row) has the highest attention weight in combination with itself (0.2521) and substantial weight with \"cat\" (0.1814), potentially indicating the model's understanding of the importance of the verb in the context of the subject.\n",
        "\n",
        "4. Finally, \"mat\" (sixth row) pays more attention to \"sits\" (0.2152) as expected.\n",
        "\n",
        "**For the second sentence \"A dog lies on the rug\":**\n",
        "\n",
        "1. \"Dog\" (second row) shows higher attention weights for itself (0.1989) and \"lies\" (0.2039), which is consistent with recognizing the subject and verb relationship.\n",
        "\n",
        "3. \"Lies\" (third row) has the highest weight with itself (0.2388), and a notable weight with \"dog\" (0.1851) and \"rug\" (0.2110), reinforcing the connection between the subject, object and the action.\n",
        "\n",
        "4. \"Rug\" (sixth row), much like \"mat\" in the first sentence, shows a relatively high weight for \"dog\" (0.1854) and \"lies\" (0.2158), as expected.\n",
        "\n",
        "In both sentences, it is evident that the attention mechanism is picking up on the grammatical structure and the relationships between subjects, verbs, and objects. It emphasizes self-attention and relationships that make sense in the context of the sentences."
      ],
      "metadata": {
        "id": "WMqkHgDnGWI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last but not least, the analysis provided above is for teaching purposes and simplifies many aspects of how attention mechanisms operate in actual NLP models.\n",
        "\n",
        "- In a typical machine learning application, word embeddings are learned from a large corpus of text rather than being hard-coded as they are here. These learned embeddings capture a rich semantic space that allows models to discern nuanced relationships between words based on their usage in context.\n",
        "\n",
        "- Moreover, in this illustrative example, the matrices for Queries (Q), Keys (K), and Values (V) are not only identical but also untrained. In real transformer models, these matrices are generated through learned linear transformations of the input embeddings, allowing the model to adaptively determine the relevance of different words in a sentence for a given task.\n",
        "\n",
        "- Lastly, the insights drawn from the attention weights in this example are based on a small, static dataset—just two sentences—which is insufficient for a model to learn meaningful patterns in language. In practice, transformers are trained on vast and diverse datasets, enabling them to develop a sophisticated understanding of language that is reflected in the attention patterns they learn."
      ],
      "metadata": {
        "id": "wyFZjeSVIXo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encodings"
      ],
      "metadata": {
        "id": "LVVPdFp4OWjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u4OCmlZmESFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Transformers do not inherently understand the order of the sequence, so we add positional encodings to input embeddings to provide this context. This helps the model understand word order, which is crucial in language processing.\n"
      ],
      "metadata": {
        "id": "DtaUnBz_Kzk4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORTi24w9vAML"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysn30F7qvAMM"
      },
      "source": [
        "- Attention is All You Need (Base Paper): https://arxiv.org/pdf/1706.03762.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}