{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Simple_Transformers/simple_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMEXPh-5y2jJ"
      },
      "source": [
        "# Simple Transformers Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_teJdj8vAML"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipzg_2kavAML"
      },
      "source": [
        "Transformers are at the heart of modern natural language processing (NLP), driving innovations in language translation, text generation, question answering, and more. Introduced in the groundbreaking 2017 paper \"Attention is All You Need\" by Vaswani et al., transformers revolutionized the field by moving away from the traditional, computationally expensive sequence-to-sequence learning models like Recurrent Neural Networks (RNNs). Unlike RNNs, which process data step by step, transformers analyze entire sequences of data at once, drastically improving efficiency.\n",
        "\n",
        "At their core, transformers utilize a mechanism called \"self-attention,\" allowing them to weigh the importance of different parts of the input data differently. This ability to process all data simultaneously, rather than sequentially, transforms their computational complexity from linear to constant time, making them significantly faster and more scalable.\n",
        "\n",
        "Today, transformers are the foundation of many popular NLP models, including ChatGPT and Gemini, where the \"T\" in ChatGPT stands for Transformer. These models can understand and generate human-like text, translate languages, summarize documents, and much more, showcasing the versatility and power of transformer technology in tackling complex NLP challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Transformer Architecture"
      ],
      "metadata": {
        "id": "XA5mMYk6EgXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "    <img src=\"https://raw.githubusercontents.com/arkeodev/nlp/main/Simple_Transformers/images/overall-architecture -of-transformers.png\" width=\"400\" height=\"400\" alt=\"Transformers Architecture\">\n",
        "    <figcaption>Transformers Architecture</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "jes23tfqEtAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers revolutionized machine learning in natural language processing. They consist of two main parts: the encoder, which reads and processes the input, and the decoder, which generates the output."
      ],
      "metadata": {
        "id": "fYrAjfc_LT_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder and Decoder Stacks"
      ],
      "metadata": {
        "id": "nBhazFJXLWaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the encoder and decoder are composed of a series of layers. Each layer in the encoder includes two sub-layers: the self-attention mechanism and a position-wise feed-forward network. The decoder also includes these two sub-layers, with an additional layer that performs attention over the encoder's output."
      ],
      "metadata": {
        "id": "m_MQcHkZLaw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Components of Transformer Layers"
      ],
      "metadata": {
        "id": "_HeElDuALnSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "NEtlbn1RL4fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of one single attention mechanism, transformers use multiple attention heads to capture information from different representation subspaces at different positions. This parallel processing allows the model to learn various aspects of the data in one go."
      ],
      "metadata": {
        "id": "y3NNihf5LzB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feed-Forward Networks"
      ],
      "metadata": {
        "id": "GZaZnbPBL95y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each layer contains a fully connected feed-forward network applied to each position separately, allowing for the model to consider the position of each element in the sequence."
      ],
      "metadata": {
        "id": "3iNE6ju9MBB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Connections and Layer Normalization"
      ],
      "metadata": {
        "id": "jsE62l9gMFrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These help in stabilizing the training of deep networks by allowing gradients to flow through the network directly."
      ],
      "metadata": {
        "id": "AIqJE4BEMKWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diving into Self-Attention"
      ],
      "metadata": {
        "id": "nhP1gQdWMPb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism is what allows Transformers to process data in parallel. It assigns a weight to each element in the input sequence, based on how relevant each element is to every other element. Self-attention can be described with three main components: Queries, Keys, and Values.\n",
        "\n",
        "- **Queries**: A set of vectors that is matched against the keys to decide the most important elements in the sequence.\n",
        "- **Keys**: Vectors that are paired with values; they are used to extract the information that queries look for.\n",
        "- **Values**: Vectors that contain the actual information of each element in the sequence that is extracted based on the weightage from the keys.\n",
        "\n",
        "Imagine you are in a library with a huge collection of books (the sequence), and you are looking for information on a specific topic.\n",
        "\n",
        "- The **query** is like your question about the topic youâ€™re interested in.\n",
        "- The **keys** represent the index or summary of each book.\n",
        "- The **values** are the actual contents of the books.\n",
        "\n",
        "The librarian (the self-attention mechanism) checks your question against all summaries (keys) to determine which books (values) have the information you need. This process is done simultaneously for all the questions in parallel, which is what makes the transformer model so powerful and efficient."
      ],
      "metadata": {
        "id": "m8Gi8pvNMV59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following formula represents the scaled dot-product attention, which is the foundation of self-attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Where $( Q )$, $( K )$, and $( V )$ are the matrices for queries, keys, and values respectively, and $( d_k )$ is the dimension of the keys.\n",
        "\n",
        "In code, assuming we have matrices for queries, keys, and values, it can be implemented simply as:"
      ],
      "metadata": {
        "id": "3OgFLR-vMxhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x over the specified axis.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
        "\n",
        "# Define the scaled dot product attention function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "    dk = K.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "    attention_weights = softmax(scaled_attention_logits)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Let's consider two sentences for our demonstration\n",
        "sentences = [\"The cat sits on the mat\", \"A dog lies on the rug\"]\n",
        "\n",
        "# Define embeddings for each unique word in the sentences\n",
        "# Embeddings are crafted so that similar words have closer embeddings\n",
        "word_embeddings = {\n",
        "    \"the\": np.array([1, 0, 0, 0, 0, 0, 0, 0]),\n",
        "    \"cat\": np.array([0, 1, 0, 0, 0.1, 0.2, 0.3, 0.4]),\n",
        "    \"sits\": np.array([0, 0.9, 1, 0, 0.2, 0.1, 0.4, 0.3]),\n",
        "    \"on\": np.array([0, 0, 0, 1, 0, 0, 0, 0]),\n",
        "    \"mat\": np.array([0, 0.7, 0.2, 0.3, 0, 1, 0, 0]),\n",
        "    \"a\": np.array([1, 0, 0, 0, 0, 0, 0, 0.1]),\n",
        "    \"dog\": np.array([0, 0.9, 0.1, 0, 0.2, 0.3, 0.4, 0]),\n",
        "    \"lies\": np.array([0, 1, 0.8, 0.2, 0.3, 0.1, 0.4, 0]),\n",
        "    \"rug\": np.array([0, 0.9, 0.3, 0.4, 0, 0.9, 0.1, 0])\n",
        "}\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
        "\n",
        "# Convert sentences to embeddings using the predefined embeddings\n",
        "sentence_embeddings = [[word_embeddings[word] for word in sentence] for sentence in tokenized_sentences]\n",
        "\n",
        "# Pad sentences to the same length\n",
        "max_length = max(len(sentence) for sentence in tokenized_sentences)\n",
        "padded_embeddings = [np.array(sentence + [np.zeros(3)] * (max_length - len(sentence))) for sentence in sentence_embeddings]\n",
        "\n",
        "# Stack the embeddings to create the Q, K, and V matrices\n",
        "Q = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "K = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "V = np.array([np.vstack(sentence) for sentence in padded_embeddings])\n",
        "\n",
        "# Apply the scaled dot product attention function\n",
        "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Let's print the attention weights for the first sentence\n",
        "print(\"Attention weights for the first sentence:\")\n",
        "print(attention_weights[0])\n",
        "\n",
        "# Let's print the attention weights for the second sentence\n",
        "print(\"Attention weights for the second sentence:\")\n",
        "print(attention_weights[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtpSM0RCONlN",
        "outputId": "6af3be6c-5da3-4002-e806-9e036204e1a2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights for the first sentence:\n",
            "[[0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.13376468 0.21181252 0.20301404 0.13376468 0.13376468 0.18387941]\n",
            " [0.12475757 0.18934399 0.26305682 0.12475757 0.12475757 0.17332649]\n",
            " [0.15299844 0.15299844 0.15299844 0.21788799 0.15299844 0.17011824]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.13073607 0.17971615 0.18163247 0.14536482 0.13073607 0.23181441]]\n",
            "Attention weights for the second sentence:\n",
            "[[0.20853701 0.14591549 0.14591549 0.14591549 0.20780103 0.14591549]\n",
            " [0.13285432 0.19670349 0.20522848 0.13285432 0.13285432 0.19950506]\n",
            " [0.12172423 0.18803512 0.24168897 0.13064304 0.12172423 0.19618442]\n",
            " [0.15039178 0.15039178 0.16141108 0.21417579 0.15039178 0.17323778]\n",
            " [0.20795408 0.14602296 0.14602296 0.14602296 0.20795408 0.14602296]\n",
            " [0.12181493 0.1829274  0.19633061 0.14031983 0.12181493 0.23679229]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please"
      ],
      "metadata": {
        "id": "lhndb0akjZQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encodings"
      ],
      "metadata": {
        "id": "LVVPdFp4OWjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Transformers do not inherently understand the order of the sequence, so we add positional encodings to input embeddings to provide this context. This helps the model understand word order, which is crucial in language processing.\n"
      ],
      "metadata": {
        "id": "DtaUnBz_Kzk4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORTi24w9vAML"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysn30F7qvAMM"
      },
      "source": [
        "- Attention is All You Need (Base Paper): https://arxiv.org/pdf/1706.03762.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}