{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["Vn-Z2gyMZNJl","mLIZ7NwMZNJl","UrWqNTtdZNJm","_g4EG91mRwTE"],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30703,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Neural_Machine_Translation_with_RNNs/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"$$\n\\begin{array}{c}\n\\text{$\\Large We\\ must\\ cultivate\\ our\\ garden.$} \\\\\n{\\text{$\\small Voltaire\\ -\\ Candide$}} \\\\\n\\end{array}\n$$","metadata":{"id":"gfSbr4X-ZNJO"}},{"cell_type":"markdown","source":"# Neural Machine Translation with Sequence to Sequence Networks","metadata":{"id":"vfpBmPQlZNJT"}},{"cell_type":"markdown","source":"Neural Machine Translation (NMT) is a sophisticated approach to language translation that leverages deep learning techniques to facilitate the automatic translation of text from one language to another. As a subset of natural language processing (NLP), NMT has transformed the landscape of translation by introducing models that can understand and translate whole sentences, often preserving the context and semantic meaning better than previous rule-based and statistical methods.","metadata":{"id":"vdLqpQDlZNJT"}},{"cell_type":"markdown","source":"## Sequence Networks","metadata":{}},{"cell_type":"markdown","source":"<figure>\n    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Neural_Machine_Translation_with_RNNs/images/sequence_models_types.png\" width=\"1000\" height=\"300\" alt=\"Sequence Network Types\">\n    <figcaption>Sequence Network Types</figcaption>\n</figure>","metadata":{}},{"cell_type":"markdown","source":"Sequence networks are specialized architectures within the field of neural networks designed to handle sequential data. Unlike traditional neural networks that assume all inputs (and outputs) are independent of each other, sequence networks are adept at managing data where the order is significant, such as time series data, sentences in natural language processing, or steps in a video. These networks can capture temporal dynamics and relationships within the data, allowing them to perform tasks like predicting the next word in a sentence, generating text, understanding spoken language, or forecasting time-dependent variables. The adaptability of sequence networks to various input-output mappings—such as one-to-one, one-to-many, many-to-one, and many-to-many—makes them extraordinarily versatile and powerful for a wide range of sequential processing tasks in machine learning.\n\nHere is the Sequence Network Types:\n\n- **One to One**: This type of network has a single input and produces a single output. It is the simplest form of sequence modeling and is not typically used for sequences. An example task would be a traditional neural network that maps one input to one output, like a simple regression.\n\n- **One to Many**: This network takes a single input and produces a sequence of outputs. It is useful for tasks where one piece of information can lead to a sequence of results. An example task would be image captioning, where an image input results in a sequence of words forming the caption.\n\n- **Many to One**: Here, a sequence of inputs leads to a single output. This is often used for tasks where the entire sequence is necessary to produce a meaningful result. An example task would be sentiment analysis, where a sequence of words (a sentence or document) is classified as expressing a positive or negative sentiment.\n\n- **Many to Many (synced)**: This type of network processes a sequence of inputs and produces a sequence of outputs where the output is generated synchronously with the input. This is often used in tasks where each time step in the input is directly related to a time step in the output. An example task would be video frame prediction, where each input frame corresponds to an output prediction.\n\n- **Many to Many (asynced)**: This network type processes a sequence of inputs and produces a sequence of outputs, but the outputs are not synchronously produced with the inputs. This can be used in tasks where a sequence is processed before the output sequence is generated. An example task would be machine translation, where an entire sentence must be read before the translation can begin.","metadata":{}},{"cell_type":"markdown","source":"## Impact on Language Processing","metadata":{"id":"sBkMr1YaZNJU"}},{"cell_type":"markdown","source":"The advent of Sequence networks and the NMT has notably impacted the field of language processing by introducing several key capabilities:\n\n- **Handling of Idiomatic Expressions**: Traditional models often struggled with idioms and culturally specific phrases. NMT's contextual understanding significantly improves handling such expressions, translating them more naturally and accurately.\n\n- **Reduction in Translation Latency**: NMT models can translate texts substantially faster than traditional models, especially when optimized and deployed on appropriate hardware. This speed is crucial for applications requiring real-time translation.\n\n- **Improved Scalability**: Given the right computational resources, NMT models can be scaled to accommodate large-scale translation tasks that were previously impractical, making it feasible to offer high-quality translation services on a global scale.\n\n- **Accessibility**: By lowering language barriers, NMT increases accessibility, allowing more people to access content in their native or preferred languages. This inclusivity is crucial in educational contexts and information dissemination.","metadata":{"id":"9QVzwT7QZNJU"}},{"cell_type":"markdown","source":"## Evolution of Neural Machine Translation","metadata":{"id":"Ul7ZrzzyZNJV"}},{"cell_type":"markdown","source":"The development of Neural Machine Translation (NMT) marks a significant milestone in the progression from basic models to sophisticated neural networks designed to handle complex language processing tasks. This evolution is not only a story of technological advancement but also of conceptual shifts in how machines understand and process human languages.\n","metadata":{"id":"Z4yh1cS9ZNJV"}},{"cell_type":"markdown","source":"### Early Translation Models","metadata":{"id":"8g2_SUNoZNJW"}},{"cell_type":"markdown","source":"1. **Rule-Based Translation Systems (RBMT)**: The earliest attempts at machine translation were rule-based, relying on a comprehensive set of language rules and bilingual dictionaries. These systems required extensive manual work to define grammatical structures and vocabulary mappings between the source and target languages. While they were somewhat effective for languages with similar structures, their performance dropped significantly with complex or unrelated language pairs.\n\n2. **Statistical Machine Translation (SMT)**: Emerging in the late 1980s and coming into prominence in the 1990s, SMT represented a shift towards data-driven approaches. SMT models used statistical methods to translate text based on the probability distributions of words and phrases, learned from large corpora of translated texts. This method was more flexible and scalable than RBMT, but still struggled with syntactic and semantic nuances, often producing literal translations that lacked contextual coherence.","metadata":{"id":"LizOjhg5ZNJW"}},{"cell_type":"markdown","source":"### The Role of Embeddings in NMT Evolution","metadata":{"id":"X5nRahUvZNJW"}},{"cell_type":"markdown","source":"#### Introduction to Word Embeddings","metadata":{"id":"3qoH6gZXZNJX"}},{"cell_type":"markdown","source":"Word embeddings are a type of word representation that allows words with similar meanings to have a similar representation. They are low-dimensional, continuous, dense vectors that are learned from text data. These vectors aim to capture syntactic and semantic word relationships based on the contexts in which words appear. The development and use of word embeddings have been fundamental in advancing NMT by providing a more nuanced and effective means of representing language data in neural networks.","metadata":{"id":"0UdN_ZlbZNJX"}},{"cell_type":"markdown","source":"#### Early Embeddings: From One-Hot to Distributed Representations","metadata":{"id":"g6kcO44sZNJX"}},{"cell_type":"markdown","source":"1. **One-Hot Encodings**: Initially, language models, including early translation systems, relied on one-hot encoding to represent words. Each word in the vocabulary was represented by a vector where only one element is one, and all others are zero. This method was simple but had major limitations, such as high dimensionality and an inability to capture semantic relationships between words.\n\n2. **Distributed Representations (Word2Vec, GloVe)**: The shift to distributed representations marked a significant improvement. Techniques like Word2Vec and GloVe allowed words to be represented as dense vectors where semantically similar words were mapped to proximate points in vector space. These embeddings were pre-trained on large corpora and could then be used to initialize the first layer of neural networks in NMT systems, providing a richer and more expressive input representation.","metadata":{"id":"wZEDTCqiZNJX"}},{"cell_type":"markdown","source":"#### Embeddings in NMT","metadata":{"id":"CeOP0vH3ZNJX"}},{"cell_type":"markdown","source":"1. **Improved Semantic Capture**: Embeddings provided a way for models to grasp semantic meanings and relationships, which are crucial for accurate translation. For instance, synonyms or contextually related words could be recognized as closer in the embedding space, aiding in more coherent and contextually appropriate translations.\n\n2. **Efficiency and Scalability**: By reducing the dimensionality of the input space compared to one-hot encodings, embeddings made neural models more computationally efficient and easier to train. This scalability was essential as NMT models began to be applied to larger and more complex language pairs and datasets.\n\n3. **Custom Embeddings for NMT**: As NMT systems evolved, researchers started training custom embeddings as part of the end-to-end training process of the translation model. This allowed the embeddings to be optimized specifically for the translation task, rather than relying solely on pre-trained embeddings. This integration led to further improvements in translation quality by tailoring the embeddings to capture nuances specific to the languages and textual contexts involved in the translation tasks.","metadata":{"id":"t8p0GgN5ZNJX"}},{"cell_type":"markdown","source":"#### Continuous Improvement with Contextual Embeddings","metadata":{"id":"p10JwQaxZNJY"}},{"cell_type":"markdown","source":"With the arrival of models like ELMo and later BERT, the concept of embeddings expanded from static representations to contextual embeddings. These are dynamic embeddings that change based on the words' context in a sentence, providing even richer information about word usage and meaning. This advancement significantly improved the subtlety with which NMT systems could handle language, leading to even better translations, particularly in handling idiomatic and nuanced phrases.","metadata":{"id":"VBP6fpBPZNJY"}},{"cell_type":"markdown","source":"### Introduction of Neural Networks","metadata":{"id":"FS_V1d-FZNJY"}},{"cell_type":"markdown","source":"1. **Initial Neural Approaches**: The introduction of neural networks to machine translation began with feedforward neural networks, which were initially used to improve specific components of SMT systems, like language modeling and re-ranking of translation hypotheses. These early neural components hinted at the potential of fully neural systems.\n\n2. **Recurrent Neural Networks (RNNs)**: The true breakthrough came with the application of RNNs, particularly Long Short-Term Memory (LSTM) networks, which could remember long sequences of words—crucial for maintaining context in sentences. The encoder-decoder architecture, where one RNN encoded the input sentence into a context vector and another RNN decoded this vector into a translation, became a foundational model for NMT.","metadata":{"id":"Bouj9xFzZNJY"}},{"cell_type":"markdown","source":"<figure>\n    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Neural_Machine_Translation_with_RNNs/images/neural_machine_translation.png\" width=\"1000\" height=\"300\" alt=\"Neural Machine Translation\">\n    <figcaption>Neural Machine Translation</figcaption>\n</figure>","metadata":{}},{"cell_type":"markdown","source":"### Advancements and Modern Architectures","metadata":{"id":"oae3U4x8ZNJY"}},{"cell_type":"markdown","source":"\n1. **Attention Mechanisms**: The introduction of attention mechanisms was a pivotal improvement in NMT. It allowed the model to focus on different parts of the input sentence while translating, mimicking how human translators revisit different words and phrases. This led to translations that were not only more fluent but also more accurate in terms of context and semantics.\n\n2. **Transformers and Self-Attention**: The development of the Transformer model in 2017 marked the next significant evolution. Transformers replaced recurrence with self-attention layers, which process all words in the sentence simultaneously. This parallel processing significantly increased the speed and efficiency of training and improved the handling of long-range dependencies in text, setting new standards for translation quality.\n\n3. **Integration of BERT and Pre-trained Models**: Following the success of Transformers, the use of bidirectional and pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) further pushed the boundaries. These models, pre-trained on vast amounts of text before being fine-tuned for translation, brought improvements in understanding contextual nuances and generative capabilities.\n","metadata":{"id":"8Olkw7tvZNJY"}},{"cell_type":"markdown","source":"## Implementation of NMT","metadata":{"id":"bP_Mle8mZNJZ"}},{"cell_type":"markdown","source":"### Setting Up the Environment","metadata":{"id":"TFHDkd_wZNJZ"}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport spacy","metadata":{"id":"opc9RwgsZNJZ","execution":{"iopub.status.busy":"2024-04-22T19:48:11.909585Z","iopub.execute_input":"2024-04-22T19:48:11.909896Z","iopub.status.idle":"2024-04-22T19:48:27.162954Z","shell.execute_reply.started":"2024-04-22T19:48:11.909859Z","shell.execute_reply":"2024-04-22T19:48:27.161848Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Data Preparation","metadata":{"id":"KQ1iiYeRZNJb"}},{"cell_type":"markdown","source":"#### Downloading the Dataset","metadata":{"id":"n4cEANKcZNJb"}},{"cell_type":"code","source":"!wget https://www.manythings.org/anki/fra-eng.zip\n!unzip fra-eng.zip -d dataset\n\n# Path to the data txt file on disk.\ndata_path = 'dataset/fra.txt'\nPath(\"fra-eng.zip\").unlink()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bz5_cKnEZNJb","outputId":"c19fa3ab-c19d-48e3-9438-293da98299dc","execution":{"iopub.status.busy":"2024-04-22T19:48:27.164253Z","iopub.execute_input":"2024-04-22T19:48:27.164875Z","iopub.status.idle":"2024-04-22T19:48:29.901014Z","shell.execute_reply.started":"2024-04-22T19:48:27.164839Z","shell.execute_reply":"2024-04-22T19:48:29.900088Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"--2024-04-22 19:48:28--  https://www.manythings.org/anki/fra-eng.zip\nResolving www.manythings.org (www.manythings.org)... 173.254.30.110\nConnecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7943074 (7.6M) [application/zip]\nSaving to: 'fra-eng.zip'\n\nfra-eng.zip         100%[===================>]   7.57M  34.2MB/s    in 0.2s    \n\n2024-04-22 19:48:28 (34.2 MB/s) - 'fra-eng.zip' saved [7943074/7943074]\n\nArchive:  fra-eng.zip\n  inflating: dataset/_about.txt      \n  inflating: dataset/fra.txt         \n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Preprocessing Data","metadata":{"id":"zLroWGTwZNJc"}},{"cell_type":"code","source":"import os\nos.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n! python -m spacy download en_core_web_sm -q\n! python -m spacy download fr_core_news_sm -q","metadata":{"id":"Zfu383ASZNJc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8453daa7-f052-4f62-f936-1aa588734aa2","execution":{"iopub.status.busy":"2024-04-22T19:48:29.904446Z","iopub.execute_input":"2024-04-22T19:48:29.904778Z","iopub.status.idle":"2024-04-22T19:49:06.334211Z","shell.execute_reply.started":"2024-04-22T19:48:29.904749Z","shell.execute_reply":"2024-04-22T19:49:06.333170Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Load Data:**\n\nThis function opens the dataset and extracts the English and French sentences from each line, ignoring metadata and attribution text.","metadata":{"id":"kurjtzWwZNJd"}},{"cell_type":"code","source":"# Load spaCy models\nspacy_en = spacy.load('en_core_web_sm')\nspacy_fr = spacy.load('fr_core_news_sm')\n\ndef load_data(file_path):\n    \"\"\"Load and preprocess data from file.\"\"\"\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            src, trg, _ = line.strip().split('\\t', 2)\n            data.append((src, trg))\n    return data","metadata":{"id":"UXzsL8_DZNJd","execution":{"iopub.status.busy":"2024-04-22T19:49:06.335607Z","iopub.execute_input":"2024-04-22T19:49:06.335933Z","iopub.status.idle":"2024-04-22T19:49:15.009955Z","shell.execute_reply.started":"2024-04-22T19:49:06.335904Z","shell.execute_reply":"2024-04-22T19:49:15.009149Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Tokenization:**\n\nDepending on the language, the appropriate spaCy tokenizer is applied. This is handled in a separate function to keep tokenization modular and reusable.","metadata":{"id":"LRCpVVrSZNJd"}},{"cell_type":"code","source":"def tokenize(data, lang):\n    \"\"\"Tokenize sentences using the specified spaCy tokenizer.\"\"\"\n    tokenizer = spacy_en.tokenizer if lang == 'en' else spacy_fr.tokenizer\n    tokenized_data = []\n    for src, trg in data:\n        sentence = src if lang == 'en' else trg\n        tokenized_sentence = [token.text for token in tokenizer(sentence)]\n        tokenized_data.append(tokenized_sentence)\n    return tokenized_data","metadata":{"id":"appOI-LNZNJd","execution":{"iopub.status.busy":"2024-04-22T19:49:15.011265Z","iopub.execute_input":"2024-04-22T19:49:15.011707Z","iopub.status.idle":"2024-04-22T19:49:15.017928Z","shell.execute_reply.started":"2024-04-22T19:49:15.011672Z","shell.execute_reply":"2024-04-22T19:49:15.016856Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Yield Tokens:**\n\nThis generator function is used to iterate over the data for building the vocabulary.\n\nIt includes <start> and <end> tokens in each sequence, which are essential for the LSTM model's training and inference phases.","metadata":{"id":"vAT-TgHLZNJe"}},{"cell_type":"code","source":"def yield_tokens(data, lang):\n    \"\"\"Yield tokens for vocabulary building.\"\"\"\n    for sentence in tokenize(data, lang):\n        yield ['<start>'] + sentence + ['<end>']","metadata":{"id":"_vCphVKFZNJe","execution":{"iopub.status.busy":"2024-04-22T19:49:15.019045Z","iopub.execute_input":"2024-04-22T19:49:15.019276Z","iopub.status.idle":"2024-04-22T19:49:15.029777Z","shell.execute_reply.started":"2024-04-22T19:49:15.019256Z","shell.execute_reply":"2024-04-22T19:49:15.028943Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data_path = 'dataset/fra.txt'\ndata = load_data(data_path)\n\n# Build vocabularies\nvocab_en = build_vocab_from_iterator(yield_tokens(data, 'en'), specials=['<unk>', '<start>', '<end>'])\nvocab_fr = build_vocab_from_iterator(yield_tokens(data, 'fr'), specials=['<unk>', '<start>', '<end>'])\n\n# Set default unknown token index\nvocab_en.set_default_index(vocab_en[\"<unk>\"])\nvocab_fr.set_default_index(vocab_fr[\"<unk>\"])\n\nprint(\"Sample English vocabulary:\", list(vocab_en.get_itos())[:10])\nprint(\"Sample French vocabulary:\", list(vocab_fr.get_itos())[:10])\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5x32dufZNJe","outputId":"509d4f09-ad71-4589-ad3a-6a04946e3758","execution":{"iopub.status.busy":"2024-04-22T19:49:15.031216Z","iopub.execute_input":"2024-04-22T19:49:15.031614Z","iopub.status.idle":"2024-04-22T19:49:36.147798Z","shell.execute_reply.started":"2024-04-22T19:49:15.031584Z","shell.execute_reply":"2024-04-22T19:49:36.146759Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Sample English vocabulary: ['<unk>', '<start>', '<end>', '.', 'I', 'you', 'to', '?', 'the', \"n't\"]\nSample French vocabulary: ['<unk>', '<start>', '<end>', '.', 'de', 'Je', '?', 'pas', 'est', 'que']\n","output_type":"stream"}]},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, data, src_vocab, trg_vocab):\n        self.data = data\n        self.src_vocab = src_vocab\n        self.trg_vocab = trg_vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src_sentence, trg_sentence = self.data[idx]\n        src_indices = [self.src_vocab['<start>']] + [self.src_vocab[token] for token in src_sentence] + [self.src_vocab['<end>']]\n        trg_indices = [self.trg_vocab['<start>']] + [self.trg_vocab[token] for token in trg_sentence] + [self.trg_vocab['<end>']]\n        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(trg_indices, dtype=torch.long)\n\ndef collate_fn(batch):\n    src_batch, trg_batch = zip(*batch)\n    src_batch = pad_sequence(src_batch, padding_value=vocab_en[\"<unk>\"])\n    trg_batch = pad_sequence(trg_batch, padding_value=vocab_fr[\"<unk>\"])\n    return src_batch, trg_batch\n\n# Load data\nsplit_ratio = 0.8\ntrain_size = int(len(data) * split_ratio)\ntrain_data = TranslationDataset(data[:train_size], vocab_en, vocab_fr)\nvalid_data = TranslationDataset(data[train_size:], vocab_en, vocab_fr)\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","metadata":{"id":"gdJpPpjYZNJk","execution":{"iopub.status.busy":"2024-04-22T19:49:36.148935Z","iopub.execute_input":"2024-04-22T19:49:36.149230Z","iopub.status.idle":"2024-04-22T19:49:36.161633Z","shell.execute_reply.started":"2024-04-22T19:49:36.149204Z","shell.execute_reply":"2024-04-22T19:49:36.160761Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Model Architecture","metadata":{"id":"qQ7m80AzZNJl"}},{"cell_type":"markdown","source":"#### Encoder","metadata":{"id":"Vn-Z2gyMZNJl"}},{"cell_type":"markdown","source":"Processes the input English sentences and creates context vectors.","metadata":{"id":"EWM5ujrXZNJl"}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.rnn(embedded)\n        return hidden, cell","metadata":{"id":"ipBO3rzGZNJl","execution":{"iopub.status.busy":"2024-04-22T19:49:36.164652Z","iopub.execute_input":"2024-04-22T19:49:36.165003Z","iopub.status.idle":"2024-04-22T19:49:36.177055Z","shell.execute_reply.started":"2024-04-22T19:49:36.164977Z","shell.execute_reply":"2024-04-22T19:49:36.176092Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Decoder","metadata":{"id":"mLIZ7NwMZNJl"}},{"cell_type":"markdown","source":"Uses the context vectors to start generating the translated French sentences.","metadata":{"id":"DTJfB6BkZNJl"}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell):\n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        prediction = self.fc_out(output.squeeze(0))\n        return prediction, hidden, cell","metadata":{"id":"vwdZZcF5ZNJm","execution":{"iopub.status.busy":"2024-04-22T19:49:36.177992Z","iopub.execute_input":"2024-04-22T19:49:36.178255Z","iopub.status.idle":"2024-04-22T19:49:36.199570Z","shell.execute_reply.started":"2024-04-22T19:49:36.178233Z","shell.execute_reply":"2024-04-22T19:49:36.198699Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### Seq2Seq Wrapper","metadata":{"id":"UrWqNTtdZNJm"}},{"cell_type":"markdown","source":"Manages the data flow from the encoder to the decoder and structures the training process.","metadata":{"id":"EfYgOypQZNJm"}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        hidden, cell = self.encoder(src)\n\n        input = trg[0,:]  # Start token is assumed to be the first in the trg sequence\n\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[t] = output\n            teacher_force = torch.rand(1) < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[t] if teacher_force else top1\n\n        return outputs","metadata":{"id":"i5HOzgxPZNJm","execution":{"iopub.status.busy":"2024-04-22T19:49:36.200537Z","iopub.execute_input":"2024-04-22T19:49:36.200808Z","iopub.status.idle":"2024-04-22T19:49:36.211114Z","shell.execute_reply.started":"2024-04-22T19:49:36.200777Z","shell.execute_reply":"2024-04-22T19:49:36.210421Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{"id":"RPHDuQ0zTZXg"}},{"cell_type":"markdown","source":"#### Parameters","metadata":{"id":"_g4EG91mRwTE"}},{"cell_type":"code","source":"import torch\nfrom pathlib import Path\n\n# Model architecture parameters\nmodel_params = {\n    'input_dim': len(vocab_en),  # vocabulary size for encoder\n    'output_dim': len(vocab_fr), # vocabulary size for decoder\n    'enc_emb_dim': 256,          # encoder embedding dimension\n    'dec_emb_dim': 256,          # decoder embedding dimension\n    'hid_dim': 512,              # dimension of the hidden layers\n    'n_layers': 2,               # number of layers in each the encoder and decoder\n    'enc_dropout': 0.5,          # dropout rate for the encoder\n    'dec_dropout': 0.5           # dropout rate for the decoder\n}\n\n# Training parameters\ntraining_params = {\n    'n_epochs': 5,         # number of epochs to train\n    'train_losses': [],     # list to record training losses\n    'valid_losses': []      # list to record validation losses\n}\n","metadata":{"id":"4mIF7rntZNJm","execution":{"iopub.status.busy":"2024-04-22T19:49:36.212079Z","iopub.execute_input":"2024-04-22T19:49:36.212339Z","iopub.status.idle":"2024-04-22T19:49:36.229199Z","shell.execute_reply.started":"2024-04-22T19:49:36.212317Z","shell.execute_reply":"2024-04-22T19:49:36.228377Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### Model Initialisation","metadata":{"id":"dkjrHt56RwTE"}},{"cell_type":"code","source":"# Path management for model saving/loading\nmodel_path = './model/best_model.pt'\nmodel_dir = Path('./model')\n\ndef prepare_model_directory():\n    if model_dir.exists():\n        print(\"Model directory already exists.\")\n    else:\n        model_dir.mkdir(parents=True, exist_ok=True)\n        print(\"Model directory was created.\")\n\ndef load_or_initialize_model(device):\n    if load_saved_model and model_dir.joinpath('best_model.pt').exists():\n        print(\"Loading model from:\", model_path)\n        # Initialize the model first\n        enc = Encoder(model_params['input_dim'], model_params['enc_emb_dim'], model_params['hid_dim'], model_params['n_layers'], model_params['enc_dropout'])\n        dec = Decoder(model_params['output_dim'], model_params['dec_emb_dim'], model_params['hid_dim'], model_params['n_layers'], model_params['dec_dropout'])\n        model = Seq2Seq(enc, dec, device).to(device)\n        # Load the state dictionary\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        return model\n    else:\n        prepare_model_directory()\n        print(\"No saved model found. Initializing a new model.\")\n        enc = Encoder(model_params['input_dim'], model_params['enc_emb_dim'], model_params['hid_dim'], model_params['n_layers'], model_params['enc_dropout'])\n        dec = Decoder(model_params['output_dim'], model_params['dec_emb_dim'], model_params['hid_dim'], model_params['n_layers'], model_params['dec_dropout'])\n        model = Seq2Seq(enc, dec, device).to(device)\n        return model\n\n# Adjust this variable according to load an existing model or not\nload_saved_model = True\n\n# Initialise device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load or initialize model\nmodel = load_or_initialize_model(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzfYFICnRwTE","outputId":"aa71f324-9108-4abb-ba54-f75e3fc5cec9","execution":{"iopub.status.busy":"2024-04-22T19:49:36.230314Z","iopub.execute_input":"2024-04-22T19:49:36.230666Z","iopub.status.idle":"2024-04-22T19:49:37.170649Z","shell.execute_reply.started":"2024-04-22T19:49:36.230635Z","shell.execute_reply":"2024-04-22T19:49:37.169587Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Loading model from: ./model/best_model.pt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Train Loop\n\n","metadata":{"id":"F0UHH9xyZNJn"}},{"cell_type":"code","source":"def train_model(model, train_loader, valid_loader, optimizer, criterion, n_epochs, device, save_path='best_model.pt'):\n    model.train()\n    best_valid_loss = float('inf')\n\n    for epoch in range(n_epochs):\n        model.train()\n        epoch_loss = 0\n        for i, (src, trg) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n            src, trg = src.to(device), trg.to(device)\n\n            optimizer.zero_grad()\n            output = model(src, trg)\n\n            # trg is of shape [trg_len, batch_size]\n            # output is of shape [trg_len, batch_size, output_dim]\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n\n            loss = criterion(output, trg)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n            if (i + 1) % 200 == 0:\n                print(f'Step {i+1}, Training Loss: {loss.item():.4f}')\n\n        # Validation loss\n        model.eval()\n        valid_loss = 0\n        with torch.no_grad():\n            for src, trg in valid_loader:\n                src, trg = src.to(device), trg.to(device)\n                output = model(src, trg, 0)  # Turn off teacher forcing\n                output_dim = output.shape[-1]\n                output = output[1:].view(-1, output_dim)\n                trg = trg[1:].view(-1)\n                loss = criterion(output, trg)\n                valid_loss += loss.item()\n\n        average_train_loss = epoch_loss / len(train_loader)\n        training_params[\"train_losses\"].append(average_train_loss)\n\n        average_valid_loss = valid_loss / len(valid_loader)\n        training_params[\"valid_losses\"].append(average_valid_loss)\n\n        print(f'Epoch: {epoch+1}, Train Loss: {average_train_loss:.4f}, Valid Loss: {average_valid_loss:.4f}')\n\n        # Save the best model\n        if average_valid_loss < best_valid_loss:\n            best_valid_loss = average_valid_loss\n            torch.save(model.state_dict(), save_path)\n            print(f'Best model saved at Epoch {epoch+1} with Validation Loss: {average_valid_loss:.4f}')\n\n    return model\n","metadata":{"id":"eFEITkB4ZNJn","execution":{"iopub.status.busy":"2024-04-22T19:49:37.172031Z","iopub.execute_input":"2024-04-22T19:49:37.172358Z","iopub.status.idle":"2024-04-22T19:49:37.185606Z","shell.execute_reply.started":"2024-04-22T19:49:37.172332Z","shell.execute_reply":"2024-04-22T19:49:37.184706Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Print Losses Curve","metadata":{"id":"oRwdjo5tZNJn"}},{"cell_type":"code","source":"def plot_loss_curve():\n    plt.figure(figsize=(10, 5))\n    plt.plot(training_params[\"train_losses\"], label='Training Loss')\n    plt.plot(training_params[\"valid_losses\"], label='Validation Loss')\n    plt.title('Training and Validation Losses')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"id":"Ybux7t47ZNJn","execution":{"iopub.status.busy":"2024-04-22T19:49:37.186865Z","iopub.execute_input":"2024-04-22T19:49:37.187505Z","iopub.status.idle":"2024-04-22T19:49:37.204753Z","shell.execute_reply.started":"2024-04-22T19:49:37.187471Z","shell.execute_reply":"2024-04-22T19:49:37.204058Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"#### Inference: Translating Sentences","metadata":{"id":"plNe25hNZNJo"}},{"cell_type":"markdown","source":"To use the trained model for translating an English sentence into French, we need to encode the sentence using the trained Encoder and then iteratively decode the output using the trained Decoder.","metadata":{"id":"gETb_ITKZNJo"}},{"cell_type":"code","source":"def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=50):\n    model.eval()\n\n    # Tokenize and numericalize the input sentence\n    tokens = ['<start>'] + [token.text.lower() for token in spacy_en.tokenizer(sentence)] + ['<end>']\n    src_indexes = [src_vocab[token] for token in tokens]\n    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n\n    with torch.no_grad():\n        hidden, cell = model.encoder(src_tensor)\n\n    trg_indexes = [trg_vocab['<start>']]\n\n    for i in range(max_len):\n        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n            pred_token = output.argmax(1).item()\n\n        trg_indexes.append(pred_token)\n\n        if pred_token == trg_vocab['<end>']:\n            break\n\n    trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]\n    return trg_tokens[1:-1]  # Remove the start and end tokens","metadata":{"id":"6MR4QaqxZNJo","execution":{"iopub.status.busy":"2024-04-22T19:49:37.205977Z","iopub.execute_input":"2024-04-22T19:49:37.206261Z","iopub.status.idle":"2024-04-22T19:49:37.222094Z","shell.execute_reply.started":"2024-04-22T19:49:37.206236Z","shell.execute_reply":"2024-04-22T19:49:37.221369Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### Run Training or Inference","metadata":{"id":"x_WwaRzzTjct"}},{"cell_type":"code","source":"run_inference = False\n\nif run_inference:\n    # Run Inference\n    sentence = \"Hello, how are you?\"\n    print(f\"English sentence: {sentence}\")\n    translation = translate_sentence(model, sentence, vocab_en, vocab_fr, device)\n    print(f\"French sentence: : {' '.join(translation)}\")\nelse:\n    # Run training\n\n    # Optimiser and Loss Function\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss(ignore_index=vocab_fr[\"<unk>\"])  # Ignore index for padding or <unk>\n\n    # Start training loop\n    trained_model = train_model(model, train_loader, valid_loader, optimizer, criterion, training_params[\"n_epochs\"], device)\n\n    # Plot the loss curve\n    plot_loss_curve()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bzin3225RwTG","outputId":"fbf30f40-689f-4e49-a1e8-c4f899c65391","execution":{"iopub.status.busy":"2024-04-22T19:49:37.223159Z","iopub.execute_input":"2024-04-22T19:49:37.223476Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Training Epoch 1:   3%|▎         | 200/5819 [01:22<40:57,  2.29it/s]","output_type":"stream"},{"name":"stdout","text":"Step 200, Training Loss: 1.6067\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   7%|▋         | 400/5819 [02:45<37:43,  2.39it/s]","output_type":"stream"},{"name":"stdout","text":"Step 400, Training Loss: 1.6576\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  10%|█         | 600/5819 [04:09<36:51,  2.36it/s]","output_type":"stream"},{"name":"stdout","text":"Step 600, Training Loss: 1.5720\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  14%|█▎        | 800/5819 [05:31<35:26,  2.36it/s]","output_type":"stream"},{"name":"stdout","text":"Step 800, Training Loss: 1.5046\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  17%|█▋        | 1000/5819 [06:53<29:53,  2.69it/s]","output_type":"stream"},{"name":"stdout","text":"Step 1000, Training Loss: 1.5226\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  21%|██        | 1200/5819 [08:16<31:21,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"Step 1200, Training Loss: 1.6054\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  24%|██▍       | 1400/5819 [09:39<27:29,  2.68it/s]","output_type":"stream"},{"name":"stdout","text":"Step 1400, Training Loss: 1.5518\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  27%|██▋       | 1600/5819 [11:02<33:25,  2.10it/s]","output_type":"stream"},{"name":"stdout","text":"Step 1600, Training Loss: 1.7273\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  31%|███       | 1800/5819 [12:24<28:20,  2.36it/s]","output_type":"stream"},{"name":"stdout","text":"Step 1800, Training Loss: 1.6889\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  34%|███▍      | 2000/5819 [13:46<27:25,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Step 2000, Training Loss: 1.8031\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  38%|███▊      | 2200/5819 [15:10<25:58,  2.32it/s]","output_type":"stream"},{"name":"stdout","text":"Step 2200, Training Loss: 1.5845\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  41%|████      | 2400/5819 [16:33<23:48,  2.39it/s]","output_type":"stream"},{"name":"stdout","text":"Step 2400, Training Loss: 1.6611\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  45%|████▍     | 2600/5819 [17:55<22:20,  2.40it/s]","output_type":"stream"},{"name":"stdout","text":"Step 2600, Training Loss: 1.5781\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  48%|████▊     | 2800/5819 [19:17<21:03,  2.39it/s]","output_type":"stream"},{"name":"stdout","text":"Step 2800, Training Loss: 1.7494\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  52%|█████▏    | 3000/5819 [20:40<18:28,  2.54it/s]","output_type":"stream"},{"name":"stdout","text":"Step 3000, Training Loss: 1.6429\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  55%|█████▍    | 3200/5819 [22:05<18:06,  2.41it/s]","output_type":"stream"},{"name":"stdout","text":"Step 3200, Training Loss: 1.7401\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  58%|█████▊    | 3400/5819 [23:28<15:31,  2.60it/s]","output_type":"stream"},{"name":"stdout","text":"Step 3400, Training Loss: 1.4396\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  62%|██████▏   | 3600/5819 [24:53<15:50,  2.34it/s]","output_type":"stream"},{"name":"stdout","text":"Step 3600, Training Loss: 1.5013\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:  62%|██████▏   | 3624/5819 [25:02<14:12,  2.58it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"## BLEU Score","metadata":{"id":"NbNdZRL3qi14"}},{"cell_type":"markdown","source":"In neural machine translation, to assess the translation performance of sequence-to-sequence (Seq2Seq) models, such as those consisting of Recurrent Neural Networks (RNNs), the Bilingual Evaluation Understudy (BLEU) score is often employed.\n\nThe BLEU score is a metric for evaluating a generated sentence to a set of reference sentences. A higher BLEU score corresponds to a translation that is more similar to the reference translations, with a score of 1 being a perfect match and a score of 0 indicating no overlap. BLEU score considers precision of n-grams in the generated text compared to the reference texts, and also includes a brevity penalty to prevent favoring overly short translations.\n\nThe BLEU score is calculated as follows:\n\n1. **Compute the n-gram precision, Pn, for n=1 to N**:\n\n  $$\n   P_n = \\frac{\\sum_{\\text{cand}\\in\\text{Candidates}} \\sum_{\\text{n-gram}\\in\\text{cand}} \\min(\\text{Count}_{\\text{cand}}(\\text{n-gram}), \\text{Max-Count}_{\\text{ref}}(\\text{n-gram}))}{\\sum_{\\text{cand}\\in\\text{Candidates}} \\sum_{\\text{n-gram}\\in\\text{cand}} \\text{Count}_{\\text{cand}}(\\text{n-gram})}\n  $$\n\n   Where:\n   - **Candidates**: The set of candidate translated sentences.\n   - **n-gram**: A contiguous sequence of n words from the candidate translation.\n   - **$\\text{Count}_{\\text{cand}}(\\text{n-gram})$**: The count of n-grams in the candidate translation.\n   - **$\\text{Max-Count}_{\\text{ref}}(\\text{n-gram})$**: The maximum count of the n-gram in any single reference translation.\n\n2. **Compute the brevity penalty, BP**:\n  $$\n     BP = \\begin{cases}\n       1 & \\text{if } c > r \\\\\n       e^{(1-r/c)} & \\text{if } c \\leq r\n     \\end{cases}\n  $$\n   \n  Where:\n   - **c**: The length of the candidate translation.\n   - **r**: The effective reference corpus length.\n\n3. **Compute the BLEU score**:\n  $$\n  \\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log P_n\\right)\n  $$\n\n   Where:\n   - **wn**: The weight for each n-gram precision score (typically uniform, such as 1/N for N=4).\n\n\n**Example Calculation:**\n\nConsider the following example with a candidate translation and two reference translations:\n\n- Candidate: \"The cat is on the mat.\"\n- Reference 1: \"The cat is on the mat.\"\n- Reference 2: \"There is a cat on the mat.\"\n\nFor simplicity, let's calculate the BLEU score for up to bigrams (N=2) with uniform weights for unigram and bigram precision (w1=w2=0.5).\n\n1. **Unigram Precision (P1)**:\n   The candidate has six unigrams, all of which appear in both reference sentences. So, P1 is 6/6 = 1.\n\n  - Candidate unigrams: \"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\"\n  - Reference 1 unigrams: \"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\"\n  - Reference 2 unigrams: \"There\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"\n\n  All unigrams in the candidate sentence appear in both reference sentences. The counts for each unigram in the candidate are:\n\n  - \"The\": 2\n  - \"cat\": 1\n  - \"is\": 1\n  - \"on\": 1\n  - \"the\": 1 (again for the second occurrence)\n  - \"mat\": 1\n\n  Since all unigrams are found in the reference sentences, we get:\n  $$\n  P_1 = \\frac{6}{6} = 1\n  $$\n\n2. **Bigram Precision (P2)**:\n   There are five bigrams in the candidate sentence. All five appear in Reference 1, and four appear in Reference 2. The maximum count from the references is used for each bigram, and hence P2 is also 5/5 = 1.\n\n    - Candidate bigrams: \"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"\n    - Reference 1 bigrams: \"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"\n    - Reference 2 bigrams: \"There is\", \"is a\", \"a cat\", \"cat on\", \"on the\", \"the mat\"\n\n  In the candidate, each bigram appears once, and all but one (\"The cat\") also appear in Reference 2. For \"The cat\", we take the count from Reference 1:\n\n  - \"The cat\": 1 (from Reference 1)\n  - \"cat is\": 1\n  - \"is on\": 1\n  - \"on the\": 1\n  - \"the mat\": 1\n\n  The counts match for all bigrams, so the precision is:\n  $$\n  P_2 = \\frac{5}{5} = 1\n  $$\n\n3. **Brevity Penalty (BP)**:\n    - Candidate sentence length (c): 6 words\n    - Reference sentence lengths: 6 words (Reference 1), 7 words (Reference 2)\n    - The effective reference corpus length (r) is the one that is closest to the candidate length, which in this case is 6 words from Reference 1.\n\n  Since the candidate length c is not less than the reference length r, the brevity penalty BP is 1:\n  $$\n  BP = 1 \\text{ (because c is not less than r)}\n  $$\n\n4. **BLEU Score**:\n  With uniform weights (0.5 each for unigram and bigram precision), the BLEU score is:\n\n   $$\n   \\text{BLEU} = BP \\cdot \\exp\\left(0.5 \\log P_1 + 0.5 \\log P_2\\right)\n   \\text{BLEU} = 1 \\cdot \\exp\\left(0.5 \\log 1 + 0.5 \\log 1\\right) = 1\n   $$\n\nIn this example, the BLEU score is 1, indicating a perfect match with the reference translations. However, it's important to note that in real-world scenarios, BLEU scores are typically less than 1, reflecting various degrees of translation quality.\n","metadata":{"id":"3ICQ8k80rT3V"}},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\n\ndef calculate_bleu(reference_sentences, candidate_sentence):\n    # The reference sentences should be tokenized and passed as a list of lists\n    # The candidate sentence should be tokenized and passed as a list\n    # Each reference and candidate sentence should start and end with <start> and <end> tokens respectively\n\n    # We'll use nltk's method for smoothing.\n    smoothing = SmoothingFunction().method1\n\n    # Calculate BLEU score\n    bleu_score = sentence_bleu(reference_sentences, candidate_sentence, smoothing_function=smoothing)\n\n    return bleu_score\n\n# Example usage:\n# Assume that translate_sentence is your provided function and it outputs the candidate sentence tokens\n# Assume reference_texts is a list of the reference translations for the input sentence, tokenized\n\n# This is an example candidate sentence produced by your model\ncandidate = translate_sentence(model, \"The quick brown fox jumps over the lazy dog\", src_vocab, trg_vocab, device)\n\n# Example reference sentences\nreferences = [\n    [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n    [\"A\", \"fast\", \"brown\", \"fox\", \"leaps\", \"over\", \"the\", \"lazy\", \"dog\"]\n]\n# Tokenize the reference sentences\nreferences = [[word.lower() for word in ref] for ref in references]\n\n# Calculate BLEU score\nbleu_score = calculate_bleu(references, candidate)\nprint(f\"BLEU score: {bleu_score}\")","metadata":{"id":"sFMGgV5g1ZYN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"0sM-jiGKZNJo"}},{"cell_type":"markdown","source":"Today, NMT systems are integral parts of global communication, supporting instant translation across numerous language pairs with increasing reliability. As NMT continues to evolve, it integrates more deeply with other AI technologies, pushing towards truly interactive, real-time multilingual communication and making the dream of removing language barriers more attainable than ever. The evolution from simple rule-based systems to complex neural architectures reflects broader trends in AI towards more holistic and context-aware systems, promising exciting developments for the future of language processing.","metadata":{"id":"nasvVV_hZNJp"}},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"- A ten-minute introduction to sequence-to-sequence learning in Keras: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html","metadata":{}}]}