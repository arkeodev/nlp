{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/arkeodev/nlp/blob/main/Neural_Machine_Translation_with_RNNs/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"gfSbr4X-ZNJO"},"source":["$$\n","\\begin{array}{c}\n","\\text{$\\Large We\\ must\\ cultivate\\ our\\ garden.$} \\\\\n","{\\text{$\\small Voltaire\\ -\\ Candide$}} \\\\\n","\\end{array}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"vfpBmPQlZNJT"},"source":["# Neural Machine Translation with Sequence to Sequence Networks"]},{"cell_type":"markdown","metadata":{"id":"vdLqpQDlZNJT"},"source":["Neural Machine Translation (NMT) is a sophisticated approach to language translation that leverages deep learning techniques to facilitate the automatic translation of text from one language to another. As a subset of natural language processing (NLP), NMT has transformed the landscape of translation by introducing models that can understand and translate whole sentences, often preserving the context and semantic meaning better than previous rule-based and statistical methods."]},{"cell_type":"markdown","metadata":{"id":"dPnuBAxbncMc"},"source":["## Sequence Networks"]},{"cell_type":"markdown","metadata":{"id":"JAZVROdHncMc"},"source":["<figure>\n","    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Neural_Machine_Translation_with_RNNs/images/sequence_models_types.png\" width=\"1000\" height=\"300\" alt=\"Sequence Network Types\">\n","    <figcaption>Sequence Network Types</figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"55pwX2AgncMd"},"source":["Sequence networks are specialized architectures within the field of neural networks designed to handle sequential data. Unlike traditional neural networks that assume all inputs (and outputs) are independent of each other, sequence networks are adept at managing data where the order is significant, such as time series data, sentences in natural language processing, or steps in a video. These networks can capture temporal dynamics and relationships within the data, allowing them to perform tasks like predicting the next word in a sentence, generating text, understanding spoken language, or forecasting time-dependent variables. The adaptability of sequence networks to various input-output mappings—such as one-to-one, one-to-many, many-to-one, and many-to-many—makes them extraordinarily versatile and powerful for a wide range of sequential processing tasks in machine learning.\n","\n","Here is the Sequence Network Types:\n","\n","- **One to One**: This type of network has a single input and produces a single output. It is the simplest form of sequence modeling and is not typically used for sequences. An example task would be a traditional neural network that maps one input to one output, like a simple regression.\n","\n","- **One to Many**: This network takes a single input and produces a sequence of outputs. It is useful for tasks where one piece of information can lead to a sequence of results. An example task would be image captioning, where an image input results in a sequence of words forming the caption.\n","\n","- **Many to One**: Here, a sequence of inputs leads to a single output. This is often used for tasks where the entire sequence is necessary to produce a meaningful result. An example task would be sentiment analysis, where a sequence of words (a sentence or document) is classified as expressing a positive or negative sentiment.\n","\n","- **Many to Many (synced)**: This type of network processes a sequence of inputs and produces a sequence of outputs where the output is generated synchronously with the input. This is often used in tasks where each time step in the input is directly related to a time step in the output. An example task would be video frame prediction, where each input frame corresponds to an output prediction.\n","\n","- **Many to Many (asynced)**: This network type processes a sequence of inputs and produces a sequence of outputs, but the outputs are not synchronously produced with the inputs. This can be used in tasks where a sequence is processed before the output sequence is generated. An example task would be machine translation, where an entire sentence must be read before the translation can begin."]},{"cell_type":"markdown","metadata":{"id":"sBkMr1YaZNJU"},"source":["## Impact on Language Processing"]},{"cell_type":"markdown","metadata":{"id":"9QVzwT7QZNJU"},"source":["The advent of Sequence networks and the NMT has notably impacted the field of language processing by introducing several key capabilities:\n","\n","- **Handling of Idiomatic Expressions**: Traditional models often struggled with idioms and culturally specific phrases. NMT's contextual understanding significantly improves handling such expressions, translating them more naturally and accurately.\n","\n","- **Reduction in Translation Latency**: NMT models can translate texts substantially faster than traditional models, especially when optimized and deployed on appropriate hardware. This speed is crucial for applications requiring real-time translation.\n","\n","- **Improved Scalability**: Given the right computational resources, NMT models can be scaled to accommodate large-scale translation tasks that were previously impractical, making it feasible to offer high-quality translation services on a global scale.\n","\n","- **Accessibility**: By lowering language barriers, NMT increases accessibility, allowing more people to access content in their native or preferred languages. This inclusivity is crucial in educational contexts and information dissemination."]},{"cell_type":"markdown","metadata":{"id":"Ul7ZrzzyZNJV"},"source":["## Evolution of Neural Machine Translation"]},{"cell_type":"markdown","metadata":{"id":"Z4yh1cS9ZNJV"},"source":["The development of Neural Machine Translation (NMT) marks a significant milestone in the progression from basic models to sophisticated neural networks designed to handle complex language processing tasks. This evolution is not only a story of technological advancement but also of conceptual shifts in how machines understand and process human languages.\n"]},{"cell_type":"markdown","metadata":{"id":"8g2_SUNoZNJW"},"source":["### Early Translation Models"]},{"cell_type":"markdown","metadata":{"id":"LizOjhg5ZNJW"},"source":["1. **Rule-Based Translation Systems (RBMT)**: The earliest attempts at machine translation were rule-based, relying on a comprehensive set of language rules and bilingual dictionaries. These systems required extensive manual work to define grammatical structures and vocabulary mappings between the source and target languages. While they were somewhat effective for languages with similar structures, their performance dropped significantly with complex or unrelated language pairs.\n","\n","2. **Statistical Machine Translation (SMT)**: Emerging in the late 1980s and coming into prominence in the 1990s, SMT represented a shift towards data-driven approaches. SMT models used statistical methods to translate text based on the probability distributions of words and phrases, learned from large corpora of translated texts. This method was more flexible and scalable than RBMT, but still struggled with syntactic and semantic nuances, often producing literal translations that lacked contextual coherence."]},{"cell_type":"markdown","metadata":{"id":"X5nRahUvZNJW"},"source":["### The Role of Embeddings in NMT Evolution"]},{"cell_type":"markdown","metadata":{"id":"3qoH6gZXZNJX"},"source":["#### Introduction to Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"0UdN_ZlbZNJX"},"source":["Word embeddings are a type of word representation that allows words with similar meanings to have a similar representation. They are low-dimensional, continuous, dense vectors that are learned from text data. These vectors aim to capture syntactic and semantic word relationships based on the contexts in which words appear. The development and use of word embeddings have been fundamental in advancing NMT by providing a more nuanced and effective means of representing language data in neural networks."]},{"cell_type":"markdown","metadata":{"id":"g6kcO44sZNJX"},"source":["#### Early Embeddings: From One-Hot to Distributed Representations"]},{"cell_type":"markdown","metadata":{"id":"wZEDTCqiZNJX"},"source":["1. **One-Hot Encodings**: Initially, language models, including early translation systems, relied on one-hot encoding to represent words. Each word in the vocabulary was represented by a vector where only one element is one, and all others are zero. This method was simple but had major limitations, such as high dimensionality and an inability to capture semantic relationships between words.\n","\n","2. **Distributed Representations (Word2Vec, GloVe)**: The shift to distributed representations marked a significant improvement. Techniques like Word2Vec and GloVe allowed words to be represented as dense vectors where semantically similar words were mapped to proximate points in vector space. These embeddings were pre-trained on large corpora and could then be used to initialize the first layer of neural networks in NMT systems, providing a richer and more expressive input representation."]},{"cell_type":"markdown","metadata":{"id":"CeOP0vH3ZNJX"},"source":["#### Embeddings in NMT"]},{"cell_type":"markdown","metadata":{"id":"t8p0GgN5ZNJX"},"source":["1. **Improved Semantic Capture**: Embeddings provided a way for models to grasp semantic meanings and relationships, which are crucial for accurate translation. For instance, synonyms or contextually related words could be recognized as closer in the embedding space, aiding in more coherent and contextually appropriate translations.\n","\n","2. **Efficiency and Scalability**: By reducing the dimensionality of the input space compared to one-hot encodings, embeddings made neural models more computationally efficient and easier to train. This scalability was essential as NMT models began to be applied to larger and more complex language pairs and datasets.\n","\n","3. **Custom Embeddings for NMT**: As NMT systems evolved, researchers started training custom embeddings as part of the end-to-end training process of the translation model. This allowed the embeddings to be optimized specifically for the translation task, rather than relying solely on pre-trained embeddings. This integration led to further improvements in translation quality by tailoring the embeddings to capture nuances specific to the languages and textual contexts involved in the translation tasks."]},{"cell_type":"markdown","metadata":{"id":"p10JwQaxZNJY"},"source":["#### Continuous Improvement with Contextual Embeddings"]},{"cell_type":"markdown","metadata":{"id":"VBP6fpBPZNJY"},"source":["With the arrival of models like ELMo and later BERT, the concept of embeddings expanded from static representations to contextual embeddings. These are dynamic embeddings that change based on the words' context in a sentence, providing even richer information about word usage and meaning. This advancement significantly improved the subtlety with which NMT systems could handle language, leading to even better translations, particularly in handling idiomatic and nuanced phrases."]},{"cell_type":"markdown","metadata":{"id":"FS_V1d-FZNJY"},"source":["### Introduction of Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"Bouj9xFzZNJY"},"source":["1. **Initial Neural Approaches**: The introduction of neural networks to machine translation began with feedforward neural networks, which were initially used to improve specific components of SMT systems, like language modeling and re-ranking of translation hypotheses. These early neural components hinted at the potential of fully neural systems.\n","\n","2. **Recurrent Neural Networks (RNNs)**: The true breakthrough came with the application of RNNs, particularly Long Short-Term Memory (LSTM) networks, which could remember long sequences of words—crucial for maintaining context in sentences. The encoder-decoder architecture, where one RNN encoded the input sentence into a context vector and another RNN decoded this vector into a translation, became a foundational model for NMT."]},{"cell_type":"markdown","metadata":{"id":"4TVy9DaBncMh"},"source":["<figure>\n","    <img src=\"https://raw.githubusercontent.com/arkeodev/nlp/main/Neural_Machine_Translation_with_RNNs/images/neural_machine_translation.png\" width=\"1000\" height=\"300\" alt=\"Neural Machine Translation\">\n","    <figcaption>Neural Machine Translation</figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"oae3U4x8ZNJY"},"source":["### Advancements and Modern Architectures"]},{"cell_type":"markdown","metadata":{"id":"8Olkw7tvZNJY"},"source":["\n","1. **Attention Mechanisms**: The introduction of attention mechanisms was a pivotal improvement in NMT. It allowed the model to focus on different parts of the input sentence while translating, mimicking how human translators revisit different words and phrases. This led to translations that were not only more fluent but also more accurate in terms of context and semantics.\n","\n","2. **Transformers and Self-Attention**: The development of the Transformer model in 2017 marked the next significant evolution. Transformers replaced recurrence with self-attention layers, which process all words in the sentence simultaneously. This parallel processing significantly increased the speed and efficiency of training and improved the handling of long-range dependencies in text, setting new standards for translation quality.\n","\n","3. **Integration of BERT and Pre-trained Models**: Following the success of Transformers, the use of bidirectional and pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) further pushed the boundaries. These models, pre-trained on vast amounts of text before being fine-tuned for translation, brought improvements in understanding contextual nuances and generative capabilities.\n"]},{"cell_type":"markdown","metadata":{"id":"bP_Mle8mZNJZ"},"source":["## Implementation of NMT"]},{"cell_type":"markdown","metadata":{"id":"TFHDkd_wZNJZ"},"source":["### Setting Up the Environment"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:41:58.932499Z","iopub.status.busy":"2024-04-23T11:41:58.932071Z","iopub.status.idle":"2024-04-23T11:41:58.943540Z","shell.execute_reply":"2024-04-23T11:41:58.942636Z","shell.execute_reply.started":"2024-04-23T11:41:58.932462Z"},"id":"opc9RwgsZNJZ","trusted":true},"outputs":[],"source":["# Import necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset\n","\n","import torchtext\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import spacy"]},{"cell_type":"markdown","metadata":{"id":"KQ1iiYeRZNJb"},"source":["### Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"n4cEANKcZNJb"},"source":["#### Downloading the Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-04-23T11:41:58.945453Z","iopub.status.busy":"2024-04-23T11:41:58.945177Z","iopub.status.idle":"2024-04-23T11:42:01.544392Z","shell.execute_reply":"2024-04-23T11:42:01.543282Z","shell.execute_reply.started":"2024-04-23T11:41:58.945431Z"},"id":"Bz5_cKnEZNJb","outputId":"de50363c-5aaa-44bd-8698-ba8db34d44e9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-04-23 23:12:30--  https://www.manythings.org/anki/fra-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n","Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7943074 (7,6M) [application/zip]\n","Saving to: ‘fra-eng.zip’\n","\n","fra-eng.zip         100%[===================>]   7,57M  3,02MB/s    in 2,5s    \n","\n","2024-04-23 23:12:34 (3,02 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n","\n","Archive:  fra-eng.zip\n","  inflating: dataset/_about.txt      \n","  inflating: dataset/fra.txt         \n"]}],"source":["!wget https://www.manythings.org/anki/fra-eng.zip\n","!unzip -o fra-eng.zip -d dataset\n","\n","# Path to the data txt file on disk.\n","data_path = 'dataset/fra.txt'\n","Path(\"fra-eng.zip\").unlink()"]},{"cell_type":"markdown","metadata":{"id":"zLroWGTwZNJc"},"source":["#### Preprocessing Data"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-04-23T11:42:01.546178Z","iopub.status.busy":"2024-04-23T11:42:01.545872Z","iopub.status.idle":"2024-04-23T11:42:36.423016Z","shell.execute_reply":"2024-04-23T11:42:36.421851Z","shell.execute_reply.started":"2024-04-23T11:42:01.546150Z"},"id":"Zfu383ASZNJc","outputId":"257c83bd-59c8-4068-9e16-3525a345e2e4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('fr_core_news_sm')\n"]}],"source":["import os\n","os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n","! python -m spacy download en_core_web_sm -q\n","! python -m spacy download fr_core_news_sm -q"]},{"cell_type":"markdown","metadata":{"id":"kurjtzWwZNJd"},"source":["**Load Data:**\n","\n","This function opens the dataset and extracts the English and French sentences from each line, ignoring metadata and attribution text."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:42:36.425126Z","iopub.status.busy":"2024-04-23T11:42:36.424745Z","iopub.status.idle":"2024-04-23T11:42:39.469660Z","shell.execute_reply":"2024-04-23T11:42:39.468675Z","shell.execute_reply.started":"2024-04-23T11:42:36.425088Z"},"id":"UXzsL8_DZNJd","trusted":true},"outputs":[],"source":["# Load spaCy models\n","spacy_en = spacy.load('en_core_web_sm')\n","spacy_fr = spacy.load('fr_core_news_sm')\n","\n","def load_data(file_path):\n","    \"\"\"Load and preprocess data from file.\"\"\"\n","    data = []\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            src, trg, _ = line.strip().split('\\t', 2)\n","            data.append((src, trg))\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"LRCpVVrSZNJd"},"source":["**Tokenization:**\n","\n","Depending on the language, the appropriate spaCy tokenizer is applied. This is handled in a separate function to keep tokenization modular and reusable."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:42:39.473239Z","iopub.status.busy":"2024-04-23T11:42:39.472879Z","iopub.status.idle":"2024-04-23T11:42:39.479075Z","shell.execute_reply":"2024-04-23T11:42:39.477986Z","shell.execute_reply.started":"2024-04-23T11:42:39.473206Z"},"id":"appOI-LNZNJd","trusted":true},"outputs":[],"source":["def tokenize(data, lang):\n","    \"\"\"Tokenize sentences using the specified spaCy tokenizer.\"\"\"\n","    tokenizer = spacy_en.tokenizer if lang == 'en' else spacy_fr.tokenizer\n","    tokenized_data = []\n","    for src, trg in data:\n","        sentence = src if lang == 'en' else trg\n","        tokenized_sentence = [token.text for token in tokenizer(sentence)]\n","        tokenized_data.append(tokenized_sentence)\n","    return tokenized_data"]},{"cell_type":"markdown","metadata":{"id":"vAT-TgHLZNJe"},"source":["**Yield Tokens:**\n","\n","This generator function is used to iterate over the data for building the vocabulary.\n","\n","It includes <start> and <end> tokens in each sequence, which are essential for the LSTM model's training and inference phases."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:42:39.480524Z","iopub.status.busy":"2024-04-23T11:42:39.480241Z","iopub.status.idle":"2024-04-23T11:42:39.487817Z","shell.execute_reply":"2024-04-23T11:42:39.486849Z","shell.execute_reply.started":"2024-04-23T11:42:39.480501Z"},"id":"_vCphVKFZNJe","trusted":true},"outputs":[],"source":["def yield_tokens(data, lang):\n","    \"\"\"Yield tokens for vocabulary building.\"\"\"\n","    for sentence in tokenize(data, lang):\n","        yield ['<start>'] + sentence + ['<end>']"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-04-23T11:42:39.489452Z","iopub.status.busy":"2024-04-23T11:42:39.489114Z","iopub.status.idle":"2024-04-23T11:43:00.318562Z","shell.execute_reply":"2024-04-23T11:43:00.317638Z","shell.execute_reply.started":"2024-04-23T11:42:39.489424Z"},"id":"v5x32dufZNJe","outputId":"0c52645d-3ef1-423f-d9e3-912f52d239b6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample English vocabulary: ['<unk>', '<start>', '<end>', '.', 'I', 'you', 'to', '?', 'the', \"n't\"]\n","Sample French vocabulary: ['<unk>', '<start>', '<end>', '.', 'de', 'Je', '?', 'pas', 'est', 'que']\n"]}],"source":["data_path = 'dataset/fra.txt'\n","data = load_data(data_path)\n","\n","# Build vocabularies\n","vocab_en = build_vocab_from_iterator(yield_tokens(data, 'en'), specials=['<unk>', '<start>', '<end>'])\n","vocab_fr = build_vocab_from_iterator(yield_tokens(data, 'fr'), specials=['<unk>', '<start>', '<end>'])\n","\n","# Set default unknown token index\n","vocab_en.set_default_index(vocab_en[\"<unk>\"])\n","vocab_fr.set_default_index(vocab_fr[\"<unk>\"])\n","\n","print(\"Sample English vocabulary:\", list(vocab_en.get_itos())[:10])\n","print(\"Sample French vocabulary:\", list(vocab_fr.get_itos())[:10])\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:43:00.320537Z","iopub.status.busy":"2024-04-23T11:43:00.320248Z","iopub.status.idle":"2024-04-23T11:43:00.334601Z","shell.execute_reply":"2024-04-23T11:43:00.333635Z","shell.execute_reply.started":"2024-04-23T11:43:00.320511Z"},"id":"gdJpPpjYZNJk","trusted":true},"outputs":[],"source":["class TranslationDataset(Dataset):\n","    \"\"\"\n","    A dataset class for handling machine translation datasets.\n","\n","    Attributes:\n","    data (list of tuples): A list where each tuple contains a source sentence and a target sentence.\n","    src_vocab (dict): A dictionary mapping source language tokens to indices.\n","    trg_vocab (dict): A dictionary mapping target language tokens to indices.\n","    \"\"\"\n","    def __init__(self, data, src_vocab, trg_vocab):\n","        \"\"\"\n","        Initialize the dataset with source and target data and vocabularies.\n","\n","        Parameters:\n","        data (list): List of tuples, where each tuple is (source_sentence, target_sentence).\n","        src_vocab (dict): Dictionary mapping source vocabulary words to integers.\n","        trg_vocab (dict): Dictionary mapping target vocabulary words to integers.\n","        \"\"\"\n","        self.data = data\n","        self.src_vocab = src_vocab\n","        self.trg_vocab = trg_vocab\n","\n","    def __len__(self):\n","        \"\"\"Return the number of items in the dataset.\"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieve an item from the dataset at the specified index.\n","\n","        Parameters:\n","        idx (int): Index of the item to retrieve.\n","\n","        Returns:\n","        tuple: A tuple containing the source indices and target indices as tensors.\n","        \"\"\"\n","        src_sentence, trg_sentence = self.data[idx]\n","        # Convert source sentence to indices, adding start and end tokens\n","        src_indices = [self.src_vocab['<start>']] + [self.src_vocab[token] for token in src_sentence] + [self.src_vocab['<end>']]\n","        # Convert target sentence to indices, adding start and end tokens\n","        trg_indices = [self.trg_vocab['<start>']] + [self.trg_vocab[token] for token in trg_sentence] + [self.trg_vocab['<end>']]\n","        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(trg_indices, dtype=torch.long)\n","\n","def collate_fn(batch):\n","    \"\"\"\n","    A function to collate data samples into batch tensors.\n","\n","    Parameters:\n","    batch (list): A list of tuples, where each tuple is (source_indices, target_indices).\n","\n","    Returns:\n","    tuple: A tuple containing batched source and target sequences.\n","    \"\"\"\n","    src_batch, trg_batch = zip(*batch)  # Unpack batch data\n","    # Pad sequences for uniform length in the batch, using <unk> token for padding\n","    src_batch = pad_sequence(src_batch, padding_value=vocab_en[\"<unk>\"])\n","    trg_batch = pad_sequence(trg_batch, padding_value=vocab_fr[\"<unk>\"])\n","    return src_batch, trg_batch\n","\n","# Load data\n","split_ratio = 0.8\n","train_size = int(len(data) * split_ratio)\n","train_data = TranslationDataset(data[:train_size], vocab_en, vocab_fr)\n","valid_data = TranslationDataset(data[train_size:], vocab_en, vocab_fr)\n","\n","# Create DataLoaders\n","batch_size = 32\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"]},{"cell_type":"markdown","metadata":{"id":"qQ7m80AzZNJl"},"source":["### Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"Vn-Z2gyMZNJl"},"source":["#### Encoder"]},{"cell_type":"markdown","metadata":{"id":"EWM5ujrXZNJl"},"source":["Processes the input English sentences and creates context vectors."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:43:00.336125Z","iopub.status.busy":"2024-04-23T11:43:00.335800Z","iopub.status.idle":"2024-04-23T11:43:00.348494Z","shell.execute_reply":"2024-04-23T11:43:00.347666Z","shell.execute_reply.started":"2024-04-23T11:43:00.336093Z"},"id":"ipBO3rzGZNJl","trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    \"\"\"\n","        Initializes the Encoder part of a Seq2Seq model.\n","\n","        Parameters:\n","        input_dim (int): The size of the input vocabulary.\n","        emb_dim (int): The dimension of the embedding layer.\n","        hid_dim (int): The dimension of the hidden states in the LSTM.\n","        n_layers (int): The number of LSTM layers.\n","        dropout (float): The dropout rate used for regularizing the model.\n","    \"\"\"\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","        self.n_layers = n_layers\n","        self.hid_dim = hid_dim\n","        \n","        self.embedding = nn.Embedding(input_dim, emb_dim) # Embedding layer to convert input indices into dense vectors\n","        # The hidden dimension is divided by 2 for bidirectional output compatibility\n","        self.rnn = nn.GRU(emb_dim, hid_dim // 2, n_layers, dropout=dropout, bidirectional=True)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        \"\"\"\n","        Forward pass through the encoder.\n","        The encoder uses a bidirectional GRU. The hidden states \n","        from both directions are concatenated to form a single output \n","        vector.\n","\n","        Parameters:\n","        src (Tensor): The input sequence to the encoder (batch of input indices).\n","\n","        Returns:\n","        Tuple[Tensor, Tensor]: A tuple containing the final hidden state and the final cell state of the LSTM.\n","        \"\"\"\n","        embedded = self.dropout(self.embedding(src)) # Apply embedding and dropout to the input sequence\n","        _, hidden = self.rnn(embedded)  # Pass the embedded input through the GRU\n","        # Reshape hidden state to handle bidirectional outputs correctly:\n","        hidden = hidden.view(self.n_layers, 2, src.size(1), self.hid_dim // 2)\n","        hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)  # Concatenate the hidden states\n","\n","        return hidden # Return only the hidden and cell states to be used by the decoder\n"]},{"cell_type":"markdown","metadata":{"id":"mLIZ7NwMZNJl"},"source":["#### Decoder"]},{"cell_type":"markdown","metadata":{"id":"DTJfB6BkZNJl"},"source":["Uses the context vectors to start generating the translated French sentences."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:43:00.349940Z","iopub.status.busy":"2024-04-23T11:43:00.349643Z","iopub.status.idle":"2024-04-23T11:43:00.359952Z","shell.execute_reply":"2024-04-23T11:43:00.359248Z","shell.execute_reply.started":"2024-04-23T11:43:00.349913Z"},"id":"vwdZZcF5ZNJm","trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","        \"\"\"\n","        Decoder initialization.\n","\n","        Parameters:\n","        output_dim (int): Size of the output vocabulary.\n","        emb_dim (int): Size of the embeddings.\n","        hid_dim (int): Dimensionality of the hidden state and cell state.\n","        n_layers (int): Number of layers in the LSTM.\n","        dropout (float): Dropout rate for regularization.\n","        \"\"\"\n","        super().__init__()\n","        self.output_dim = output_dim  # Target vocabulary size\n","        self.embedding = nn.Embedding(output_dim, emb_dim)  # Embedding layer\n","        # Adjust GRU to accept double the layer count\n","        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n","        self.fc_out = nn.Linear(hid_dim, output_dim)  # Fully connected layer to get the output\n","        self.dropout = nn.Dropout(dropout)  # Dropout layer for regularization\n","\n","    def forward(self, input, hidden):\n","        \"\"\"\n","        Forward pass of the decoder.\n","        The decoder's initial hidden state needs to handle the doubled output \n","        from the bidirectional encoder.\n","\n","        Parameters:\n","        input (Tensor): Input tensor for the decoder.\n","        hidden (Tensor): Hidden state from the last time step or the encoder.\n","        cell (Tensor): Cell state from the last time step or the encoder.\n","\n","        Returns:\n","        Tuple[Tensor, Tensor, Tensor]: A tuple containing the output predictions, the new hidden state, and the new cell state.\n","        \"\"\"\n","        input = input.unsqueeze(0)  # Add a batch dimension (batch size is assumed to be 1)\n","        embedded = self.dropout(self.embedding(input))  # Embed the input word and apply dropout\n","        output, hidden = self.rnn(embedded, hidden)\n","        prediction = self.fc_out(output.squeeze(0))  # Pass the GRU output through the linear layer to get predictions\n","        \n","        return prediction, hidden # Return the prediction, new hidden state"]},{"cell_type":"markdown","metadata":{"id":"UrWqNTtdZNJm"},"source":["#### Seq2Seq Wrapper"]},{"cell_type":"markdown","metadata":{"id":"EfYgOypQZNJm"},"source":["Manages the data flow from the encoder to the decoder and structures the training process."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:43:00.361373Z","iopub.status.busy":"2024-04-23T11:43:00.361096Z","iopub.status.idle":"2024-04-23T11:43:00.371747Z","shell.execute_reply":"2024-04-23T11:43:00.370865Z","shell.execute_reply.started":"2024-04-23T11:43:00.361352Z"},"id":"i5HOzgxPZNJm","trusted":true},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        \"\"\"\n","        Initializes the Seq2Seq model which encompasses an encoder and decoder.\n","\n","        Parameters:\n","        encoder (nn.Module): The encoder neural network module.\n","        decoder (nn.Module): The decoder neural network module.\n","        device (torch.device): The device to run the model on ('cpu' or 'cuda').\n","        \"\"\"\n","        super().__init__()\n","        self.encoder = encoder  # The RNN/LSTM/GRU encoder module\n","        self.decoder = decoder  # The RNN/LSTM/GRU decoder module\n","        self.device = device    # Device on which to perform computations\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        \"\"\"\n","        The forward pass for the Seq2Seq model.\n","\n","        Parameters:\n","        src (Tensor): The input tensor containing the source sequence.\n","        trg (Tensor): The target tensor containing the target sequence.\n","        teacher_forcing_ratio (float, optional): The probability of using teacher forcing.\n","\n","        Returns:\n","        Tensor: The output tensor containing the predicted sequence.\n","        \"\"\"\n","        # Determine the batch size and target sequence length\n","        trg_len = trg.shape[0]\n","        batch_size = trg.shape[1]\n","        trg_vocab_size = self.decoder.output_dim # The size of the target vocabulary\n","        \n","        # Initialize a tensor to hold the decoder outputs\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        # Run the source sequence through the encoder\n","        hidden = self.encoder(src)\n","        \n","        # Take the first token of the target sequence as the initial decoder input\n","        input = trg[1,:]  # Start token\n","        \n","        # Iterate over each token in the target sequence\n","        for t in range(1, trg_len):\n","            # Run the input and the previous hidden and cell states through the decoder\n","            output, hidden = self.decoder(input, hidden)\n","            outputs[t] = output\n","            \n","            # Get the highest probability word from the decoder's output\n","            top1 = output.argmax(1)\n","            \n","            # Use the actual next token as the next input (teacher forcing),\n","            # or use the predicted token (no teacher forcing)\n","            input = trg[t] if (torch.rand(1) < teacher_forcing_ratio) else top1\n","            \n","        return outputs  # Return the tensor containing all the decoder outputs\n","    \n","    def loss(self, Y_hat, Y):\n","        \"\"\"\n","        Compute the masked cross-entropy loss, ignoring the padding tokens in the target sequences.\n","\n","        Args:\n","        Y_hat (torch.Tensor): Predicted output probabilities from the model (logits).\n","        Y (torch.Tensor): Ground truth target sequences (indices).\n","\n","        Returns:\n","        torch.Tensor: Scalar value of the average loss computed over non-padded tokens.\n","        \"\"\"\n","        # Apply softmax and calculate log probabilities\n","        log_probs = torch.nn.functional.log_softmax(Y_hat, dim=-1)\n","        \n","        # Calculate negative log-likelihood loss with no reduction to keep losses for each element\n","        l = torch.nn.functional.nll_loss(log_probs.transpose(1, 2), Y, reduction='none')\n","        \n","        # Create a mask for padding tokens (0 where Y is padding, 1 elsewhere)\n","        mask = (Y != self.tgt_pad).type(torch.float32)\n","        \n","        # Apply mask to the losses\n","        masked_loss = l * mask\n","        \n","        # Calculate the mean loss only over non-masked elements\n","        return masked_loss.sum() / mask.sum()"]},{"cell_type":"markdown","metadata":{"id":"RPHDuQ0zTZXg"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"_g4EG91mRwTE"},"source":["#### Parameters"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:45:31.079214Z","iopub.status.busy":"2024-04-23T11:45:31.078841Z","iopub.status.idle":"2024-04-23T11:45:31.085379Z","shell.execute_reply":"2024-04-23T11:45:31.084417Z","shell.execute_reply.started":"2024-04-23T11:45:31.079185Z"},"id":"4mIF7rntZNJm","trusted":true},"outputs":[],"source":["# Model architecture parameters\n","model_params = {\n","    'input_dim': len(vocab_en),  # vocabulary size for encoder\n","    'output_dim': len(vocab_fr), # vocabulary size for decoder\n","    'enc_emb_dim': 300,           # encoder embedding dimension\n","    'dec_emb_dim': 300,           # decoder embedding dimension\n","    'hid_dim': 512,              # dimension of the hidden layers\n","    'n_layers': 1,               # number of layers in each the encoder and decoder\n","    'enc_dropout': 0.5,          # dropout rate for the encoder\n","    'dec_dropout': 0.5           # dropout rate for the decoder\n","}\n","\n","# Training parameters\n","training_params = { \n","    'n_epochs': 5,               # number of epochs to train\n","    'train_losses': [],          # list to record training losses\n","    'valid_losses': [],          # list to record validation losses\n","    'print_step_size': 1000      # print step size\n","}"]},{"cell_type":"markdown","metadata":{"id":"dkjrHt56RwTE"},"source":["#### Model Initialisation"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-04-23T11:45:34.678252Z","iopub.status.busy":"2024-04-23T11:45:34.677540Z","iopub.status.idle":"2024-04-23T11:45:34.757573Z","shell.execute_reply":"2024-04-23T11:45:34.756730Z","shell.execute_reply.started":"2024-04-23T11:45:34.678217Z"},"id":"IzfYFICnRwTE","outputId":"41455875-81b1-4ab9-fd22-a1283a1b515c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model from: ./model/best_model.pt\n"]},{"name":"stderr","output_type":"stream","text":["/Users/kenanagyel/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}],"source":["# Path management for model saving/loading\n","model_path = './model/best_model.pt'\n","model_dir = Path('./model')\n","\n","def prepare_model_directory():\n","    if model_dir.exists():\n","        print(\"Model directory already exists.\")\n","    else:\n","        model_dir.mkdir(parents=True, exist_ok=True)\n","        print(\"Model directory was created.\")\n","\n","def load_or_initialize_model(device):\n","    if load_saved_model and model_dir.joinpath('best_model.pt').exists():\n","        print(\"Loading model from:\", model_path)\n","        # Initialize the model first\n","        enc = Encoder(model_params['input_dim'], model_params['enc_emb_dim'], model_params['hid_dim'], model_params['n_layers'], model_params['enc_dropout'])\n","        dec = Decoder(model_params['output_dim'], model_params['dec_emb_dim'], model_params['hid_dim'], model_params['n_layers'], model_params['dec_dropout'])\n","        model = Seq2Seq(enc, dec, device).to(device)\n","        # Load the state dictionary\n","        model.load_state_dict(torch.load(model_path, map_location=device))\n","        return model\n","    else:\n","        prepare_model_directory()\n","        print(\"No saved model found. Initializing a new model.\")\n","        enc = Encoder(model_params['input_dim'], model_params['enc_emb_dim'], model_params['hid_dim'], model_params['n_layers'], model_params['enc_dropout'])\n","        dec = Decoder(model_params['output_dim'], model_params['dec_emb_dim'], model_params['hid_dim'], model_params['n_layers'], model_params['dec_dropout'])\n","        model = Seq2Seq(enc, dec, device).to(device)\n","        return model\n","\n","# Adjust this variable according to load an existing model or not\n","load_saved_model = True\n","\n","# Initialise device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load or initialize model\n","model = load_or_initialize_model(device)"]},{"cell_type":"markdown","metadata":{"id":"F0UHH9xyZNJn"},"source":["#### Train Loop\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:47:27.307233Z","iopub.status.busy":"2024-04-23T11:47:27.306787Z","iopub.status.idle":"2024-04-23T11:47:27.320319Z","shell.execute_reply":"2024-04-23T11:47:27.319385Z","shell.execute_reply.started":"2024-04-23T11:47:27.307199Z"},"id":"eFEITkB4ZNJn","trusted":true},"outputs":[],"source":["def train_model(model, train_loader, valid_loader, optimizer, criterion, n_epochs, device, print_step_size, save_path='best_model.pt'):\n","    model.train()\n","    best_valid_loss = float('inf')\n","\n","    for epoch in range(n_epochs):\n","        model.train()\n","        epoch_loss = 0\n","        for i, (src, trg) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n","            src, trg = src.to(device), trg.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(src, trg)\n","\n","            # trg is of shape [trg_len, batch_size]\n","            # output is of shape [trg_len, batch_size, output_dim]\n","            output_dim = output.shape[-1]\n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","            if (i + 1) % print_step_size == 0:\n","                print(f'Step {i+1}, Training Loss: {loss.item():.4f}')\n","\n","        # Validation loss\n","        model.eval()\n","        valid_loss = 0\n","        with torch.no_grad():\n","            for src, trg in valid_loader:\n","                src, trg = src.to(device), trg.to(device)\n","                output = model(src, trg, 0)  # Turn off teacher forcing\n","                output_dim = output.shape[-1]\n","                output = output[1:].view(-1, output_dim)\n","                trg = trg[1:].view(-1)\n","                loss = criterion(output, trg)\n","                valid_loss += loss.item()\n","\n","        average_train_loss = epoch_loss / len(train_loader)\n","        training_params[\"train_losses\"].append(average_train_loss)\n","\n","        average_valid_loss = valid_loss / len(valid_loader)\n","        training_params[\"valid_losses\"].append(average_valid_loss)\n","\n","        print(f'Epoch: {epoch+1}, Train Loss: {average_train_loss:.4f}, Valid Loss: {average_valid_loss:.4f}')\n","\n","        # Save the best model\n","        if average_valid_loss < best_valid_loss:\n","            best_valid_loss = average_valid_loss\n","            torch.save(model.state_dict(), save_path)\n","            print(f'Best model saved at Epoch {epoch+1} with Validation Loss: {average_valid_loss:.4f}')\n","\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"oRwdjo5tZNJn"},"source":["#### Print Losses Curve"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:47:28.424002Z","iopub.status.busy":"2024-04-23T11:47:28.423612Z","iopub.status.idle":"2024-04-23T11:47:28.429820Z","shell.execute_reply":"2024-04-23T11:47:28.428880Z","shell.execute_reply.started":"2024-04-23T11:47:28.423967Z"},"id":"Ybux7t47ZNJn","trusted":true},"outputs":[],"source":["def plot_loss_curve():\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(training_params[\"train_losses\"], label='Training Loss')\n","    plt.plot(training_params[\"valid_losses\"], label='Validation Loss')\n","    plt.title('Training and Validation Losses')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"plNe25hNZNJo"},"source":["#### Inference: Translating Sentences"]},{"cell_type":"markdown","metadata":{"id":"gETb_ITKZNJo"},"source":["To use the trained model for translating an English sentence into French, we need to encode the sentence using the trained Encoder and then iteratively decode the output using the trained Decoder."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-23T11:47:29.984634Z","iopub.status.busy":"2024-04-23T11:47:29.984292Z","iopub.status.idle":"2024-04-23T11:47:29.993737Z","shell.execute_reply":"2024-04-23T11:47:29.992726Z","shell.execute_reply.started":"2024-04-23T11:47:29.984607Z"},"id":"6MR4QaqxZNJo","trusted":true},"outputs":[],"source":["def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=50):\n","    model.eval()\n","\n","    # Tokenize and numericalize the input sentence\n","    tokens = ['<start>'] + [token.text.lower() for token in spacy_en.tokenizer(sentence)] + ['<end>']\n","    src_indexes = [src_vocab[token] for token in tokens]\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n","\n","    with torch.no_grad():\n","        hidden = model.encoder(src_tensor)\n","\n","    trg_indexes = [trg_vocab['<start>']]\n","\n","    for i in range(max_len):\n","        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n","\n","        with torch.no_grad():\n","            output, hidden = model.decoder(trg_tensor, hidden)\n","            pred_token = output.argmax(1).item()\n","\n","        trg_indexes.append(pred_token)\n","\n","        if pred_token == trg_vocab['<end>']:\n","            break\n","\n","    trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]\n","    return trg_tokens[1:-1]  # Remove the start and end tokens"]},{"cell_type":"markdown","metadata":{"id":"x_WwaRzzTjct"},"source":["#### Run Training or Inference"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"execution":{"iopub.execute_input":"2024-04-23T11:47:31.093011Z","iopub.status.busy":"2024-04-23T11:47:31.092337Z"},"id":"Bzin3225RwTG","outputId":"c58bdc71-c6b2-408a-830d-79dfc68fa804","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["English sentence: Hello, how are you?\n","French sentence: : , n c ' a s t - c a s s a s s a n s a n s ?\n"]}],"source":["run_inference = True\n","\n","if run_inference:\n","    # Run Inference\n","    sentence = \"Hello, how are you?\"\n","    print(f\"English sentence: {sentence}\")\n","    translation = translate_sentence(model, sentence, vocab_en, vocab_fr, device)\n","    print(f\"French sentence: : {' '.join(translation)}\")\n","else:\n","    # Run training\n","\n","    # Optimiser and Loss Function\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss(ignore_index=vocab_fr[\"<unk>\"])  # Ignore index for padding or <unk>\n","\n","    # Start training loop\n","    trained_model = train_model(model, train_loader, valid_loader, optimizer, criterion, training_params[\"n_epochs\"], device, training_params[\"print_step_size\"])\n","\n","    # Plot the loss curve\n","    plot_loss_curve()"]},{"cell_type":"markdown","metadata":{"id":"NbNdZRL3qi14"},"source":["## BLEU Score"]},{"cell_type":"markdown","metadata":{"id":"3ICQ8k80rT3V"},"source":["In neural machine translation, to assess the translation performance of sequence-to-sequence (Seq2Seq) models, such as those consisting of Recurrent Neural Networks (RNNs), the Bilingual Evaluation Understudy (BLEU) score is often employed.\n","\n","The BLEU score is a metric for evaluating a generated sentence to a set of reference sentences. A higher BLEU score corresponds to a translation that is more similar to the reference translations, with a score of 1 being a perfect match and a score of 0 indicating no overlap. BLEU score considers precision of n-grams in the generated text compared to the reference texts, and also includes a brevity penalty to prevent favoring overly short translations.\n","\n","The BLEU score is calculated as follows:\n","\n","1. **Compute the n-gram precision, Pn, for n=1 to N**:\n","\n","  $$\n","   P_n = \\frac{\\sum_{\\text{cand}\\in\\text{Candidates}} \\sum_{\\text{n-gram}\\in\\text{cand}} \\min(\\text{Count}_{\\text{cand}}(\\text{n-gram}), \\text{Max-Count}_{\\text{ref}}(\\text{n-gram}))}{\\sum_{\\text{cand}\\in\\text{Candidates}} \\sum_{\\text{n-gram}\\in\\text{cand}} \\text{Count}_{\\text{cand}}(\\text{n-gram})}\n","  $$\n","\n","   Where:\n","   - **Candidates**: The set of candidate translated sentences.\n","   - **n-gram**: A contiguous sequence of n words from the candidate translation.\n","   - **$\\text{Count}_{\\text{cand}}(\\text{n-gram})$**: The count of n-grams in the candidate translation.\n","   - **$\\text{Max-Count}_{\\text{ref}}(\\text{n-gram})$**: The maximum count of the n-gram in any single reference translation.\n","\n","2. **Compute the brevity penalty, BP**:\n","  $$\n","     BP = \\begin{cases}\n","       1 & \\text{if } c > r \\\\\n","       e^{(1-r/c)} & \\text{if } c \\leq r\n","     \\end{cases}\n","  $$\n","   \n","  Where:\n","   - **c**: The length of the candidate translation.\n","   - **r**: The effective reference corpus length.\n","\n","3. **Compute the BLEU score**:\n","  $$\n","  \\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log P_n\\right)\n","  $$\n","\n","   Where:\n","   - **wn**: The weight for each n-gram precision score (typically uniform, such as 1/N for N=4).\n","\n","\n","**Example Calculation:**\n","\n","Consider the following example with a candidate translation and two reference translations:\n","\n","- Candidate: \"The cat is on the mat.\"\n","- Reference 1: \"The cat is on the mat.\"\n","- Reference 2: \"There is a cat on the mat.\"\n","\n","For simplicity, let's calculate the BLEU score for up to bigrams (N=2) with uniform weights for unigram and bigram precision (w1=w2=0.5).\n","\n","1. **Unigram Precision (P1)**:\n","   The candidate has six unigrams, all of which appear in both reference sentences. So, P1 is 6/6 = 1.\n","\n","  - Candidate unigrams: \"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\"\n","  - Reference 1 unigrams: \"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\"\n","  - Reference 2 unigrams: \"There\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"\n","\n","  All unigrams in the candidate sentence appear in both reference sentences. The counts for each unigram in the candidate are:\n","\n","  - \"The\": 2\n","  - \"cat\": 1\n","  - \"is\": 1\n","  - \"on\": 1\n","  - \"the\": 1 (again for the second occurrence)\n","  - \"mat\": 1\n","\n","  Since all unigrams are found in the reference sentences, we get:\n","  $$\n","  P_1 = \\frac{6}{6} = 1\n","  $$\n","\n","2. **Bigram Precision (P2)**:\n","   There are five bigrams in the candidate sentence. All five appear in Reference 1, and four appear in Reference 2. The maximum count from the references is used for each bigram, and hence P2 is also 5/5 = 1.\n","\n","    - Candidate bigrams: \"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"\n","    - Reference 1 bigrams: \"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"\n","    - Reference 2 bigrams: \"There is\", \"is a\", \"a cat\", \"cat on\", \"on the\", \"the mat\"\n","\n","  In the candidate, each bigram appears once, and all but one (\"The cat\") also appear in Reference 2. For \"The cat\", we take the count from Reference 1:\n","\n","  - \"The cat\": 1 (from Reference 1)\n","  - \"cat is\": 1\n","  - \"is on\": 1\n","  - \"on the\": 1\n","  - \"the mat\": 1\n","\n","  The counts match for all bigrams, so the precision is:\n","  $$\n","  P_2 = \\frac{5}{5} = 1\n","  $$\n","\n","3. **Brevity Penalty (BP)**:\n","    - Candidate sentence length (c): 6 words\n","    - Reference sentence lengths: 6 words (Reference 1), 7 words (Reference 2)\n","    - The effective reference corpus length (r) is the one that is closest to the candidate length, which in this case is 6 words from Reference 1.\n","\n","  Since the candidate length c is not less than the reference length r, the brevity penalty BP is 1:\n","  $$\n","  BP = 1 \\text{ (because c is not less than r)}\n","  $$\n","\n","4. **BLEU Score**:\n","  With uniform weights (0.5 each for unigram and bigram precision), the BLEU score is:\n","\n","   $$\n","   \\text{BLEU} = BP \\cdot \\exp\\left(0.5 \\log P_1 + 0.5 \\log P_2\\right)\n","   \\text{BLEU} = 1 \\cdot \\exp\\left(0.5 \\log 1 + 0.5 \\log 1\\right) = 1\n","   $$\n","\n","In this example, the BLEU score is 1, indicating a perfect match with the reference translations. However, it's important to note that in real-world scenarios, BLEU scores are typically less than 1, reflecting various degrees of translation quality.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.status.busy":"2024-04-23T11:43:01.487197Z","iopub.status.idle":"2024-04-23T11:43:01.487920Z","shell.execute_reply":"2024-04-23T11:43:01.487671Z","shell.execute_reply.started":"2024-04-23T11:43:01.487651Z"},"id":"sFMGgV5g1ZYN","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["English sentence: Hello, how are you?\n","French sentence: : , n c ' a s t - c a s s a s s a n s a n s ?\n","BLEU score: 0\n"]}],"source":["import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","\n","def calculate_bleu(reference_sentences, candidate_sentence):\n","    # The reference sentences should be tokenized and passed as a list of lists\n","    # The candidate sentence should be tokenized and passed as a list\n","    # Each reference and candidate sentence should start and end with <start> and <end> tokens respectively\n","\n","    # We'll use nltk's method for smoothing.\n","    smoothing = SmoothingFunction().method1\n","\n","    # Calculate BLEU score\n","    bleu_score = sentence_bleu(reference_sentences, candidate_sentence, smoothing_function=smoothing)\n","\n","    return bleu_score\n","\n","sentence = \"Hello, how are you?\"\n","print(f\"English sentence: {sentence}\")\n","translation = ' '.join(translate_sentence(model, sentence, vocab_en, vocab_fr, device))\n","print(f\"French sentence: : {translation}\")\n","\n","# Tokenize the reference sentences\n","references = [translation]\n","\n","# Calculate BLEU score\n","bleu_score = calculate_bleu(references, [sentence])\n","print(f\"BLEU score: {bleu_score}\")"]},{"cell_type":"markdown","metadata":{"id":"0sM-jiGKZNJo"},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{"id":"nasvVV_hZNJp"},"source":["Today, NMT systems are integral parts of global communication, supporting instant translation across numerous language pairs with increasing reliability. As NMT continues to evolve, it integrates more deeply with other AI technologies, pushing towards truly interactive, real-time multilingual communication and making the dream of removing language barriers more attainable than ever. The evolution from simple rule-based systems to complex neural architectures reflects broader trends in AI towards more holistic and context-aware systems, promising exciting developments for the future of language processing."]},{"cell_type":"markdown","metadata":{"id":"m5kq1OirncM1"},"source":["## References"]},{"cell_type":"markdown","metadata":{},"source":["- A ten-minute introduction to sequence-to-sequence learning in Keras: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Vn-Z2gyMZNJl","mLIZ7NwMZNJl","UrWqNTtdZNJm","_g4EG91mRwTE"],"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
