{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlocking the Mysteries of Language Generation: A Deep Dive into Decoding Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rapidly evolving field of natural language processing (NLP), the ability to generate coherent and contextually relevant text is a cornerstone of innovation. From chatbots and virtual assistants to automated content creation, the underlying technology that powers these advancements hinges on sophisticated decoder models. \n",
    "\n",
    "This article embarks on a journey to demystify the complex world of decoder strategies—namely, Greedy, Beam Search, Pure Sampling, Top-K, and Top-P sampling methods. By weaving together the mathematical underpinnings, practical code implementations, and intuitive visualizations, we aim to not only illuminate the inner workings of these methods but also explore their unique strengths and limitations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explain the algorithms, I want to use a simple model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model for the Test Pruposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Small Corpus: We'll create a small corpus of sentences to simulate a simple natural language processing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus\n",
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Dogs are great pets.\",\n",
    "    \"Birds fly in the sky.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Fish swim in the sea.\",\n",
    "    \"Chocolate is the best dessert.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the Corpus: Tokenize the corpus into words, build a vocabulary, and prepare numerical representations for the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization and Numerical Encoding (Simplified tokenization: split by spaces and punctuation )\n",
    "tokens = set(word.strip(\".,\").lower() for sentence in corpus for word in sentence.split())\n",
    "vocab = {word: i+2 for i, word in enumerate(tokens)}\n",
    "vocab['<s>'] = 0  # Start token\n",
    "vocab['</s>'] = 1  # End token\n",
    "inv_vocab = {i: word for word, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 7, 11, 9, 27, 7, 2, 1],\n",
       " [0, 8, 4, 14, 5, 1],\n",
       " [0, 26, 15, 24, 7, 12, 1],\n",
       " [0, 7, 10, 23, 16, 17, 22, 7, 6, 13, 1],\n",
       " [0, 28, 18, 24, 7, 25, 1],\n",
       " [0, 3, 21, 7, 20, 19, 1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode sentences\n",
    "encoded_sentences = [[vocab['<s>']] + [vocab[word.strip(\".,\").lower()] for word in sentence.split()] + [vocab['</s>']] for sentence in corpus]\n",
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(DecoderModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embeddings(x)\n",
    "        out = self.fc(torch.sum(embeds, dim=0).unsqueeze(0))\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "# Example usage with the simple model\n",
    "vocab_size = 5  # Our vocabulary size\n",
    "embedding_dim = 10  # Embedding dimension\n",
    "output_dim = vocab_size  # Output dimension same as vocabulary size for simplicity\n",
    "\n",
    "# Create the model\n",
    "model = DecoderModel(len(vocab), embedding_dim, len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.569339418411255\n",
      "Epoch 10, Loss: 2.9727208763360977\n",
      "Epoch 20, Loss: 2.4815364092588426\n",
      "Epoch 30, Loss: 2.0504838064312936\n",
      "Epoch 40, Loss: 1.6878063097596168\n",
      "Epoch 50, Loss: 1.3987934745848178\n",
      "Epoch 60, Loss: 1.1779991440474986\n",
      "Epoch 70, Loss: 1.0159291876479983\n",
      "Epoch 80, Loss: 0.9005912322551012\n",
      "Epoch 90, Loss: 0.8192859986796975\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Prepare the data: For each sentence, use every word to predict the next word\n",
    "inputs = []\n",
    "targets = []\n",
    "for sentence in encoded_sentences:\n",
    "    inputs.extend(sentence[:-1])  # All but the last word\n",
    "    targets.extend(sentence[1:])  # All but the first word\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training Loop\n",
    "n_epochs = 100  # Number of epochs (iterations over the dataset)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        input_word = inputs[i].unsqueeze(0)  # Add batch dimension\n",
    "        target_word = targets[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        optimizer.zero_grad()  # Clear existing gradients\n",
    "        output = model(input_word)  # Get the model output for the current word\n",
    "        loss = criterion(output, target_word)  # Calculate the loss\n",
    "        loss.backward()  # Perform backpropagation\n",
    "        optimizer.step()  # Update the model's weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:  # Print average loss every 10 epochs\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss/len(inputs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = vocab['<s>'] # start_token (int): The index of the start token to begin generation.\n",
    "end_token = vocab['</s>'] # end_token (int): The index of the end token which stops generation.\n",
    "max_length = 10 # max_length (int): The maximum length of the sequence to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Greedy Decoder selects the word with the highest probability at each step of the sequence generation. \n",
    "\n",
    "Mathematically, it can be expressed as:\n",
    "\n",
    "$$\n",
    "y_t = \\arg\\max P(y_t | y_{1:t-1}, X)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $y_t$ is the word selected at time $t$.\n",
    "- $X$ is the input to the decoder.\n",
    "- $y_{1:t-1}$ are the words selected in previous steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decoding: A Conceptual Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below demonstrates how beam search run: \n",
    "\n",
    "1. **Initialization**: Start with the start token `<s>` as the initial sequence.\n",
    "\n",
    "2. **Step 1**: Generate all possible next words from `<s>` and their log probabilities. Unlike beam search, greedy decoding immediately selects the word with the highest probability without keeping multiple sequences. Suppose \"I\" has the highest probability; the sequence is now `<s> I`.\n",
    "\n",
    "3. **Step 2**: Generate all possible continuations from `<s> I` and select the word with the highest probability as the next word. If \"am\" is the most probable continuation, the sequence becomes `<s> I am`.\n",
    "\n",
    "4. **Continue**: This process repeats, at each step selecting the most probable next word. There's no need to keep multiple sequences as in beam search—only the single best choice at each step is considered.\n",
    "\n",
    "5. **Termination**: The process ends when the sequence generates an end token `</s>` or reaches a predefined maximum length. The final sequence might be `<s> I am a student </s>`, chosen one word at a time based solely on immediate probability without considering longer-term impacts on sequence quality.\n",
    "\n",
    "6. **Comparison to Beam Search**: In contrast to the beam search where the final choice might be between `<s> I am a student </s>` and `<s> The cat </s>` with considerations of length normalization, greedy decoding might end up with a less globally optimal but locally optimal choice like `<s> I am a student </s>`. There's no comparison of alternative paths; the decision at each step is final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decoding: Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence1: <s> the sky </s>\n",
      "Generated sequence2: <s> the sky </s>\n",
      "Generated sequence3: <s> the sky </s>\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoder(model):\n",
    "    \"\"\"\n",
    "    Generate a sequence of words using a greedy decoding strategy.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The trained model for generating sequences.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: The generated sequence of words.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    input_word = torch.tensor([start_token], dtype=torch.long)\n",
    "    generated_sequence = [start_token]\n",
    "    \n",
    "    for _ in range(max_length - 1):\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            output_logits = model(input_word.unsqueeze(0))  # Add batch dimension\n",
    "        next_word = output_logits.argmax(dim=-1).item()\n",
    "        generated_sequence.append(next_word)\n",
    "        \n",
    "        if next_word == end_token:\n",
    "            break\n",
    "        \n",
    "        input_word = torch.tensor([next_word], dtype=torch.long)\n",
    "    \n",
    "    # Convert indices to words using the inverse vocabulary\n",
    "    generated_words = [inv_vocab[idx] for idx in generated_sequence if idx in inv_vocab]\n",
    "    \n",
    "    return generated_words\n",
    "\n",
    "generated_sequence = greedy_decoder(model)\n",
    "print(\"Generated sequence1:\", ' '.join(generated_sequence))\n",
    "print(\"Generated sequence2:\", ' '.join(generated_sequence))\n",
    "print(\"Generated sequence3:\", ' '.join(generated_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sequential Prediction**: Greedy decoding generates text one word at a time, starting from a specified start token. At each step, it selects the word with the highest probability as the next word in the sequence. This process continues until either the end token is generated or the maximum sequence length is reached.\n",
    "- **Deterministic Output**: Given the same start conditions (e.g., the start token and model state), greedy decoding will always produce the same output sequence. This is evident from the repeated outputs (`<s> the sky </s>`) shown in the example. The deterministic nature stems from always choosing the most likely next word without exploring less probable alternatives.\n",
    "- **Efficiency**: Greedy decoding is computationally efficient because it involves a single forward pass through the model for each word in the sequence, without the need to track or compare multiple potential sequences.\n",
    "- **Limitations**:\n",
    "  - **Lack of Diversity**: The deterministic nature of greedy decoding means it may not explore diverse or creative text possibilities. It tends to favor more common or expected word sequences, which can lead to repetitive or generic outputs.\n",
    "  - **Local Optima**: By focusing on the highest probability word at each step without considering the overall sequence quality, greedy decoding can get trapped in local optima—choices that seem best in the short term but don't necessarily lead to the best overall sequence.\n",
    "  - **End Token Dependency**: The decoding process's conclusion relies heavily on the model's ability to predict the end token (`</s>`) at the appropriate time. If the model struggles with this, sequences might end abruptly or not at all within the given maximum length.\n",
    "\n",
    "In summary, greedy decoding serves as a straightforward and efficient baseline strategy for sequence generation. However, depending on the application's needs for creativity, diversity, and context sensitivity, exploring alternative decoding strategies may be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam Search keeps track of multiple hypotheses (sequences) at each step. The score of a sequence is the sum of the log probabilities of the words in the sequence, normalized by the sequence length to penalize longer sequences. It's a heuristic search algorithm that expands upon the idea of greedy decoding by exploring a wider set of possible sequences at each step and selecting the best ones to continue expanding.\n",
    "\n",
    "The goal of beam search is to find a sequence $Y = (y_1, y_2, \\ldots, y_{|Y|})$ that maximizes the score $Score(Y)$, given the input $X$. The score of a sequence is determined by the sum of the log probabilities of the words in the sequence, normalized by the sequence length raised to the power of a normalization parameter $\\alpha$. This can be mathematically represented as:\n",
    "\n",
    "$$\n",
    "Score(Y) = \\frac{1}{|Y|^\\alpha} \\sum_{t=1}^{|Y|} \\log P(y_t | y_{1:t-1}, X)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $(|Y|)$ is the length of the sequence $(Y)$.\n",
    "- $(y_t)$ is the word at time $(t)$.\n",
    "- $(X)$ is the input to the sequence generation model.\n",
    "- $(\\alpha)$ is a length normalization parameter that helps balance the preference between longer and shorter sequences. A higher value of $(\\alpha)$ penalizes longer sequences more heavily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search: A Conceptual Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below demonstrates how beam search balances exploration and exploitation in sequence generation, considering multiple paths and using the score function to select the most promising sequences to explore further.\n",
    "\n",
    "Imagine you're using beam search to generate text with a beam size $k$ of 2, meaning at each step, you keep the top 2 sequences according to their scores:\n",
    "\n",
    "1. **Initialization**: Start with the start token `<s>` as the initial sequence.\n",
    "\n",
    "2. **Step 1**: Generate all possible next words from `<s>` and their log probabilities. Suppose we have two words with the highest probabilities, \"I\" and \"The\" (assume \"I\" has a slightly higher score). We keep these two sequences: `<s> I` and `<s> The`.\n",
    "\n",
    "3. **Step 2**: For each of the two sequences, generate all possible continuations and calculate their scores. For `<s> I`, the top two continuations might be `<s> I am` and `<s> I have`. For `<s> The`, they might be `<s> The cat` and `<s> The dog`. Among these four, suppose `<s> I am` and `<s> The cat` have the highest scores; we keep these two.\n",
    "\n",
    "4. **Continue**: Repeat this process, expanding each of the top sequences and keeping only the top $k$ sequences at each step.\n",
    "\n",
    "5. **Termination**: The process continues until a stopping criterion is met, such as reaching a maximum sequence length or all of the top sequences ending with an end token `</s>`. The highest-scoring sequence among the final set is selected as the output.\n",
    "\n",
    "6. **Selection**: Consider the final two sequences are `<s> I am a student </s>` with a total log probability of -3 and `<s> The cat </s>` with a total log probability of -1. Without length normalization $(\\alpha = 0)$, `<s> I am a student </s>` would win despite being longer and having a lower average probability per word. With length normalization $(\\alpha > 0)$, we penalize the longer sequence, potentially making `<s> The cat </s>` the winner if we value brevity or if the penalty is high enough to offset the lower total log probability of the shorter sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Decoding: Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence with length normalization: <s> birds fly in the lazy dog </s> in the lazy dog </s> brown fox jumps over the sky </s> in the sky </s> in the sky </s> in the\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "\n",
    "def beam_search_decoder_with_length_norm(model, beam_width, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Generate a sequence of words using a beam search decoding strategy with length normalization.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The trained model for generating sequences.\n",
    "    - beam_width (int): The number of sequences to keep at each step.\n",
    "    - alpha (float): The length normalization parameter.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: The highest-scoring generated sequence of words.\n",
    "    \"\"\"\n",
    "    model.eval()  # Evaluation mode\n",
    "    hypotheses = [([start_token], 0.0)]  # Initialize with the start token and zero score\n",
    "    \n",
    "    for _ in range(max_length - 1):\n",
    "        new_hypotheses = []\n",
    "        \n",
    "        for seq, score in hypotheses:\n",
    "            last_word = torch.tensor([seq[-1]], dtype=torch.long).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                # Get log probabilities from the model\n",
    "                log_probs = model(last_word).squeeze(0)  # Assume model output shape is [1, vocab_size]\n",
    "            \n",
    "            for idx in range(log_probs.size(1)):  # Iterate over the second dimension (vocab_size)\n",
    "                next_word_log_prob = log_probs[0, idx].item()  # Correctly access scalar log probability\n",
    "                new_seq = seq + [idx]\n",
    "                new_log_prob_sum = score + next_word_log_prob  # Accumulate log probabilities correctly\n",
    "                \n",
    "                # Apply length normalization\n",
    "                normalized_score = new_log_prob_sum / (len(new_seq) ** alpha)\n",
    "                \n",
    "                new_hypotheses.append((new_seq, normalized_score))  # normalized_score is already a Python float\n",
    "\n",
    "        # Keep the top 'beam_width' sequences after filtering\n",
    "        hypotheses = sorted(new_hypotheses, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "        if all(seq[-1] == end_token for seq, _ in hypotheses):\n",
    "            break\n",
    "    \n",
    "    # Choose the best sequence\n",
    "    best_sequence, _ = sorted(hypotheses, key=lambda x: x[1], reverse=True)[0]\n",
    "    generated_words = [inv_vocab.get(idx, '<unk>') for idx in best_sequence]  # Convert indices to words\n",
    "    \n",
    "    return generated_words\n",
    "\n",
    "beam_width = 3  # Beam width\n",
    "alpha = 0.3  # Length normalization parameter\n",
    "generated_sequence = beam_search_decoder_with_length_norm(model, beam_width, alpha)\n",
    "print(\"Generated sequence with length normalization:\", ' '.join(generated_sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Diversity in Hypotheses**: The output sequence \"`<s> birds fly in the lazy dog </s> in the lazy dog </s> brown fox jumps over the sky </s> in the sky </s> in the sky </s> in the`\" showcases the diversity that beam search can introduce compared to greedy decoding. By keeping multiple hypotheses (sequences) at each step, beam search explores a wider range of possible continuations, leading to more varied text generation.\n",
    "\n",
    "2. **Impact of Length Normalization**: The inclusion of length normalization, controlled by the parameter \\(\\alpha\\), helps to balance between shorter and longer sequences. By penalizing longer sequences to a degree (\\(\\alpha = 0.3\\) in this example), it encourages the selection of sequences that are coherent but not excessively verbose. This parameter is crucial for ensuring that the final generated text doesn't favor overly long or short sequences disproportionately.\n",
    "\n",
    "3. **Repetition and Coherence**: The generated sequence does show some repetition (e.g., multiple occurrences of \"`in the sky </s>`\"), which is a common challenge in sequence generation tasks. Beam search's focus on keeping the top scoring sequences can sometimes lead to repetition if those sequences dominate the beam. Length normalization helps mitigate this by penalizing sequences that merely extend without adding meaningful content, but repetition may still occur.\n",
    "\n",
    "4. **Selection of the Best Sequence**: The final sequence chosen demonstrates how beam search, combined with length normalization, attempts to find a balance between sequence length, coherence, and the cumulative probability of the sequence. The selection of \"`<s> birds fly in the lazy dog </s>`\" as part of the output suggests that this sequence had a favorable combination of length and probability score under the given model and parameters.\n",
    "\n",
    "5. **Model Dependence**: The quality and relevance of the generated sequences heavily depend on the underlying model's training and architecture. A well-trained model on a comprehensive dataset can produce more coherent and contextually appropriate sequences. The example output reflects the limitations and capabilities of the provided `DecoderModel`, which operates on a simplified level without the complexities of real-world language understanding.\n",
    "\n",
    "This example illustrates the nuanced dynamics of beam search in text generation, emphasizing the technique's strengths in expanding search space and the importance of careful parameter tuning and model design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Sampling Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure sampling can be represented as choosing the next word $y_t$ at each timestep $t$ based on the probability distribution:\n",
    "\n",
    "$$\n",
    "P(y_t | y_{1:t-1}, X)\n",
    "$$\n",
    "\n",
    "provided by the model, where:\n",
    "- $y_t$ is the word selected at time $t$.\n",
    "- $X$ is the input to the decoder.\n",
    "- $y_{1:t-1}$ are the words selected in previous steps.\n",
    "\n",
    "The key distinction from greedy decoding is that pure sampling takes into account the full distribution for the next word selection, rather than simply picking the most likely word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Sampling Decoding: A Conceptual Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a step-by-step example of how pure sampling might run:\n",
    "\n",
    "1. **Initialization**: Start with the start token `<s>` as the initial sequence.\n",
    "\n",
    "2. **Step 1**: Generate all possible next words from `<s>` along with their probabilities. Instead of selecting the highest probability word, we **sample** a word based on the probability distribution. Suppose the distribution slightly favors \"I\", but \"The\" is also a strong contender; we might end up with either `<s> I` or `<s> The` based on the randomness of the sampling.\n",
    "\n",
    "3. **Step 2**: For the selected sequence, generate all possible continuations and their probabilities. Again, **sample** the next word based on this distribution. The process introduces variability, as even less probable words have a chance of being selected.\n",
    "\n",
    "4. **Continue**: This process repeats, with each step introducing potential for diverse continuations based on the probability distribution of the next word.\n",
    "\n",
    "5. **Termination**: The sequence generation concludes when an end token `</s>` is sampled or the maximum sequence length is reached. Due to the stochastic nature of pure sampling, different runs will produce different sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Sampling Decoding: Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence with pure sampling decoding: <s> the cat sat on cat the best dessert </s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "def pure_sampling_decoder(model):\n",
    "    \"\"\"\n",
    "    Generate a sequence of words using a pure sampling decoding strategy.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The trained model for generating sequences.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: The generated sequence of words.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    input_word = torch.tensor([start_token], dtype=torch.long)\n",
    "    generated_sequence = [start_token]\n",
    "    \n",
    "    for _ in range(max_length - 1):\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            output_logits = model(input_word.unsqueeze(0))  # Add batch dimension\n",
    "            probabilities = F.softmax(output_logits, dim=-1).squeeze()  # Ensure probabilities is 1D or 2D\n",
    "\n",
    "            # Check if probabilities is 2D (batched) and select the first (and only) batch\n",
    "            if probabilities.dim() > 1:\n",
    "                probabilities = probabilities[0]\n",
    "            \n",
    "            next_word = torch.multinomial(probabilities, 1).item()  # Sample based on probabilities\n",
    "            generated_sequence.append(next_word)\n",
    "        \n",
    "            if next_word == end_token:\n",
    "                break\n",
    "        \n",
    "            input_word = torch.tensor([next_word], dtype=torch.long)\n",
    "    \n",
    "    # Convert indices to words using the inverse vocabulary\n",
    "    generated_words = [inv_vocab[idx] for idx in generated_sequence if idx in inv_vocab]\n",
    "    \n",
    "    return generated_words\n",
    "\n",
    "generated_sequence = pure_sampling_decoder(model)\n",
    "print(\"Generated sequence with pure sampling decoding:\", ' '.join(generated_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Insights from Pure Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the pure sampling decoder, \"`<s> the cat sat on cat the best dessert </s>`\", provides a valuable demonstration of pure sampling's characteristics and its implications for text generation. Here are several insights drawn from this output regarding pure sampling decoding:\n",
    "\n",
    "1. **Diversity and Unpredictability**: The output sequence showcases the inherent diversity and unpredictability of pure sampling. Unlike greedy decoding, which would likely produce the same sequence given the same starting conditions, pure sampling can generate a wide variety of sequences due to its stochastic nature. This is evident from the unexpected repetition of \"cat\" and the seemingly unrelated conclusion \"the best dessert\".\n",
    "\n",
    "2. **Creative and Novel Outputs**: Pure sampling has the potential to generate more creative and novel outputs by exploring less probable paths that might be overlooked by deterministic approaches. This can lead to interesting and sometimes surprising sequences, as seen with the inclusion of \"dessert\" in a sequence starting with \"the cat\".\n",
    "\n",
    "3. **Potential for Reduced Coherence**: While pure sampling introduces diversity, it also risks producing sequences with reduced coherence or logical flow. The sequence \"`<s> the cat sat on cat the best dessert </s>`\" might be grammatically correct but lacks a clear, coherent narrative, highlighting the trade-off between creativity and coherence.\n",
    "\n",
    "4. **Balance Between Exploration and Exploitation**: Pure sampling represents a shift towards exploration in the exploration-exploitation trade-off, potentially at the expense of exploiting more probable (and possibly more coherent) sequences. This approach is beneficial for generating diverse content but requires careful consideration of the application's needs regarding coherence and predictability.\n",
    "\n",
    "5. **Application-Specific Suitability**: The suitability of pure sampling depends heavily on the specific application. For tasks requiring high creativity and diversity, such as story generation or ideation, pure sampling can be advantageous. For tasks requiring high precision and coherence, such as formal text generation, modifications or alternative strategies might be necessary.\n",
    "\n",
    "6. **Impact of the Model's Training**: The effectiveness and coherence of the sequences generated by pure sampling are also influenced by the underlying model's training. A well-trained model on a comprehensive and diverse dataset can produce more meaningful and contextually appropriate sequences even under pure sampling, as it learns robust representations of language patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Sampling Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mathematical Foundation:** Top-K Sampling narrows the sampling pool to the top \\(K\\) most probable words at each step. This balances the trade-off between randomness and determinism, allowing for diverse but plausible sequence generation.\n",
    "\\[P'(y) = \\frac{P(y | y_{1:t-1}, X) \\cdot \\mathbb{1}_{y \\in K}}{\\sum_{y' \\in K} P(y' | y_{1:t-1}, X)}\\]\n",
    "where \\(P'\\) is the modified probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P (Nucleus) Sampling Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Mathematical Foundation:** Top-P Sampling, also known as Nucleus Sampling, selects the smallest set of words whose cumulative probability exceeds the threshold \\(P\\). This method aims to focus on a dynamic number of high-probability words while filtering out the tail of less likely ones, balancing diversity and relevance.\n",
    "\\[ \\text{Choose the smallest set } S \\text{ such that } \\sum_{y \\in S} P(y | y_{1:t-1}, X) > P \\]\n",
    "\n",
    "**Code Implementation:**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def top_p_sampling_decoder(model, input, start_token, end_token, max_length, p=0.9):\n",
    "    output_sequence = [start_token]\n",
    "    for _ in range(max_length):\n",
    "        output = model(input, torch.tensor(output_sequence).unsqueeze(0))\n",
    "        probabilities = torch.softmax(output[:, -1], dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        # Remove tokens with cumulative sum above p, keeping at least one word\n",
    "        sorted_indices_to_remove = cumulative_probs > p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        probabilities[indices_to_remove] = 0\n",
    "        next_word = torch.multinomial(probabilities, 1).item()\n",
    "        output_sequence.append(next_word)\n",
    "        if next_word == end_token:\n",
    "            break\n",
    "    return output_sequence\n",
    "```\n",
    "\n",
    "**Beam Search Decoder Usage:**\n",
    "```python\n",
    "beam_width = 5  # Width of the beam\n",
    "\n",
    "beam_search_sequences = beam_search_decoder(model, input, start_token, end_token, max_length, beam_width)\n",
    "print(\"Beam Search Sequences:\", beam_search_sequences)\n",
    "```\n",
    "\n",
    "**Pure Sampling Decoder Usage:**\n",
    "```python\n",
    "pure_sampling_sequence = pure_sampling_decoder(model, input, start_token, end_token, max_length)\n",
    "print(\"Pure Sampling Sequence:\", pure_sampling_sequence)\n",
    "```\n",
    "\n",
    "**Top-K Sampling Decoder Usage:**\n",
    "```python\n",
    "k = 10  # Top-K value\n",
    "\n",
    "top_k_sampling_sequence = top_k_sampling_decoder(model, input, start_token, end_token, max_length, k)\n",
    "print(\"Top-K Sampling Sequence:\", top_k_sampling_sequence)\n",
    "```\n",
    "\n",
    "**Top-P Sampling Decoder Usage:**\n",
    "```python\n",
    "p = 0.9  # Cumulative probability threshold\n",
    "\n",
    "top_p_sampling_sequence = top_p_sampling_decoder(model, input, start_token, end_token, max_length, p)\n",
    "print(\"Top-P Sampling Sequence:\", top_p_sampling_sequence)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
